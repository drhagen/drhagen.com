{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"David R Hagen","text":"<p>Statistics is math. Because it's math, there is at most one correct answer to every statistical problem. It takes knowledge to compute the answer when it can be computed. It takes art to craft a robust approximation when it cannot.</p> <p>This is the personal website of David R Hagen, scientific software engineer. You may be interested in my open source software projects or my infrequently updated blog.</p>"},{"location":"#highlighted-blog-posts","title":"Highlighted blog posts","text":"<ul> <li>solving xkcd's riddle of the 11th of the month</li> <li>how to eliminate money from politics while alleviating voter apathy</li> <li>how to sanely define equality in a programming language</li> </ul>"},{"location":"cv/","title":"CV","text":""},{"location":"cv/#education","title":"Education","text":"<p>Ph. D. in Biological Engineering, 2014 Massachusetts Institute of Technology, Cambridge MA</p> <p>B. S. in Biochemistry and B. A. in Economics, 2008 Case Western Reserve University, Cleveland OH Minor in Chemistry; graduated summa cum laude with 4.0 GPA</p> <p>Ayersville High School, Defiance OH Graduated valedictorian with 4.0 GPA</p>"},{"location":"cv/#expertise","title":"Expertise","text":"<ul> <li> <p>Software Engineering: in particular, the data structures representing scientific workflows and the architecture to execute those workflows</p> </li> <li> <p>Statistics: in particular, information theory and Bayesian statistics with an appreciation for the importance of understanding the assumptions behind the techniques being applied</p> </li> <li> <p>Programming: Python, Matlab, and R; also Bash and C; past proficiency in Scala and C#</p> </li> <li> <p>Cloud computing: in particular, dockerized deployments and high performance computing in AWS</p> </li> </ul>"},{"location":"cv/#career-history","title":"Career History","text":""},{"location":"cv/#software-engineer-applied-biomath-certara-2017present","title":"Software Engineer, Applied BioMath / Certara, 2017\u2013present","text":"<p>Director of Engineering, Feb 2023\u2013present</p> <p>Senior Principal Scientist, Sep 2020\u2013Feb 2023</p> <p>Associate Director of IT, Apr 2020\u2013Feb 2023</p> <p>Principal Software Engineer, Jun 2017\u2013Sep 2020</p> <ul> <li> <p>Designed, architected, and managed the development of the Applied BioMath quantitative systems pharmacology (QSP) modeling platform, which was acquired by Certara in Dec 2023</p> </li> <li> <p>Oversaw the roll-out of the software platform to enthusiastic scientists as it replaced all internal usage of Matlab for systems pharmacology</p> </li> <li> <p>Replaced Django with FastAPI to provide a lower-maintenance backend for the platform</p> </li> <li> <p>Containerized the platform with Docker and deployed it to AWS using Terraform, eliminating reproducibility and scalability issues associated with running code on scientists' laptops</p> </li> <li> <p>Designed and implemented the parsing, validation, and compilation of four model text file formats to improve the productivity of modelers with progressively more expressive modeling domain-specific languages (DSLs)</p> </li> <li> <p>Directed the development of a core simulation engine in C, wrapping the SUNDIALS ODE solver library, achieving a 100x single-threaded performance improvement over the existing modeling tool, which enabled virtual patient population simulations on a large immuno-oncology consortium model that were previously intractable</p> </li> <li> <p>Designed and implemented a data grammar library in NumPy, exposing it via an API to replace specialized post-processing steps with customizable user-defined data pipelines</p> </li> <li> <p>Designed and implemented an expressions library to process models with arbitrary user-defined expressions, lower them to C code, compile them on-the-fly, and execute them as machine code for maximum performance</p> </li> <li> <p>Designed and implemented type checking, including units, across the API, ensuring consistency within models, across simulations, and through the data grammar pipelines, nearly eliminating unit errors from modeling reports</p> </li> <li> <p>Designed a custom job scheduler in asynchronous Python, reducing the latency of interactive jobs by 20x over Celery/Rabbit</p> </li> </ul>"},{"location":"cv/#data-scientist-merck-20142017","title":"Data Scientist, Merck, 2014\u20132017","text":"<p>Associate Principal Scientist, May 2017\u2013Jun 2017</p> <p>Senior Scientist,\u00a0Jul 2014\u2013May 2017</p> <ul> <li> <p>Improved the KroneckerBio toolbox to serve as platform for a QSP model of diabetes, which used literature, preclinical, and early data to predict outcomes of later trials and to reveal the desirable characteristics of backup compounds</p> </li> <li> <p>Used the KroneckerBio toolbox to develop a PBPK model of anacetrapib to investigate the source of the compound's long terminal half-life, supplying superior estimates of parameter uncertainty</p> </li> <li> <p>When the simulation platform for Keytruda crashed with a imminent filing, stepped in to reconstitute the workflow at the Linux command line over a couple of days, also bringing simulation time from 48 hours down to 20 minutes and analysis time from 40 hours to 4 minutes</p> </li> <li> <p>Served as technical lead on the development of a web-based workbench for NONMEM modeling and R analysis, ensuring traceability and reproducibility for modeling and simulation</p> </li> <li> <p>Designed and developed a preclinical compartmental fitting tool to automate the fitting and selection of a set of basic PK models to every batch of incoming study data with an appropriate design</p> </li> <li> <p>Designed and developed a Matlab tool for submitting arbitrary functions from a user's Matlab desktop to the Linux cluster, saving $17000 in license fees per user</p> </li> <li> <p>Designed and developed an internal R Shiny web application for converting SAS datasets to CSV to reduce the need for SAS installations and expertise</p> </li> <li> <p>Collaborated on the design and implementation of a R library for standardized plotting and reporting; designed and implemented a library to ease the checking of datasets for errors and for easily comparing two datasets</p> </li> </ul>"},{"location":"cv/#graduate-researcher-bruce-tidor-lab-massachusetts-institute-of-technology-20092014","title":"Graduate Researcher, Bruce Tidor lab, Massachusetts Institute of Technology, 2009\u20132014","text":"<ul> <li> <p>Developed a novel method for computing the probability distribution on a set of discrete topologies according to a set of data using linearization as an approximation technique, showing that it gave a comparable answer to a gold-standard Monte Carlo method, while taking far less computational resources</p> </li> <li> <p>Demonstrated that linearization was an effective technique for approximating parameter uncertainty for the purpose of optimal experimental design, showing in a computational scenario, that the parameters of a biological network could be recovered from noisy data using a handful of optimally chosen experiments</p> </li> <li> <p>Developed and maintained KroneckerBio, the laboratory's systems biology toolbox written in Matlab, preforming simulation, parameter fitting, and statistical analysis of parameter and topology uncertainty.</p> </li> <li> <p>Managed a 2000-core high performance computing cluster</p> </li> </ul>"},{"location":"cv/#teaching-assistant-biological-engineering-department-massachusetts-institute-of-technology-2009","title":"Teaching Assistant, Biological Engineering Department, Massachusetts Institute of Technology, 2009","text":"<ul> <li>In collaboration with co-TA Tim Curran, re-wrote the class tutorials, homework instructions, and implementation assignments of 20.420 Biomolecular Kinetics and Cellular Dynamics class, which by both the student and instructor accounts, greatly improved the learning to effort ratio of the out-of-class assignments, an effect that persists year after year as the material is reused by subsequent TAs</li> </ul>"},{"location":"cv/#undergraduate-researcher-richard-w-hanson-lab-case-western-reserve-university-20052008","title":"Undergraduate Researcher, Richard W Hanson lab, Case Western Reserve University, 2005\u20132008","text":"<ul> <li>Responsible for developing and testing hypotheses concerning the PEPCK-Cmus mice, a strain of super-active mice created in the lab</li> <li>Applied for and received funding for my own research project studying the effect of the transgene on metabolism and diabetes</li> </ul>"},{"location":"cv/#publications","title":"Publications","text":"<p>Diana H Marcantonio, Andrew Matteson, Marc Presler, John M Burke, David R Hagen, Fei Hua, Joshua F Apgar, \"Early Feasibility Assessment: A Method for Accurately Predicting Biotherapeutic Dosing to Inform Early Drug Discovery Decisions\", Frontiers in Pharmacology, 2022.</p> <p>Bo Zheng, Lucia Wille, Karsten Peppel, David Hagen, Andrew Matteson, Jeffrey Ahlers, James Schaff, Fei Hua, Theresa Yuraszeck, Enoch Cobbina, Joshua F. Apgar, John M. Burke, John Roberts, Raibatak Das, \"A systems pharmacology model for gene therapy in sickle cell disease\", CPT: Pharmacometrics &amp; Systems Pharmacology, 2021.</p> <p>Rajesh Krishna, Ferdous Gheyas, Yang Liu, David R Hagen, Brittany Walker, Akshita Chawla, Josee Cote, Robert O Blaustein, David E Gutstein, \"Chronic Administration of Anacetrapib Is Associated With Accumulation in Adipose and Slow Elimination\", Clinical Pharmacology and Therapeutics, 2017.</p> <p>David R Hagen and Bruce Tidor, \"A Computational Experimental Design Method for Efficiently Reducing Topology Uncertainty\",\u00a0unsubmitted.</p> <p>David R Hagen and Bruce Tidor, \"Efficient Bayesian Estimates for Discrimination among Topologically Different Systems Biology Models\", Molecular BioSystems, 2015.</p> <p>David R Hagen, \"Parameter and Topology Uncertainty for Optimal Experimental Design\", MIT Doctoral Thesis, 2014.</p> <p>David R Hagen, Jacob K White, and Bruce Tidor, \"Convergence in parameters and predictions using computational experimental design\", Interface Focus, 2013.</p> <p>David R Hagen, Joshua F Apgar, David K Witmer, Forest M White, Bruce Tidor, \"Reply to Comment on 'Sloppy models, parameter uncertainty, and the role of experimental design'\", Molecular BioSystems, 2011.</p> <p>Parvin Hakimi, Jianqi Yang, Gemma Casadesus, Duna Massillon, Fatima Tolentino-Silva, Colleen K Nye, Marco E Cabrera, David R Hagen, Christopher B Utter, Yacoub Baghdy, David H Johnson, David L Wilson, John P Kirwan, Satish C Kalhan, Richard W Hanson, \"Overexpression of the cytosolic form of phosphoenolpyruvate carboxykinase (GTP) in skeletal muscle repatterns energy metabolism in the mouse\", Journal of Biological Chemistry, 2007.</p>"},{"location":"software/","title":"Software","text":""},{"location":"software/#parsita","title":"Parsita","text":"<p>Parsita is a parser combinator library for Python. It is hosted\u00a0on Github and available on PyPI. Its main goal is a maximally simple interface for parsing. The API is inspired by Scala's parser combinator library, but the operators were carefully selected to have intuitive meaning and precedence in Python. A\u00a0unique feature of Parsita is that it uses metaclass magic to allow for forward declarations of values by converting names not found during class body execution into forward declaration objects resolved during class instantiation.</p>"},{"location":"software/#tabeline","title":"Tabeline","text":"<p>Tabeline is a data table and data grammar library for Python. It is inspired by dplyr in R and uses Polars underneath. It provides and easy-to-use interface for doing data manipulations by providing the standard verbs (like <code>select</code>, <code>filter</code>, and <code>mutate</code>) as methods on its central <code>DataTable</code> class and having those methods accept strings (like <code>table.filter(\"t &lt; 24\")</code>) which are parsed and executed in the context of the table.</p>"},{"location":"software/#tensora","title":"Tensora","text":"<p>Tensora is a sparse and dense tensor algebra library for Python. It is based on the Tensor Algebra Compiler (TACO). The user can create tensors whose formats are varying combinations of dense and sparse dimensions. The tensor can be populated from a variety of sources, such as dictionaries of keys, list of tuples, tuples of lists, NumPy arrays, and SciPy sparse matrices. While basic mathematical operators, such as <code>__add__</code>, <code>__sub__</code>, <code>__matmul__</code>, are defined, the most important feature is the <code>evaluate</code> function, which takes a string like <code>\"y(i) = A(i,j) * x(j)\"</code> and executes that expression on the given tensors. Under the hood, <code>evaluate</code> parses the tensor algebra expression, validates it, turns it into C code, invokes a C compiler on that code, executes the code on the given tensors, and returns the result to the user. All in all, it provides an easy-to-use interface to a very powerful tool.</p>"},{"location":"software/#serialite","title":"Serialite","text":"<p>Serialite is serialization/deserialization library for Python. It is similar to Pydantic, but is much more strict. It's main abstract base class anticipates <code>to_data</code> and <code>from_data</code> methods. It provides the <code>serializable</code> decorator, which can be applied to data classes to generate those methods from the data class fields. It also provides the <code>abstract_serializable</code> decorator, which can be applied to sealed classes to generate those methods using a <code>\"_type\"</code> discriminator which switches on the <code>__subclasses__</code> of the base class. Serialite implements enough of the Pydantic interface so that it can be used by FastAPI.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/stochasticity-incompatible-with-optimization/","title":"Stochasticity Incompatible with Optimization","text":"<p>I have been thinking about the design of a framework capable of modelling an entire cell. Two of the design specifications,\u00a0stochasticity and optimization, that I have been planning to incorporate seem to be incompatible.</p> <p>Stochasticity is a form of randomness in cells that originates from the fact cells are made of molecules and these molecules react after bouncing around until they hit something that makes them react. This makes the concentration of the molecules and, hence, the cell change somewhat\u00a0 randomly. Only when there are large numbers of each molecular species, these random fluctuations average out so that overall behavior is quite smooth and predictable.</p> <p>Optimization is the automated changing of the parameters to make the model more consistent with measured data. I usually envision a black box into which data goes in and a description of the parameter space consistent with that data comes out. The only work the researcher has to put in to define the likelihood function for new types of data.</p> <p>For mass action models for biological systems, I have found that this optimization fairly easy to accomplish for large models. There is usually one minimum or, occasionally, a small number of local minima.\u00a0 The minimum can be found efficiently by sequential quadratic programming using an analytical gradient, once the problem has been properly scaled and a few other tweaks for biological systems have been applied. Starting from the best-fit parameters, the model ensemble can be characterized effectively by a linear expansion or by Hastings sampling of the likelihood. From here, one can compute estimates and uncertainties on the topology, parameters, and predictions. But the key piece is that analytical gradient! All my experience has shown that, without that, an algorithm cannot figure out which way to move in parameter space to get to the better parameters; as the number of parameters increases, the parameter space grows exponentially until it is quickly a hyperdimensional labyrinth.</p> <p>Optimization becomes a problem when considering stochasticity. And stochasticity cannot be ignored if we hope to make an accurate model of a simple living organism because the assumption of a large number of every species is simply not true for many species. If we use a deterministic approximation to the stochastic noise, such as the linear noise approximation (LNA) or mass fluctuation kinetics (MFK), I suspect that optimization would still work well. It is tricky to compute the gradient on LNA or MFK, but I have done it. Nevertheless, I am skeptical that noise approximations can properly capture all behaviors of the cell. In particular, DNA is a reactant that exists and is important in only 0, 1, or 2 copies per cell. Ribosome assembly is also problematic because the number of states is astronomically larger than the number of ribosome copies. If we use a stochastic model to represent these processes, then we will have a problem for several reasons: (1) there is no good way to compute the likelihood for some stochastic data, and (2) there is no way to compute the derivative of the likelihood with respect to the parameters for any stochastic data.</p> <p>How could you not compute the likelihood? you may ask. Consider some single cell data where a batch of cells had many proteins observed over time. The naive way to compute the likelihood that a particular parameter set is true is to simulate the stochastic model many times and try to match each simulation to each data set, where each data set comes from the observation of a single cell. That is, ask what is the probability that I would see this data set given that the cell had these stochastic trajectories. Then multiply those likelihoods across the data sets for each simulation and average across all simulations\u2014this is the likelihood that this data would be seen given this parameter set. This is not so bad unless the measurement uncertainty is smaller than the stochastic uncertainty. Then, the odds that a data point would be consistent with the trajectories of a simulation is quite small even if the perfect parameters were used. Even when using perfect parameters, a simulation lining up with a data point will be fairly rare, making it very expensive to see enough instances to get a good estimate of how often this happens.\u00a0 This is still not so bad until the large number of data points per experiment is considered. If enough measurements are taken per cell in this experiment, the odds that all measurements will line up with a particular trajectory is infinitesimal. As a result, all trajectories will come back with essentially 0 probability for all simulations for all parameters unless a enormously impractical number of simulations are done.</p> <p>One way around this problem is to decouple the data points in a single cell from each other, simply multiply the likelihood over all data points rather than all data sets. The fraction of simulations that match a particular data point will be a lot higher than the fraction that match an entire data set. But this is not entirely correct, this throws away the correlation information between the data points. For example, if the data measured one state at the beginning and end of a simulation and the true model would either be high, low, rise from low to high, or fall from high to low, then a model that would be only either low or high only would fit just as well as the true model. Fortunately, it is not completely wrong to throw out information. The parameter estimates will have higher uncertainty because some parameter sets that are clearly wrong according to the data will be retained, but no correct parameter sets will be removed. If it is easier to collect more data than to fit the data we already have, this may be the right course of action.</p> <p>The gradient, the derivative of the likelihood with respect to the parameters, is a critical part of gradient-descent-based global optimizers and is even more problematic for stochastic models. Assuming that the likelihoods can be calculated precisely, then the gradient can be calculated by finite differences. But as far as I can imagine, the likelihood will always be random with a stochastic model, meaning that taking the difference between two nearby points in parameter space will be dominated by the random noise, especially since the parameter space of biological models is shaped like thin needles with a very gentle slope toward the better parameters. Without a precise analytical gradient, it is extremely difficult to fit a biological model with many parameters. How one would construct an analytical gradient for a model that is not analytical eludes me. One possibility would be to fit the deterministic model to the data, and hope it gets close enough before using the stochastic model for predictions. Another possibility would be dump optimization entirely and require the system parameters to be determined outside the cell in a macroscopic context. In a any case, it seems unlikely that I will be able to build my black box to fit whole-cell stochastic models to huge amounts of single-cell data.</p>"},{"location":"blog/one-bad-naming-fad/","title":"One\u2122 Bad Naming Fad","text":"<p>When a trendy corporate fad is embraced by the dinosaur in the market, that usually marks the peak and indicates the coming decline in the fad. I for one hope that the recent release of the Xbox One digital dinosaur Microsoft marks the decline of one bad naming fad\u2014naming your latest product \"One\". It seems like everyone has a \"&lt;MyBrand&gt; One\" out. In addition to Xbox One, I have run across Pandora One, Ubuntu One,\u00a0Dell's XPS One, HTC One, and PLOS ONE.</p> <p>The one that is particularly unfortunate is PLOS ONE; the others will be forgotten in a few years, but the groundbreaking (and now largest) journal will last much longer. Ten years from now that name is going to seem silly. Just imagine if they had launched it in the early dot-com years and today it was called \"eJournal.com\". That would have been kind of hot in 1999. I predict that they will change the name to just \"Plos\" around 2017.</p> <p>I have been trying to figure out when this fad started. The problem is that the word \"one\" is simply too common to discriminate in a web search. Though it may be because this is when I first started noticing, it seems that PLOS ONE the oldest one with an origin of 2006. And that makes the name more forgivable; they couldn't know how that every major digital organization was going to name their latest product \"One\" also.</p> <p>There will be a few more bad names before this is all over. So here's to you Samsung One, Facebook One, Twitter One, Sony One, etc.</p>"},{"location":"blog/trial-by-jury-just-give-us-the-facts/","title":"Trial by Jury: Just Give Us the Facts","text":"<p>I just finished a seven-day trial as a juror on a civil suit. This was my first time called and serving on a jury. It was an enlightening experience. There are a number of observations that I have. I may make an omnibus post to share all the little things I found surprising, from the high quality of the jury video to the great facial expressions of the judge. For now, I am going to focus on one major observation: most of the court time was spent extracting from the witnesses facts that were not in dispute, and if these facts had been directly conveyed to us, it would have been far more efficient without sacrificing any fairness in the proceedings. I am going to change a few details of the case so that this article will not draw any unrequested attention to the participants in this case.</p> <p>At the start of the trial, the jury is told only that the case is a lawsuit and the names of the plaintiff and the defendant. If it was not for the opening statements, we would have no idea at all what the case was about at all. There are several problems with opening statements. Firstly, they are really short. You may think that the bias in the opening statement is the biggest problem. It\u2019s not; the paucity of context is confusing. We hear a flurry of names for the first time. One company contracted a construction company to perform some work, who then subcontracted out a part of the work to another company, who then subcontracted out a part of that part, who then hired the plaintiff who was injured on the job. All of these companies have various employees, supervisors, foremen, and carpenters. I furiously tried to construct a map of the major players in my notebook, but was ultimately left with a gap-riddled tree. This was a big problem for me. We started hearing the evidence without the context to understand it.</p> <p>After the opening statements, they went right into presenting evidence. Now there is a strange thing about evidence in the US justice system that I did not know going in. Apparently, no one can tell the jury anything, present any evidence to the jury, even physical evidence, except for witnesses. To understand how weird this is, I\u2019m going to present two different scenarios. In both scenarios, the jury starts with no information concerning what happened.</p> Scenario 1 <p>Someone: Here is a picture of the warehouse where the incident in question occurred. (Shows picture to jury.)</p> Scenario 2 <p>Lawyer: (To witness) I\u2019m going to show you a picture. (To judge) May I approach the witness? Judge: You may. Lawyer: Is this a picture of the place where the incident in question occurred? Witness: (Studies picture for some time.) Hmm, it could be. Lawyer: If I represent to you that this is the warehouse in question, would that be consistent with your recollection? Witness: Yes. Lawyer: (To judge) I would like to have this marked as an exhibit. Judge: (To other lawyer) Any objection? Other Lawyer: No objection. Judge: Mark it. Clerk: So marked as exhibit 5.</p> <p>The goal of each interchange is to simply show the jury a picture of the place that everyone is going to be talking about for the next week. This fact is not in dispute, but the jury needs the context of the lawsuit. There are many facts that are not in dispute, but every fact is entered with the same care as every other fact. This results in hours upon hours of very slowly telling and retelling a story.</p> <p>The difficulty is compounded by the fact that no witness witnessed the whole story, so the early witnesses testify with no context. For a simple case with only one or a few witnesses to a single scene (like a cop pulling over a drunk driver), this is not a problem. The entire incident can be described by a single witness; there is no important context to consider.</p> <p>By the end of the trial, I feel like I got the whole story. The extraordinarily slow and repetitive presentation of the evidence helps overcome the lack of context at the beginning of the trial. The story was told many times over, so I had to get it eventually just from sheer drilling. But I always felt that I could have gotten the story far more efficiently than I did.</p> <p>And this inefficiency bothered me. About ninety percent of the facts in the case were not in dispute. Why were we not given a few sheets of paper that began, \u201cThe following is a narrative of the events relevant to this lawsuit. Portions of this narrative were accepted by both parties as undisputed. Some facts are in question, and the differing stories according to the plaintiff and defendant are clearly marked.\u201d I know that they did not do it in this case because that is just not the way American courts are run. But why they not run this way? Why are the undisputed facts not given directly to the jury?</p> <p>The risk of introducing bias is minimal. At its simplest, if one side thinks that something is disputed, then it is not given to the jury in this way. If either side disputed all the facts, the system would simply revert to the current method. As far I could tell, all of the evidence was already reviewed by both sides and judge in order to determine what was admissible. While classifying evidence as admissible or inadmissible, why not also classify it as disputed or undisputed?</p> <p>I can understand why the system was originally constructed in this way. If your population is widely illiterate, then all the information needs to be provided to the jury orally. If publishing is expensive and slow, then you cannot do the back and forth necessary to build a narrative that both sides agree on and then give copies to the jury to read. But neither of those problems exist today. America is universally literate, and editing and printing documents is trivial.</p> <p>While there are a few physical pieces of evidence, the principle substrate of the court is words. A court takes in words and outputs decisions. With all of the information technology that we have invented in the several centuries, can we not improve this important and expensive argument processor? We should at least narrow down the argument portion of the trial to the points that actually need arguing. Give jurors the undisputed context directly, without all the legalese.</p>"},{"location":"blog/trial-by-jury-keep-the-jury-eliminate-the-trial/","title":"Trial by Jury: Keep the Jury, Eliminate the Trial","text":"<p>Yesterday, I made a simple argument for handing the jurors the facts that are not in dispute at the beginning of the trial. I want to extend that argument a bit further to a more expansive idea that I have. Rather than just giving the undisputed facts to the jurors, why not boil the entire trial down to its text and give it to the jury as a kind of case study. All of the questions asked in the trial are already in multiple depositions. If a witness gives an answer that is different from the deposition, the deposition is brought out and read. Why not deliver all the evidence to the jury in writing? I can read faster than I can listen, and importantly, I can alter my pace as needed.</p> <p>By using writing alone, the emotional expressions of the witnesses would be lost. Sometimes the expressions of the witnesses were helpful for interpreting their answers. But on the whole, I felt that it just gave more credibility to witnesses who were adamant about their testimony, which was a problem when the circumstances were clear that they should not be so certain of their memories of events from seven years ago. Abolishing the impassioned speeches from the lawyers would not be a bad thing. Also, being able to remove inadmissible testimony entirely would be better than telling the jury to disregard testimony that was already heard.</p> <p>Here is my idea that I think some state should try: Have a judge meet with the lawyers from both sides to create an unbiased document that describes the case. The undisputed facts are written as normal narrative, and the disputed facts are noted within the narrative with references to evidence. The entire packet is sent to each juror to read before coming to court. (There might be some need to ensure that each juror actually reviews the material.) The jury then meets and reaches a verdict without ever meeting any of the parties involved. This would result in a much faster process. It would also make a smooth-talking lawyer less valuable. It would also remove much of the emotion from the proceedings, replacing it with cold hard text.</p> <p>The principle goal of this proposal is to reduce the cost to the jurors in terms of their time. A substantial cost of any trial is the time of the jury\u2014twelve people for several days at mean wages is thousands of dollars. This cost is mostly externalized as jurors are not paid more than a nominal amount for their time. Because of lost wages, potential jurors are widely acknowledged to try avoiding service. Those who stand to lose significant wages will be underrepresented in the jury. Simply reducing the cost of jury service will ameliorate this underrepresentation without costing the government or lawsuit parties any more. It is truly a win-win for all parties involved.</p>"},{"location":"blog/trial-by-jury-omnibus/","title":"Trial by Jury: Omnibus","text":"<p>Here is a collection of short thoughts from my time as a juror.</p> <ul> <li>I have never spent that much time with that many native New Englanders. Apparently, \"the pike was pahking lot this munning\".</li> <li>There is a video that the court shows all the prospective jurors. It is actually pretty good. The officer hinted that this was recently introduced to replace a several-decade-old video.</li> <li>The courtroom was very modern. It looked like an office with some special furniture. It did not have the oak paneling and carved symbols.</li> <li>There was a bookcase behind the judge, which we thought was for decoration, until she onetime turned around, pulled a book out, and looked up something while in a sidebar.</li> <li>The judge had some great facial expressions. When witnesses started to pontificate, she gave this look out of the corner of her eye that is exactly the same as the one a mother gives when she is watching a child who looks like he is about to do something he shouldn't do. She is awaiting and expecting a moment that she will have to intervene.</li> <li>They did not buy us lunch except on deliberation day. Even during full trial days we had to find our own lunch.</li> <li>I loved it when the judge told us to disregard a particular statement and I had already disregarded it because it was stupid question anyway.</li> <li>Could both sides not share a projector and screen?</li> <li>When you show a witness a picture, please show it to the jury; otherwise, you two are having a private conversation up there.</li> <li>Hearsay rules need to be loosened. Most of the time it just results in really awkwardly stories. This usually has to do with someone explaining why he did something, and it is because someone told him to do it. When a judge says, \"Can you get this out of him without hearsay?\", she is asking that the exact same information be given, just in a legalese phrasing. Juries should be allowed to judge hearsay as it is.</li> <li>There is this white noise that the clerk plays over the jury box whenever the lawyers go to a sidebar.</li> <li>Judge: [After a question was asked, objected to, the objection sustained, and then the question was reworded and reasked by the lawyer.]\u00a0 \"Oh, come on!\"</li> <li>Judge: [When a question is being asked, the other lawyer stands up to object. To the talking lawyer.] \"Stop! We are not going there.\" Talking lawyer: \"May we have a sidebar.\" [Upon return, we went there.]</li> <li>Based on the stories I have heard about juries and my experience with people, I expected emotions to play a central role in the deliberations. I was wrong and pleasantly surprised. No one held a lawyer's behavior against his client or referred to evidence that had been thrown out.</li> </ul>"},{"location":"blog/the-electoral-jury/","title":"The Electoral Jury","text":"<p>How much time to do you spend figuring out how you should vote? A few hours? A few minutes? Any time at all? I could not find a study that measured the average time that voters spend researching their candidates, but I did find that three quarters of Americans do not know how long a senator's term is. Perhaps an average voter reads the blurbs in the local newspaper, listens to some soundbites on the news about each candidate, and watches a myriad of television ads. Does he read each candidate's website? If not, this is like owning a bagel shop and hiring a manager without reading his resume. Imagine a bagel shop where all employees are hired with the same consideration that the average voter gives to political candidates. Such a shop would operate with the legendary efficiency and effectiveness of a DMV.</p> <p>You may think I am writing to encourage you to put more effort into deciding for whom to vote. I am not. You are doing it right. Unlike choosing a bagel shop manager, choosing a governor culminates in the counting of millions of votes. The odds that your vote either way will change anything at all is infinitesimal. Your vote, frankly, doesn't matter. It is irrational to vote at all. Enough people do it out of altruism which keeps the system running, but not enough people do the research necessary to keep it running well.</p> <p>How much research would be appropriate to choose the governor? I just spent a week on a jury deciding a workplace injury lawsuit. Companies and universities spend months looking for a president. Would we be better off if everyone took a week off of work to research the candidates for governor? Well, no. We would undoubtedly end up with a better governor, but the cost would astronomical. And that is just for the governor. Imagine having to do this for the mayor, the city council, the state representative, the state senator, the federal representative, the federal senator, and the President\u2014not to mention lots of other positions that probably should not even be elected positions, like dogcatchers and judges. With the cost of research so high and payoff so small we collectively settle for a million snap judgements and hope for the best.</p> <p>Can we do better, and if so, how? The reason that an individual vote is worthless is simply because there are so many voters. The decision, while valuable and important by itself, once spread over millions of voters is irrelevant. The huge number of voters per election is simultaneously the reason that it would be prohibitively expensive to have all voters be politically educated. A week of one person's time is not that much; a week of million persons' time is overwhelming. If we want to have votes count, we need to have fewer voters. A solution to this problem is something I call the electoral jury (technically, a form of sortition). In the electoral jury, a random subset of the eligible voting population is chosen and this subset, the jury, is tasked with electing a particular office. Rather than having the entire adult population of a state vote for the governor, a random few are told that they and they alone get to choose the next governor. I present here a small subset of the details of this plan.</p>"},{"location":"blog/the-electoral-jury/#jury-size","title":"Jury Size","text":"<p>Choosing the size of the jury is the most important variable. If the jury is too small, then stochastic effects become important. With an 11-person jury and 60%/40% split between two candidates, the 60% candidate will win 75% of the time. With a 101-person jury, the 60% candidate will win 98% of the time. With a 1001-person jury, the 60% candidate will win 100% of the time. The closer the race, the more likely it will be for the majority winner to lose simply by chance when jury is small. With a 1001-person jury and a 51%/49% split, the 51% candidate will win 72% of the time. At a 10001-person jury, the odds are 98% in the favor of the 51% candidate. So it takes about 10000 people to completely eliminate chance from the election. I am not convinced that one needs to eliminate chance entirely. Is a 51% candidate\u00a0 clearly better than a 49% candidate such that there needs to be no chance that the 49% candidate is elected? The main benefit of increasing the jury size is to get rid of chance and prevent the government from constantly changing hands even when there is no change in public opinion. If a governor is elected with 60% of the vote and, going into his reelection, 60% of the people still support him, then he should keep his seat. With a jury of only a dozen people, there is a good chance it will not happen.</p> <p>If the jury is too small, then a successful bribe carries a lot of weight. We do not think much about secret bribes in today's politics. The number of voters is so large that secret bribery is infeasible. In the end, this should not be a large problem. We manage to prevent bribery in criminal and civil cases, where the juries are small and the stakes are high. Nevertheless, it is one effect to keep in mind.</p> <p>On the flipside, if the jury is too big, then the original problem returns. Each vote begins to not matter much. The original US House of Representatives had one representative for every 30,000 people. Each representative was elected by much fewer than 30,000 people, considering that suffrage was fairly restricted at the time.</p> <p>Keeping all this in mind, I think a number of jurors in the 1000-10000 range is about right.</p>"},{"location":"blog/the-electoral-jury/#jury-location","title":"Jury Location","text":"<p>The easiest place to have the voters vote is at home. It's cheap, easy, and exactly the same as the current system. But with so fewer voters, we can do so much better. Here is where the big payoff of the electoral jury comes. With a small number of people (1000-10000 range), it is feasible to sequester them at a convention center for a week. Here, they will have the opportunity to listen to speeches made by the candidates, read material provided by the candidates, and discuss among each other their ideas and opinions. Importantly, they will be isolated from all forms of external media. Why is this important? Because this removes money from politics. TV ads, newspaper ads, street criers, door-to-door solicitors\u2014they all swing elections and they all cost money. This requires all candidates to be friendlier toward people with lots of money. With the jury sequestered and every candidate given equal and large access to the voters, campaign contributions are now meaningless. And on top of it all, no one's freedom of speech is impinged. Anyone can say what he wants, spending as much money as he likes to say it; it just will not matter. The voters will not see the ads, and if they do in the months leading up to the election, the message of the ads will be swamped by a week's worth of speeches. A 30 second TV ad is only effective because each voter spend so little time considering to the candidates.</p> <p>Would sequestering the jury eliminate the value of journalism? If a journalist uncovered the fact that one of the candidates was a secret communist, would the jury never see this? No, the jury would see it, because the candidates are not sequestered. If something important is found outside, one of them can bring it in and present it to the jury.</p> <p>Another advantage of bringing everyone together is that people are typically good at judging the character of those they meet in person. Judging character is one of the most important aspects of voting. It is not what a politician says he will do that matters; it is what he actually does. That people are pretty good at judging people they actually get to know is more of an impression I have. I have been unable to find research to support this, so maybe this is wishful thinking.</p>"},{"location":"blog/the-electoral-jury/#accommodations","title":"Accommodations","text":"<p>Sequestration for a week will place many jurors under hardship. It is important that they be accommodated as best as possible. The juror's wage should be paid to his family for the week he is absent, so that the electoral jury is not biased against those with dependents. All travel expenses should be paid for by the state so that the jury is not biased against those who live farther away. Childcare either at home or at the convention center should be provided for those who care for small children.</p> <p>It is impossible to remove all inconvenience. But choosing 10 thousand out of 10 million every four years means that being selected is a once-in-a-lifetime opportunity. Almost everyone will never be chosen to serve on a jury in his lifetime. I expect that most would be willing to tolerate inconvenience to participate.</p>"},{"location":"blog/the-electoral-jury/#final-thoughts","title":"Final Thoughts","text":"<p>The largest barrier to implementing a plan like this is the status quo. If the politician is chosen by a tiny fraction of the population, how could that be fair? The best point of reference is probably the current court system. The jury is fair, not because everyone in the county votes on it, but because the jurors are randomly selected from everyone in the county. Imagine that you were falsely accused of murder and there are two possible juries. The first possibility is twelve people forced to listen to all the evidence given every day by opposing sides. The second possibility is a million people otherwise going about their day who can choose to watch the case or just catch the news about it (with ads from the prosecutor). The potential jurors may vote on your conviction regardless of how much attention they actually paid. It can be fairer to have a smaller sample represent the whole, especially if we can force that sample to sit through fairly presented evidence before making a decision.</p> <p>All it takes is one brave state to take up this plan for electing their governor. That state and others watching shall be pleasantly surprised.</p>"},{"location":"blog/comparison-of-iteration-styles-in-programming/","title":"Comparison of Iteration Styles in Programming","text":"<p>It is difficult to overstate the importance of iteration in programming\u2014the process performing an operation on each element of a list. I would rank only variable assignment, functions calling, and branching as more\u00a0important. Unlike the <code>if</code> statement, which is essentially the same in every language, the semantics of the <code>for</code> loop varies across languages. Mainly for my own future reference, this post compares\u00a0the various styles of iteration that I have come across. The examples are in pseudo code which is the only way to write so many different iteration styles under\u00a0similar\u00a0syntax. Each of the examples is trying to do the same thing: print out each element of a list called <code>list</code>.</p> <p>There are several features that I am looking for. Most techniques are missing one or more of these.</p> <ol> <li>Type safety. There is no way to get an\u00a0exception in code that passes a static type checker.</li> <li>Immutability. Both using and creating iterators should be possible without resorting to mutable state.</li> <li>Generality. Arbitrary elements can be considered</li> <li>Utility.\u00a0Arbitrary collections can be iterated</li> <li>Controlability. Arbitrary things can be done with the elements</li> </ol>"},{"location":"blog/comparison-of-iteration-styles-in-programming/#indexing","title":"Indexing","text":"<pre><code>for (i = 1; i &lt;= list.count; i++):\n  print(list(i))\n\n# which is just sugar for\ni = 1\nloop:\n  if not i &lt;= list.count:\n    break\n  else:\n    print(list(i))\n    i = i + 1\n</code></pre> <p>This is an ancient form of iteration. It is also closest to the metal\u2014that is, how computers actually do this sort of thing. Each iteration requires\u00a0one comparison and one increment. Getting the next element and then checking\u00a0to see if it is valid will be a recurring theme of iteration.</p> <p>Its simplicity is really the only thing going for it. It has more boilerplate than a steam locomotive. And it is easy to get wrong in a way that will pass the type checker. For example, don't mix up\u00a0<code>i &lt; list.count</code>\u00a0and\u00a0<code>i &lt;= list.count</code>! It depends on whether or not your language is 0-indexed or 1-indexed. You also cannot iterate over anything whose size is not (cheaply) known beforehand. For example, you cannot iterator over the lines of a file this way.</p> <p>This method succeeds on #3 Generality, but fails on all other counts. It is the only method mentioned here that fails on #4 Utility.</p>"},{"location":"blog/comparison-of-iteration-styles-in-programming/#throwing-iterators","title":"Throwing Iterators","text":"<pre><code>for (element in list):\n  print(element)\n\n# which is just sugar for\niterator = list.iterator()\nloop:\n  try:\n    element = iterator.next()\n    print(element)\n  catch (StopIteration e):\n    break\n</code></pre> <p>This is the first of many styles\u00a0that use the syntax <code>for (element in list)</code>. Here, <code>list</code> can any object that implements the <code>Iterable</code> interface. The <code>Iterable</code> interface has a single method <code>iterator</code>, which returns an instance of <code>Iterator</code>, a mutable object with a method <code>next</code>. What <code>next</code> does exactly is the\u00a0difference between throwing, sentinel, option, and peeking iterators. In every case, the <code>for</code> loop calls <code>next</code> repeatedly, getting a different element of the list in turn, until the iterator is exhausted and something special happens.</p> <p>Once there are no elements left in a throwing iterator,\u00a0the next\u00a0call to <code>next</code> throws a <code>StopIteration</code> exception. The <code>for</code> loop catches this exception and exits the iteration.</p> <p>From a purity standpoint, it is rather unsanitary to be\u00a0using exceptions to handle something that is not only unexceptional, but a mandatory consequence of iteration.\u00a0Python uses this technique and recently had to change some things because of the fact that exceptions can be throw anywhere, which leads to weird behavior like loops quietly\u00a0terminating when an errant exception is thrown. This also seems like it would be the most difficult to optimize because exceptions interrupt the normal flow of the program.</p> <p>This style succeeds on #3 Generality, #4 Utility, and #5 Controlability. It fails on #1 Type-safety and #2 Immutability.</p>"},{"location":"blog/comparison-of-iteration-styles-in-programming/#peeking-iterators","title":"Peeking Iterators","text":"<pre><code>for (element in list):\n  print(element)\n\n# which is just sugar for\niterator = list.iterator()\nloop:\n  if not iterator.has_next():\n    break\n  else:\n    element = iterator.next()\n    print(element)\n</code></pre> <p>A peeking iterator also throws an exception when <code>next</code> is called, but that is not how one is supposed to use a peeking iterator. It has a second method <code>has_next</code>, which returns <code>true</code> if <code>next</code> will return an element and <code>false</code> if the iterator is exhausted.</p> <p>One\u00a0disadvantage is that the implementation of all iterators requires\u00a0an extra function whose only purpose is to avoid running into undefined behavior when the iterator runs out.\u00a0If the next value is generated on the fly, it may be necessary to compute the value of <code>next</code>, without calling <code>next</code> of course, because that would increment the iterator and throw the value away. If the computation of next is expensive, it will be necessary to store that value after computing <code>has_next</code> in anticipation of the subsequent\u00a0call to <code>next</code>.</p> <p>This style succeeds in #3 Generality, #4 Utility, and #5 Controlability. After the indexing iterator, this is the worst violator of #1 type-safety. Naturally, it does not follow #2 Immutability.</p>"},{"location":"blog/comparison-of-iteration-styles-in-programming/#sentinel-iterators","title":"Sentinel Iterators","text":"<pre><code>for (element in list):\n  print(element)\n\n# which is just sugar for\niterator = list.iterator()\nloop:\n  element = iterator.next()\n  if element is Null:\n    break\n  else:\n    print(element)\n</code></pre> <p>Rather than throw an exception when no more elements are found, a sentinel iterator returns a special value (often called <code>null</code>). Every <code>for</code> loop tests if the returned value is the sentinel value and aborts before running the next iteration if that is true. If the collection has elements of type <code>Element</code>, then the return type of <code>iterator.next()</code> is the union type <code>Element|Null</code>.</p> <p>The disadvantage of this approach is that now collections cannot contain arbitrary objects, making it the only style that fails on #3 Generality. In particular, a collection cannot contain the sentinel value. Otherwise, when an iterator returns that element of the collection, it will think that the collection has been exhausted. Ceylon uses this method, which means that <code>{true, finished, null}.size</code>\u00a0returns <code>1</code> because <code>finished</code> is the sentinel value.</p> <p>This style succeeds on all points except #3 Generality and #2 Immutability.</p>"},{"location":"blog/comparison-of-iteration-styles-in-programming/#option-iterators","title":"Option\u00a0Iterators","text":"<pre><code>for (element in list):\n  print(element)\n\n# which is just sugar for\niterator = list.iterator()\nloop:\n  option = iterator.next()\n  if option is None:\n    break\n  else:\n    element = option.get\n    print(element)\n</code></pre> <p>A slight variation of the\u00a0sentinel iterator is to\u00a0box the response with a type\u00a0that makes it possible to distinguish between the iterator being finished and the iterator\u00a0happening to have a sentinel value in the list. This wrapper class is\u00a0an abstract class called <code>Option</code> with two concrete classes, <code>Some</code> and <code>None</code>. An iterator returns <code>Some(element)</code> until it is exhausted and then it returns <code>None</code>. In this case, <code>None</code> is the sentinel value that causes termination of the iterations, but if the list has a <code>None</code> in it, it returns <code>Some(None)</code> which can be distinguished from plain <code>None</code>.</p> <p>This style succeeds on all points except #2 Immutability.</p>"},{"location":"blog/comparison-of-iteration-styles-in-programming/#list-iterators","title":"List Iterators","text":"<pre><code>for (element in list):\n  print(element)\n\n# which is just sugar for\niterator = list.iterator\nloop:\n  if iterator is Empty:\n    break\n  else:\n    element = iterator.head\n    print(element)\n    iterator = iterator.tail\n</code></pre> <p>The throwing, peeking, sentinel, and option iterators all require mutable iterators\u2014each call to <code>next</code> causes the iterator object to be different upon the next call. Mutable state is mildly harmful to sane programming. Fortunately, most users only encounter iterators via the <code>for</code> loop, which safely hides all the mutability. The writers of generators still need to deal with mutability, however. Mutability can be completely avoided by having the iterator not only return the next value but the next iterator also. Here, there are two types of iterators <code>Full</code> and <code>Empty</code>. The <code>Full</code> instance\u00a0has a <code>head</code> method that returns the next element of the iteration, but the iterator is also immutable so it returns the same value every time. The <code>Full</code> iterator has a second method <code>tail</code>, which returns a fresh iterator. That tail is also immutable, and it also is either <code>Full</code> or <code>Empty</code>. If it is <code>Empty</code>, the iteration terminates, but if it is <code>Full</code>, the iteration continues with a call to its head.</p> <p>This is my favorite iterator. There is no required mutability either for\u00a0creators of iterators or\u00a0users of iterators. There are no weird\u00a0sentinel values. There are no required exceptions and no exceptions for anything that type-checks. There is a performance issue here. Rather than just returning\u00a0one element per iteration, the iteration process needs to create one new iterator object per iteration. This should be easy to optimize away as the old iterator of exactly the same type is immediately discarded. You can also safely\u00a0reuse the iterator as many times as necessary, even starting in the middle using one of the intermediate immutable tails.</p> <p>The one situation where this style could be a burden is when one needs an iterator that can only be iterated\u00a0once and which permanently discards the elements it has produced. I only say this because Scala must have the TraversableOnce trait for a reason, though I have never uncovered it.</p> <p>This style is the only one that succeeds on all points.</p>"},{"location":"blog/comparison-of-iteration-styles-in-programming/#internal-iterators","title":"Internal Iterators","text":"<pre><code>list.foreach(element =&gt; print(element))\n</code></pre> <p>An internal iterator is just a method, often called <code>foreach</code>, that accepts a function as its only argument which is then applied to each element of the collection. It moves the control of the iteration from the caller to the callee. As a consequence, the ability to break, continue, or otherwise alter the normal iteration is lost. In Scala, even the <code>for</code> loops are sugar for internal iterators, and thus lack break or continue.</p> <p>Internal iterators can coexist with an external ones.\u00a0Robert Nystrom\u00a0has a blog series\u00a0about why certain problems are harder or easier to solve external vs internal iterators. The subsequent posts talk\u00a0about ways to get the benefits of one in the other\u00a0and\u00a0more\u00a0about sentinel iterators, his favorite. It is a good series for those interested in learning even more about iterators.</p> <p>This is the only the style that fails #5 Controlability, but it succeeds on all other counts.</p>"},{"location":"blog/free-star-wars-for-everyone/","title":"Free Star Wars for Everyone","text":"<p>You got a robotic vacuum cleaner for Christmas. It is the cutest little thing\u2014circling around the room, diligently picking up confetti from the New Years Eve party the night before.\u00a0You just can't let all that cuteness go to waste. So you record the whole cycle on your phone\u2014the whole two hours and twenty minutes\u2014and post it to your Youtube channel. You have 5 subscribers, all spam bots. Your\u00a0video goes\u00a0on to get a million views because the pet name you gave to the vacuum matches the name of an politician recently embroiled in scandal, making \"&lt;politician's name&gt; Cleans House\" accidental\u00a0clickbait.</p> <p>Later that night, you sit down with a blanket and your vacuum to watch a movie you bought out of the $5 bin at Walmart. Like all the films from that mid-aisle bucket, this one is resoundingly entertaining. People thought that vacuum was great, just wait till they see this. You wait for the movie to end, then restart it and record the whole thing\u00a0with\u00a0your phone\u2014the whole two hours and twenty minutes\u2014and post it also to your channel. But before it gets even a single view, Youtube slaps a big ol'\u00a0\"This video contains content from &lt;conglomerate's name&gt;, who has blocked it on copyright grounds. Sorry about that.\"\u00a0They may have been polite about it, but that does not change the fact that they have blocked your attempt to share this incredible movie with all your friends. What's that? Well, with all your subscribers, then.</p> <p>You click around until you discover that such a takedown is disputable. The dispute menu provides a helpful list of possible reasons that you should be allowed to post the video, the first of which is \"I own the DVD\". You check that box only to discover that that option is a trick. Youtube politely laughs under its breath and tells you that owning the DVD is not sufficient to post a recording of the DVD. What is this?! You owned the vacuum and the internet killjoys had no problem with a two-hour shaky-cam rendition of \"&lt;politician's name&gt; Cleans House\". You own the DVD of \"&lt;90's franchise&gt; III\"; why is filming that any different?\u00a0Isn't there something called freedom of speech in this country?</p>"},{"location":"blog/free-star-wars-for-everyone/#copyright","title":"Copyright","text":"<p>Make no mistake. Copyright laws are a restriction on freedom of speech. But like a few other\u00a0restrictions, this one exists for a good reason. Creating original works\u00a0takes real time and talent, whether it is making a blockbuster movie, making a video game, or writing a novel. Consuming original works provides a real economic good to its consumers, whether watching that movie, playing that video game, or reading that novel.\u00a0Why can't creators sell their works like every other business, without the government-enforced restrictions on copying and distribution?\u00a0Someone sells gas down the street, and no law prevents someone else from selling identical gas right across the intersection. In fact, the law works the other way. Colluding to fix gas\u00a0prices\u00a0is the crime, but when it comes to original works, the government is the one enforcing a monopoly. What gives?</p> <p>In gasoline, the cost of developing the concept of gasoline plays no part in the cost today. Ignoring collusion higher up in the production chain, the cost of a gallon of gas is governed strictly the marginal cost of pumping out and refining it. Once you have created one gallon of gasoline, it still costs money to make the next 130 billion gallons. For original works, the cost structure is flipped. It costs tremendously to make the first copy of a novel, but it costs next to nothing to make a copy for every person on the planet.\u00a0If everyone had the freedom to copy any work of which they own a copy, then once one copy gets out, everyone else can get\u00a0his\u00a0copy for free. Thus, the opportunity to make a living off the selling of original works is incompatible with the liberty to distribute copies\u00a0of works to which one has access.</p> <p>A long time ago, we chose copyright as the mechanism by which creators would get paid. It is one of the delegated powers of the Constitution. If you produce an creative\u00a0work, the government gives you a copyright, and for a certain amount of time, only you have the right to make and sell copies of the work.</p>"},{"location":"blog/free-star-wars-for-everyone/#how-much-time","title":"How Much Time","text":"<p>There are alternative ways fund the creation of original works. The government could collect a tax and directly hire talented creators to produce works for the public. This is the BBC mechanism. But alternative mechanisms are not the point of this post. I am going to assume that creators receive copyrights. The question I want to explore is how long copyright should last? There is a sort of balance here. If it is too short, then creators will be unable to make enough money on their works\u00a0to motivate them to create the works\u00a0in the first place. If the time became especially short, consumers could wait out the copyright until it became free. If the time is too long, freedom of speech, especially the freedom to make derivatives of works, is hindered unnecessarily.</p> <p>The original copyright length in the USA starting in 1790 was 28 years. It was increased in 1831 to 42 years to give it parity with Europe. Then in 1909 it went to 56 years. Then in 1976 it went to life of author for 50 years. Then in 1998 it went to life of author plus 70 years or, if the work was made by a corporation, 95 years after publication. If you have not seen CGP Grey's history of copyright, I highly recommend it. Naturally, Wikipedia has a nice graph of this, too. Why was it increased? Were creators saying that they could not make a living selling the work in their lifetimes, so Congress increased the length of copyright to promote the creative arts? Obviously, that was not the case. This was a unadulterated example of special interests (Disney Corporation in the latest extension) buying special favors from Congress.</p>      Source Wikipedia. Image licensed under CC-BY-SA.  <p>It is impossible to compute an exact amount of time (e.g. 32 years 7 months) that balances the loss\u00a0from making copyright too long and the loss from making copyright too short. But there are questions that we can ask to help us find a balance.\u00a0How long does it take for a work to make most of the money it will ever make?\u00a0If everyone makes hardly any money\u00a0on their works beyond, say, 50 years, then allowing copyright beyond this term would serve no purpose. But this is not quite right. We should be asking, How long does it take for a work to make enough money to motivate the creator into making it? That is the\u00a0correct amount of time for copyright to exist. Just long enough to ensure the work gets created and not a minute longer. This is\u00a0tricky, of course, because the amount of time required for each work and creator is different, but for practical purposes, the law has to apply evenly to everyone.</p> <p>If someone can find a study that actually measures the drop-off in sales of copyrighted work over time, I will link it here. The information about long term sales is either well guarded by the publishers, or no one except a dark-backgrounded blogger cares about it.\u00a0Let's see if we can get a feeling\u00a0by going back 30 years to my birthyear 1985. The top movie was Back to the Future. Ok, people probably still watch that movie. You can rent in on Amazon for $4 or get the DVD through Netflix for $7/month. But I had never even heard of #2 The Goonies or #3 The Breakfast Club. I have seen #4 Clue, but none of\u00a0the rest of the list. Maybe people who were not in utero at the time of the release are willing to pay to see these movies.\u00a0The important question is, Would any of these movies have been cancelled\u00a0if the studio knew that they would lose all additional\u00a0revenue starting in 2015. The answer to that is obviously no. So for movies anyway, 30 years is plenty long enough for copyright.</p> <p>And that's my main argument. The original length of copyright, 28 years, was about right. Let's round it off to the nearest decade and set the length to copyright to 30 years after first publication.</p>"},{"location":"blog/free-star-wars-for-everyone/#does-it-matter","title":"Does it Matter?","text":"<p>CGP Grey's video, if you have not watched it yet, gives a great explanation of how long copyrights hurt culture. Culture progresses through mutation and modification. Part of this is retelling existing stories from a modern angle. Long copyrights slow down this mutation rate.\u00a0We never see the\u00a0creative works that\u00a0never exist because of\u00a0copyright. Copyright has gotten so long we often miss this. The movie Frozen is based on The Snow Queen published by Hans Christian Andersen in 1845. That may seem like a long time ago, but only works published before 1922 are out of copyright. We won't get another shot at a Hobbit movie until 2043 (1973+70).\u00a0No one alive will ever see the Star Wars prequels made properly. These things are important not because the works were good, but because they were culturally important. Any\u00a0value the Star Wars prequels may have today was not created by George Lucas and his team, but quite the opposite, by the audience itself. I went to see the Hobbit, not because Peter Jackson made a good trilogy, but because everyone else was talking about it. The people who make creative works would like us to believe that the value of a work was created almost entirely by them. This may be true for short lived pieces of entertainment, but this becomes increasingly untrue as time marches on. As a work is referenced in conversation or interpreted by the audience, the value starts to be encapsulated in the conversation and interpretation itself. The value is imparted by the people for the people. To give a perpetual monopoly of these ideas to the original creator is to infringe on the natural rights of his audience.</p> <p>The effect on culture is profound. As printing has become\u00a0cheaper, the number of books published per year has\u00a0exploded. In the Amazon catalog, you can see this exponential rise in available books starting the middle of the 1800s to the beginning of the 1900s.\u00a0But then there is a cliff in the 1920s. That's the copyright wall, 95 years after publication. If you want to study or read books from\u00a0the 1910s or the 1890s, you have that entire world available to you for free. If you want to study books from 1930s or the 1960s, you are out of luck. Most of those books are effectively gone; they are not available at any price. They are books still under copyright but not worth enough to sell or even to track down the current copyright holders.</p>      Source Offsetting Behavior citing research by Paul Heald.  <p>The benefit to Disney of long copyrights is actually pretty small, but enough to justify a few lobbyists and a handful of discrete payments to Congressmen. The only way forward is for average voters to recognize and get others to recognize the importance of short copyrights. The cultural argument is more important in my opinion, but it may be easier to appreciate the number of things you would get for free if copyrights were 30 years rather than their current infinity. You get free Star Wars, at least the good ones. You get free Lord of the Rings books. You get all those Disney movies from the 1950s, 1960s, and 1970s. You get remakes of dated stories. Not the terrible reboots authorized by the current copyright holders, but a fresh look from someone who simply has a good idea. I am supportive of\u00a0helping people acquire great wealth through the\u00a0making of great works of art, but we have no business helping\u00a0people\u00a0acquire wealth by assigning\u00a0them monopolies on culturally important works of art created by founders and ancestors who physically or creatively died a long time ago. Let's have\u00a0free Star Wars for\u00a0everyone.</p>"},{"location":"blog/difficulty-of-pattern-matching-syntax/","title":"Difficulty of Pattern Matching Syntax","text":"<p>Pattern matching is compact syntax, originating in functional programming languages, that acts\u00a0a super-charged switch statement, allowing one to concisely branch on the types and components\u00a0of a value\u00a0of interest. Like a traditional switch statement, a pattern match takes a single object and compares it to sequence of cases, running the code body associated with the matching case. There are many parts to a pattern matcher. and design concise and unambiguous syntax is a difficult endeavor, one failed by many popular\u00a0programming languages.</p> <p>There are five\u00a0things that pattern matchers can do:</p> <ol> <li>Equality: match\u00a0if the value is equal to another particular value</li> <li>Instance: match\u00a0if the value is an instance of a particular type</li> <li>Destructuring:\u00a0split the value up into its components\u00a0and match on each one</li> <li>Assignment: assign the value to a name</li> <li>Otherwise: match any value</li> <li>Guards: after-the-fact test an arbitrary boolean expression</li> </ol>"},{"location":"blog/difficulty-of-pattern-matching-syntax/#otherwise-_","title":"Otherwise (_)","text":"<p>In some sense, the default case is the simplest. This is the pattern that matches any\u00a0value. It is often given its own keyword, such as <code>default</code> or <code>otherwise</code> or <code>else</code>. In other languages, it gets its own symbol, such as <code>*</code> or <code>_</code>. In languages with full pattern matching, such as Scala, Haskell, and F#, <code>_</code> is\u00a0most common, which is what will be used in this post.</p> <pre><code>print(\"What happened today?\")\nresponse = read()\n\nswitch response:\n  _:\n    print(\"And how does that make you feel?\")\n</code></pre> <p>Because the rest of the tests can be combined, this really represents an empty list of tests. The best syntax for this may be simply listing nothing. The nothingness starts to become a little too invisible for my taste.</p> <pre><code>switch response:\n  :\n    print(\"And how does that make you feel?\")\n</code></pre>"},{"location":"blog/difficulty-of-pattern-matching-syntax/#equality-eq","title":"Equality (eq)","text":"<p>With equality testing, a case is a match if the value of interest is equal to the value supplied in the case. In a switch statement, this is all there is. Because there are so many kinds of tests in full pattern matching, this post will use a two letter keyword to disambiguate\u00a0each kind. For the\u00a0equality test, the keyword <code>eq</code> will be used.</p> <p>Here is an example of a pattern match used only like\u00a0a switch statement:</p> <pre><code>print(\"Pick a number between 1 and 10\")\nnumber = Integer.parse(read()).or_die();\n\nmagic_number = random.integer(1,10)\n\nswitch number:\n  eq 0:\n    print(\"I said 1 and 10, not 0 and 9!\")\n  eq magic_number:\n    print(\"You guessed my magic number!\")\n  _:\n    print(\"You did not guess it.\")\n</code></pre>"},{"location":"blog/difficulty-of-pattern-matching-syntax/#instance-is","title":"Instance (is)","text":"<p>The first kind of pattern matching that you\u00a0will not see in non-functional\u00a0programming languages is instance testing\u2014testing that the value is an instance of a given type. By itself, this is really only useful if the language also has union types and flow-sensitive static typing. Say variable <code>x</code> has type <code>Integer|String</code>. If a case tests for <code>x</code> being type <code>String</code>, then the body will only be executed when <code>x</code> actually is a <code>String</code>, making it safe to assume <code>x</code> has a static type of <code>String</code> within the body of the case.\u00a0Ceylon, with its flow-sensitive typing and union types, is the only language I have found that has this kind of test, but not full pattern matching. This post will use the keyword <code>is</code> to indicate instance tests.</p> <pre><code>def double(x: Integer|String):\n  switch x:\n    is Integer:\n      return 2*x\n    is String:\n      return x.concatenate_back(x)\n</code></pre> <p>It is rarely\u00a0useful to test with <code>eq</code> and <code>is</code> in the same expression, because the type of the value being tested with <code>eq</code> is usually known.\u00a0This assumes that the pattern matcher does not throw an error when comparing two incomparable types like String and Integer. In the example below, <code>is Integer</code> is pointless if <code>eq 0</code> is false for all values of <code>x</code> that are not <code>Integer</code>s. With implicit conversions, this could occasionally not be true.</p> <pre><code>switch x:\n  # Don't write code where `is Integer` is meaningful here\n  is Integer eq 0:\n    print(\"It's really zero\")\n  _:\n    pass\n</code></pre>"},{"location":"blog/difficulty-of-pattern-matching-syntax/#destructuring-to","title":"Destructuring (to)","text":"<p>A\u00a0quintessential part of pattern matching is destructuring.\u00a0Destructuring breaks the value into components\u00a0and then pattern matches on each of\u00a0them. It is the recursive construct\u00a0of pattern matching, but is not really a test in itself. The entire pattern matches only if each component matches its pattern. The value must be of a type that can be destructured. It is a type error to attempt to destructure a value that cannot be interpreted in this way, so usually destructuring is preceded by\u00a0an instance check.\u00a0This post\u00a0will use the construct <code>to (...,...)</code> to perform destructuring.</p> <p>Note the patterns testing for equality inside the destructuring construct.</p> <pre><code>class Point(x, y)\norigin = Point(0.0, 0.0)\n\nclass Line(b, m)\n\nif random.boolean():\n  shape = Point(3.0, 0.0)\nelse:\n  shape = Line(0.0, 1.0)\n\nswitch shape:\n  eq origin:\n    print(\"Point at origin\")\n  is Point to (eq 0.0, _):\n    print(\"Point on x-axis\")\n  is Point to (_, eq 0.0):\n    print(\"Point on y-axis\")\n  is Line to (eq 0.0, _):\n    print(\"Line through origin\")\n  _:\n    print(\"Something boring\")\n</code></pre>"},{"location":"blog/difficulty-of-pattern-matching-syntax/#assignment-as","title":"Assignment (as)","text":"<p>Assignment is not a test at all, but binds\u00a0the value of a component\u00a0and assigns it to a name. It is nearly\u00a0useless without destructuring, because\u00a0the base value is the\u00a0value being tested, which is usually assigned to a name already. The values bound\u00a0are usually the components of a destructured value. \u00a0This post will use the keyword <code>as</code> to indicate a pattern of\u00a0binding a value to\u00a0a name. Other possible keywords are <code>val</code> or <code>let</code>.</p> <p>The values bound as usually referenced\u00a0in the body of the case.</p> <pre><code>switch shape:\n  eq origin:\n    print(\"At origin\")\n  is Point to (eq 0.0, as y):\n    print(\"On x-axis at {y}\")\n  is Point to (as x, eq 0.0):\n    print(\"On y-axis at {x}\")\n  is Line to (eq 0.0, as m):\n    print(\"Line through origin with slope {m}\")\n  _:\n    print(\"Something boring\")\n</code></pre>"},{"location":"blog/difficulty-of-pattern-matching-syntax/#guards","title":"Guards","text":"<p>Guards can\u00a0seem like an afterthought to pattern matching because their behavior is so\u00a0orthogonal. Guards are the front half of an if statement appended onto the end of a case. The case is only a match if the expression following the <code>if</code> keyword evaluates to <code>true</code>. Usually, the guards reference variables bound\u00a0via <code>as</code>.\u00a0Guards are strictly more powerful than <code>eq</code> alone because it allows us to compare destructured components to each other or switch on the members of components or\u00a0apply arbitrary functions to components such as this:</p> <pre><code>switch shape:\n  as point is Point if distance_from_origin(point) &gt; 1:\n    print(\"Point outside unit circle\")\n  is Point to (as x, as y) if x == y:\n    print(\"Point on 45 degree radius\")\n  _:\n    pass\n</code></pre>"},{"location":"blog/difficulty-of-pattern-matching-syntax/#other-uses-of-pattern-matching","title":"Other Uses of\u00a0Pattern Matching","text":"<p>Other than in a switch-like construct, pattern matching is used in overloading and variable assignment.</p> <p>Some programming languages use pattern matching to define overloaded functions. Once famous example is using it to define the factorial function.\u00a0Because there is no inherent order\u00a0to the overloaded methods, there needs to be some sort of overload priority resolution that says that <code>factorial(0)</code> calls the correct method even through <code>0</code> matches both.</p> <pre><code>def factorial(0):\n  return 1\ndef factorial(n):\n return n * factorial(n-1)\n</code></pre> <p>Note that this is just syntactic sugar for a single, non-overloaded function that pattern matches its parameters.</p> <pre><code>def factorial(n):\n  switch n:\n    eq 0:\n      return 1\n    _:\n      return n * factorial(n-1)\n</code></pre> <p>Some programming language also allow values to be pattern matched\u00a0on assignment. Even Python, which does not even have a switch statement, allow tuple destucturing like this: <code>x, y = my_tuple</code>. When doing an assignment it never makes sense to have tests (what would happen if they failed?). Therefore, assignment pattern matching is restricted to destructuring, \u00a0assignment, and otherwise.</p> <pre><code># Swap\nto (x, y) = [y, x]\n\n# Extract just the x\nto (x, _) = point\n</code></pre>"},{"location":"blog/difficulty-of-pattern-matching-syntax/#simplification","title":"Simplification","text":"<p>If you have ever used pattern matching in programming, you will notice that this syntax contains a lot more keywords than anything you have ever seen. There are six keywords. Can that be reduced a little bit? Is there any redundancy in the syntax? The short answer is that it can be reduced by one keyword without losing features or becoming ambiguous. The long answer is that most programming languages reduce it even more resulting in ambiguity and complex rules to resolve the ambiguity.</p> <p>Without losing any ambiguity, we can allow one kind of shorthand when an expression starts a pattern without a keyword. How is this bare expression interpretted so that the code can be made terser? Below\u00a0are some possibilities.</p>"},{"location":"blog/difficulty-of-pattern-matching-syntax/#eq-only","title":"eq only","text":"<p>In C#, C++, and other non-functional languages, the\u00a0switch statement only does equality testing, so a keyword is not required.</p> <p>In Ceylon,\u00a0the switch statement does not do destructuring, assignment, or guards and\u00a0assignment only allows destructuring and assignment. In Ceylon, a bare expression in a switch is interpreted as an equality test and a bare expression in an assignment is interpreted as an assignment.</p>"},{"location":"blog/difficulty-of-pattern-matching-syntax/#eq-on-literals","title":"eq on literals","text":"<p>In every language I have seen, <code>eq</code> is dropped on literals.\u00a0Because <code>is 0.0</code> and <code>as 0.0</code> make no sense, we know that a bare <code>0.0</code> must refer to <code>eq 0.0</code>. In fact, most languages provide no mechanism for equality testing other than literals\u2014the general <code>eq</code> operation does not exist. Languages with full pattern matching allow guards to be used in place of equality testing against variables; a value to be tested is captured in a name and then tested for equality in the guard.</p> <p>This may not be a good idea for one special reason: referential transparency. Referential transparency means that an expression can be\u00a0replaced by its value. In a referentially transparent language, <code>f(2.0)</code> is always the same as <code>x=2.0; f(x)</code>. If we allow for the <code>eq</code> to be dropped before\u00a0literals, then\u00a0replacing those literals with a name of the same value will result in an error. Being able to rely on referential transparency makes it easier to reason about how a programming is working. Nevertheless, dropping <code>eq</code> on literals is a popular choice. Scala, F#, Haskell, and Rust all sacrifice referential transparency to make matching literals easier.</p> <p>The following two definitions are equivalent, showing how <code>eq</code> can be eliminated entirely.</p> <pre><code>def is_zero(x: Integer):\n  eq 0:\n    return true\n  _:\n    return false\n\ndef is_zero(x: Integer):\n  zero = 0\n  as temp if temp == zero:\n    return true\n  _:\n    return false\n</code></pre>"},{"location":"blog/difficulty-of-pattern-matching-syntax/#as-outside-pattern-matching","title":"as outside pattern matching","text":"<p>Programming languages that allow pattern matching\u00a0in assignment\u00a0usually have bare identifiers\u00a0interpreted as assignments.\u00a0Instance testing, equality testing, and guards make no sense in this context so there no chance of ambiguity.</p>"},{"location":"blog/difficulty-of-pattern-matching-syntax/#is-and-to-combo","title":"is and to combo","text":"<p>It may be tempting to drop <code>to</code> also in addition to <code>eq</code> or <code>is</code> or <code>as</code>, especially if <code>(something, something)</code> is not a legal expression\u00a0in the language. This cannot be done ambiguously, because the one-element destructuring <code>(something)</code> is undoubtedly legal syntax as simple unnecessary parentheses.\u00a0It is clear that <code>(A, B)</code> means <code>to (A, B)</code>, but does <code>(A)</code> mean <code>to\u00a0(A)</code> or <code>is A</code>? Technically, it is the parentheses that are unnecessary, but who wants to write <code>Point to x, y</code>?</p> <p>We could take advantage of the fact that destructuring is almost useless without an instance check beforehand. Let's try\u00a0a hybrid approach and interpret all patterns starting with <code>Name(...)</code> as <code>is Name to (...)</code>. This is not the same as\u00a0dropping <code>is</code> entirely because <code>Name(...)</code> could be a function call returning either a type, unless types are not first class objects in the language and cannot be returned from functions.</p> <p>This syntax is almost universal among functional programming languages because it is almost entirely how algebraic data types are used in pattern matching. For example, here is how one would use the <code>Option</code> data type:</p> <pre><code>print(\"What's your favorite number?\")\ninteger = Integer.parse(read())\n# Type of integer is Option[Integer]\n\nswitch integer:\n  Some(7):\n    print(\"That's my favorite number too!\")\n  Some(as value):\n    print(\"I am ok with {value}\")\n  None:\n    print(\"That's not a number!\")\n</code></pre>"},{"location":"blog/difficulty-of-pattern-matching-syntax/#lowercase-as-and-uppercase-is","title":"lowercase as and uppercase is","text":"<p>By using the <code>is</code>/<code>to</code> combo and restricting <code>eq</code> to literals and guards, we can get rid of\u00a0the <code>is</code>, <code>to</code>, and <code>eq</code> keywords, leaving only <code>as</code>. Even with <code>eq</code> gone, we cannot get rid of <code>as</code> without introducing ambiguity. When we write <code>A</code>, does that mean <code>is A</code> or <code>as A</code>. Nevertheless, most of the full-featured pattern matching languages eliminate it, introducing some\u00a0of the weirdest corner cases in programming history. They parse a lowercase identifier (like <code>name</code>) as a binding and parse an uppercase identifier (like <code>Name</code>) as an instance test. In languages where types must begin with a capital letter, like Haskell and Ceylon, it\u00a0is not a big deal. However, in languages\u00a0where this is the only place where\u00a0capitalization matters, like Scala and F#,\u00a0it is particularly jarring.</p>"},{"location":"blog/difficulty-of-pattern-matching-syntax/#type-is-and-object-eq","title":"type is and object eq","text":"<p>One possibility that I have never implemented is to switch between the behaviors of <code>is</code> and <code>eq</code> based on whether or not the pattern is a type. If the name refers to a type, then it is likely the user intends to test if the value is an instance. Testing if a value is equal to a type is pretty rare. Similarly, if the name refers to an object that is not a type, then it is likely the user intends to test for equality, as an instance test\u00a0would be an error.</p> <pre><code>a = 0\nb = Integer\n\nswitch obj:\n  a: # eq a\n    print(\"Found the zero\")\n  b: # is b\n    print(\"Nonzero\")\n</code></pre> <p>This can be interpreted as requiring a type and always doing an <code>is</code> comparison, but having an implicit conversion from any object to a singleton type. It is a good principle to avoid\u00a0general implicit conversions because it easily masks errors. Scala learned this the hard way by including a conversion from Any to String.</p> <p>This could also be done based on the runtime type of the pattern, but that would be even weirder. A pattern that is usually compared via <code>eq</code> would suddenly compare via <code>is</code> if the object happened to also be a type.</p>"},{"location":"blog/difficulty-of-pattern-matching-syntax/#type-is-and-object-eq-and-undefined-as","title":"type is and object eq and undefined as","text":"<p>An extension to the previous technique would be to use <code>as</code> whenever the name was undefined, because that is the only right thing to do when the name has no value.</p> <pre><code>a = Some\nb = none # an object\n\nswitch obj:\n  a(c): # is a to (as c)\n    print(\"Some contains {c}\")\n  b: # eq none\n    print(\"None\")\n</code></pre> <p>This solution has the additional problem that defining a top-level class or other object would silently and subtly alter the behavior of pattern matches using that name as assignment.\u00a0For saving characters, this solution is a dream. For saving one's sanity, not so much.</p>"},{"location":"blog/difficulty-of-pattern-matching-syntax/#conclusions","title":"Conclusions","text":"<p>It is kind of difficult to render a judgment on this one. The full syntax is too verbose for simple cases, but the choices made by other programming languages lead to weird corner cases.\u00a0I think the <code>is</code>/<code>to</code> combo is a great choice for the behavior of a bare expression. And despite the loss of referential transparency, I think equality should be applied to literals. This is also because literals interact differently with an exhaustiveness checker than variables with the same value. An exhaustiveness checker is something that I did not talk about, but it is an important aspect of pattern matching. I think all six keywords should be retained to allow for unambiguous definitions when necessary.</p> <p>Testing for equality of non-literals is actually pretty rare (because it interacts poorly with exhaustiveness checking), so shortening the syntax for it seems wasteful. In fact, by having the <code>eq</code> keyword, I already make it shorter than most other languages which require that equality testing be done with guards.</p> <p>Under no circumstances would I choose to have the case of the first letter be significant. Users of any language I design\u00a0will have to suck it up and use a keyword to assign a value. I am not sure why this is considered so burdensome. In all languages with both block scoping and mutability, the programmer has to use a keyword (like <code>val</code> or <code>var</code>) to declare every other variable in the program. Why not those declared in pattern matchers? In my world, pattern matching would look something like this:</p> <pre><code>switch shape:\n  eq origin:\n    print(\"At origin\")\n  Point(0.0, val y):\n    print(\"On x-axis at {y}\")\n  Point(val x, 0.0):\n    print(\"On y-axis at {x}\")\n  Point(val x, val y) if x == y:\n    print(\"Point on diagonal\")\n  Line(0.0, val m):\n    print(\"Line through origin with slope {m}\")\n  _:\n    print(\"Something boring\")\n</code></pre>"},{"location":"blog/one-sided-debate-over-sequence-syntax/","title":"One-Sided Debate over Sequence Syntax","text":"<p>Computers are famously stupid machines. You have to tell them in perfect detail not just what you want them to do but how to do it. A computer may be able to add 1 and 2 faster than I can, but it will take me longer to tell it to do that\u00a0than for me to do it myself. The more complex the task, the more time it takes to code. Coding is still laborious and entirely not worth it unless such code will be used many times. I posit that a computer is only useful for doing work when the vast majority of the work to be done is repetitive tasks on simple objects. The most common abstraction for representing a bunch of objects is the sequence (also known as\u00a0a\u00a0list or array), in which each object in the collection is associated with an\u00a0integer called its index.\u00a0There is a wide diversity of syntax and semantics for accessing and changing sequences.</p>"},{"location":"blog/one-sided-debate-over-sequence-syntax/#indexing-with-or","title":"Indexing with <code>()</code> or <code>[]</code>","text":"<p>Getting things out of a sequence is essential. In fact, programming languages almost universally have special\u00a0syntax for this operation. If <code>seq</code> is a sequence and <code>i</code> is an integer, then usually either <code>seq(i)</code> or <code>seq[i]</code> is an expression that returns an\u00a0element of\u00a0the array.\u00a0Which element it returns is strangely a matter of debate discussed in the next section. The use of <code>[]</code> usually appears\u00a0in languages descended from\u00a0C and the use of <code>()</code> appears in languages either descended from Fortran or designed by people with a penchant for minimalism and abstraction. Those who are familiar with using <code>[]</code> may be wondering what the syntax is for function calls if <code>()</code> is already taken. It is the same for both; Matlab and Scala all use <code>()</code> for both function calls and array access. This is not ambiguous at all.\u00a0Like any operation on an object, the behavior depends on the type.\u00a0Just as some languages uses <code>+</code> for both integer addition and string concatenation, some languages use <code>()</code> for both function calling and sequence indexing.</p> <p>By abstracting away the interfaces for function calling and sequence access, the designer gives the programmer the ability to his own objects that behave like functions and sequences.\u00a0Once they are abstracted away, there is little reason to keep separate syntax for <code>()</code> and <code>[]</code>. Python is one of the few languages that allows the user to implement his own versions of <code>()</code> (<code>__call__</code>) and <code>[]</code> (<code>__getitem__</code>). I have never seen a class use both in a way that could not be better designed to use just one of them.</p> <p>If I were designing a language, I would reserve <code>[]</code> for doing more useful things like applying type parameters (e.g. <code>sum[Integer](my_numbers)</code>) and leave <code>()</code> to pull double duty on functions and sequences.</p>"},{"location":"blog/one-sided-debate-over-sequence-syntax/#indexing-with-0-or-1","title":"Indexing with 0 or 1","text":"<p>If you have a sequence <code>seq</code> equal to <code>[5,6,7,8]</code>, does <code>seq(1)</code> return 5 or 6? \u00a0If you have only used languages where it returns\u00a0the first element 5, you may be surprised that not only are there languages where <code>seq(1)</code>\u00a0returns the second element 6, but that it is actually more common to return the second element than the first, and that there are people out there who strongly believe in\u00a0this design. Unlike the first section, which describes\u00a0diversity that many people don't notice, the first\u00a0index\u00a0is a highly contentious debate. The underlying difficulty is that humans prefer 1 to be the first element (called 1-indexing) and computers\u00a0prefer 0 to the be first element (called 0-indexing).</p> <p>The most famous argument in favor of 0-indexing was made by Dijkstra. But I think a\u00a0better argument is a practical one: when the sequence represents a multidimensional array, finding the <code>[i,j]</code> element is quite easier when everything is 0-indexed. Consider this example with a two-dimensional array embedded in a sequence:</p> <pre><code># Array with values stored in column-major order\nn = 3 # three rows\nm = 3 # three columns\nA = [1,2,3,4,5,6,7,8,9]\n# Interpreted as\n# 1,2,3\n# 4,5,6\n# 7,8,9\n</code></pre> <pre><code># 1-indexing\ni = 2 # second row\nj = 3 # third column\nA((i-1)*n + j)\n</code></pre> <pre><code># 0-indexing\ni = 1 # second row\nj = 2 # third column\nA(i*n + j)\n</code></pre> <p>Notice how cleaner\u00a0the code is inside the indexing when using 0-indexing. It becomes\u00a0uglier as the number of dimensions increases. If you are thinking that is not worth the\u00a0silly definitions of <code>i</code>, <code>j</code>, and <code>k</code>, then we are thinking the same thing. It would be much better to actually structure the data so that it\u00a0does the math for the programmer:</p> <pre><code># Sequence of sequences\nA = [[1,2,3],[4,5,6],[7,8,9]]\n\ni = 2 # second row\nj = 3 # third column\nA(i)(j)\n\n# 2D array\nB = [1,2,3;4,5,6;7,8,9]\nB(i,j)\n</code></pre> <p>When using a sequence of sequences or a full-fledged multidimensional array, there is no need to store the number of rows and columns in variables\u00a0because that information is contained in the object itself. Once the linear indexing no longer has to be calculated\u00a0by hand, the advantages of 0-indexing falls away leaving the far more natural 1-indexing to prevail.</p>"},{"location":"blog/one-sided-debate-over-sequence-syntax/#mutating-or-updating","title":"Mutating or updating","text":"<p>Every language not only has a way to access an element of the sequence but also a way to change an element of the sequence.\u00a0The syntax for changing a sequence is deceptively similar across languages. Most languages have something like looks like <code>seq(1) = 2</code> which conceptually replaces the first element with the object <code>2</code>.\u00a0Depending on the language, it will actually mean one of these three things:</p> <ol> <li>Mutate. Replace\u00a0the first element of the sequence object\u00a0with<code>2</code>. All previous references to the object now see a new sequence.</li> <li>Alter. Make a copy of the sequence\u00a0with the first element changed to <code>2</code> and then repoint the name <code>seq</code> to this copy.\u00a0The local name <code>seq</code> is now a new sequence but other references to the sequence remain unchanged.</li> <li>Update. Be an expression whose value is a copy of the\u00a0sequence\u00a0with the fist element changed to <code>2</code>. No references are changed, but the user can assign this value to a name or perform other operations on it.</li> </ol> <p>I just made up all those names to help with the discussion. The procedural programming languages, like C and Python, use the Mutate interpretation. Replacing an element in these languages has far-reaching consequences. The scientific programming languages, like R and Matlab, use the Alter interpretation. It looks like sequences are mutable in these languages when looking at Matlab or R, but in fact, all sequences\u00a0are immutable (or assignment always makes a copy). For those not familiar with one of the syntaxes, here is an example of each:</p> <pre><code>A = [1,2,3]\nB = A\n\nA(1) = 5\n\nprint([A(1), B(1)])\n# Mutate prints [5,5]\n# Alter prints [5,1]\n# Update prints [1,1] (value was discarded)\n</code></pre> <p>In how one uses them, Mutate and Alter are very similar. In what they actually do internally, Mutate and Update are almost exactly the same. But Mutate and Update have something in common that Alter does not have\u2014Mutate and Update can be replaced by method calls. Mutate is a void method\u00a0that simply has the side effect of changing the object (e.g. <code>void\u00a0mutate(Element, Integer)</code>), while Update returns a value of the same type (e.g. <code>def update(Element,Integer): [Element*]</code>). Alter requires help from the language to actually complete its task because it repoints a name (like an assignment) after calling the update function. Below is how the language may desugar the <code>A(1) = 5</code> line from above.</p> <pre><code>A.mutate(1,5) # Mutate\nA = A.update(1,5) # Alter\nA.update(1,5) # Update\n</code></pre> <p>The only language that I have found that actually uses the Update syntax is Scala when using an immutable type; mutable types interpret that syntax as mutate, which can be a little confusing. The Update syntax is a little awkward to actually use, which is probably why it is so rare. Notice how it does nothing in the example. To actually do something, one has to assign it to a name like <code>C = A(1) = 5</code>, whose two\u00a0equal signs are a little weird.</p> <p>The conflict here is between mutability and immutability. Unlike the other debates, this is one where both sides have merit. There are times when\u00a0immutable objects are useful and times when mutable objects are useful. One of the nice things about Scala is that it allows these two paradigms to be used side by side. Scala is the only language I have found that provides syntax for two of the three possibilities, even though the syntax is a little clunky for update.</p> <p>What syntax should be used? Here are some possible syntaxes:</p> <pre><code>A(i,j) = a\nA(i,j) &lt;- a\nA(i,j) := a\n\nA.(i,j) = a # etc.\n\nA{(i,j) = a} # etc\n</code></pre> <p>Alter is special in that it is the only one that requires reassignment. Because of this, I think whatever syntax is used for normal\u00a0assignment (an <code>=</code> in most programming languages), that same symbol should be used for Alter. \u00a0Let's assume that I get fancy\u00a0so that <code>A\u00a0\u2190 [1,2,3]</code> is the syntax for assigning<code>[1,2,3]</code> to the name <code>A</code>. This would make <code>A(1) \u2190 5</code> the natural choice for Alter.</p> <p>Mutate can be translated to a method call, but we should still choose some nice syntax for it. I guess I am partial to <code>A(1) := 5</code> for mutation. There is a case to be made for using <code>=</code>, the most common syntax, for mutation, the most common interpretation.\u00a0My hesitance is that <code>=</code> for assignment was a bad choice for assignment in the first place. The <code>=</code> sign is much more natural\u00a0at acting as an equality operator. But let's not get into that here.</p> <p>Update can also be translated to a method call, but it too deserves some nice syntax. I am partial toward something like <code>A{(i,j) \u2190 a}</code> because it is clear that there is no reassignment of the name <code>A</code> going on here. It also naturally allows for more than one update to be done at once, like <code>A{(i,j)\u00a0\u2190 a; (k,l) \u2190 b}</code>. Considering that the sequence\u00a0will have to be copied for each update, this may result in some easy computational savings. This may seem silly for sequences, but if we extend this to structures, like <code>A{field1\u00a0\u2190 a; field2 \u2190 b}</code>, the utility becomes clearer.</p>"},{"location":"blog/when-sequences-go-bad/","title":"When Sequences Go Bad","text":"<p>In my last post, I talked about the\u00a0various kinds of syntax for getting and setting elements in sequences. This post will talk about semantics.\u00a0What exactly should <code>get</code>\u00a0and <code>mutate</code>\u00a0do when invoked?\u00a0What should happen when the index is valid is hopefully obvious. But because we have to handle the case of an invalid index\u2014in particular, an\u00a0index larger than the length of the sequence, the answer is not as clear-cut as it may seem.\u00a0If \"throw an exception\" is the only thing that comes to mind, you have been stuck in\u00a0procedural programming for too long.</p>"},{"location":"blog/when-sequences-go-bad/#return-value-of-get","title":"Return value of <code>get</code>","text":"<p>Whether mutable or immutable, the getting process is the same. Let's ignore the possibility that <code>seq(i)</code> is not the right tool for your job and that something like <code>map</code> or a <code>for</code> comprehension would be better suited for the problem. For many reasons, using an index to get an item is necessary. Unless your programming language is either not Turing complete or its compiler has solved the halting problem, there is always the possibility that getting an item via its index will be outside the bounds of the sequence at runtime. What should happen, ignoring the infamous return-a-random-value-from-memory?</p> <ol> <li>Throw an exception</li> <li>Return null</li> <li>Return an optional value</li> </ol> <p>I am not a fan of using exceptions for non-exceptional cases. And an index being out-of-bounds is not exceptional\u2014it is to be expected. An exception\u00a0makes it too easy to ignore the missing case while writing the code. The type system, or at least the design, should encourage handling the out-of-bounds case. This is how Scala does it, probably for nostalgia purposes given its otherwise attention to safety.</p> <p>Returning <code>null</code> is an attractive idea. It is how Ceylon does it. For a collection of type <code>Seq[A]</code> the return type of <code>get</code> is <code>A|Null</code>, which forces the user to handle the <code>null</code> case. But there is minor difficulty with this: what if the sequence\u00a0itself contains <code>null</code>? If I ran <code>seq(i)</code> and get back <code>null</code>, how could I tell the difference between there being a <code>null</code> at point <code>i</code> in the sequence\u00a0and <code>i</code> being larger than the length? I couldn't\u2014the value is <code>null</code> is both cases. Maybe it doesn't matter because I want to handle both cases the same. Maybe the collection is typed so that it can't have <code>null</code>s.\u00a0But if I do care and it can happen, then I am stuck. This makes this a suboptimal choice for a collections library, which needs to handle sequences of arbitrary objects.</p> <p>We can escape the confusion of <code>null</code> by returning a optional value\u2014returning <code>Some(value)</code> if we found a value and returning <code>None</code> if not value was found. Notice how a sequence\u00a0containing a <code>None</code> is handled seamlessly. If <code>seq(i)</code> is <code>None</code>, then <code>Some(None)</code> is returned, which is different from <code>None</code> itself. Like in my iterators post, I would go with this, the safest option.</p>"},{"location":"blog/when-sequences-go-bad/#return-value-of-mutate","title":"Return value of <code>mutate</code>","text":"<p>Some\u00a0languages\u00a0(Ceylon, Haskell) don't even have <code>mutate</code> syntax. It\u00a0is not a construct of functional programming, so it is difficult for me to have strong of feelings about it. \u00a0It\u00a0is always\u00a0called for its side effect\u2014mutating the sequence\u2014rather than for its return value. Nevertheless, having a return value is not unheard of, so what should it be?</p> <ol> <li>Required to have no return value</li> <li>Required to have a return value</li> <li>Not required either way</li> </ol> <p>In many\u00a0languages, mutation is a built-in construct, not syntactic sugar over a method of the sequence. This means that the return value and not just the return type\u00a0of mutation\u00a0is defined\u00a0by the language itself and not just a constraint\u00a0or convention. In C, C++, Java, and C#, the value of mutation is always the element, the right-hand side. In Matlab, alteration is a statement with no value at all. Most languages seem to not care either way. Python, whose mutation syntax <code>array[1] = 2</code> is a statement with no value, allows user implementations of the desugared method <code>__setitem__</code>\u00a0to return any value, which is quietly discarded, though the convention\u00a0appears to be to return<code>None</code>. I have yet to find a language that both let's its users define their own mutation methods,\u00a0yet constrains those implementations\u00a0to return no value (<code>void</code> or <code>Unit</code>) or to return a particular value (<code>Element</code> or <code>Seq[Element]</code>).</p> <p>If I were designing a language, I would discard the return value of <code>mutate</code> so that it formed\u00a0a statement, like Python. If someone really needed that return value, then he could call the underlying method the normal way.</p> <p>Let's assume that a language does want to return a value. If\u00a0so, what should that value be?</p> <ol> <li>The mutable\u00a0sequence if it succeeded and throws an exception if\u00a0it did not</li> <li>The element if it succeeded and throws if it did not</li> <li>The mutable sequence and silently does nothing if the mutation is not legal</li> <li>The element if it succeeded and silently does nothing if the mutation is not legal</li> <li>An option containing <code>Some(element)</code> if it succeeded or <code>None</code> if it failed</li> <li>An option containing <code>Some(element)</code> if it failed or <code>None</code> if it succeeded (\"Couldn't put it in, here's your element back!\")</li> <li>A flag (<code>True</code> or <code>False</code>) indicating whether or not the mutation succeeded</li> </ol> <p>I don't like the throwing options #1 and #2 for the same reasons I don't like it in <code>get</code>\u2014this is not exceptional behavior. The silent options (#3 and #4) are type safe but even worse. I can't imagine trying to\u00a0debug something like that.</p> <p>There is something both elegant and pointless with returning the <code>Option</code>. #5 quite symmetric with the <code>get</code> and there is something cute about #6, but\u00a0echoing the object just passed in undeniably redundant. The first four options (echoing the mutable sequence or the element) are also redundant, but they mirror what the\u00a0C languages do.</p> <p>The only choice that is neither redundant nor unsafe is #7, though I have never seen this design in action. It likely because the languages that care about safety that comes from\u00a0removing exceptions also care about the safety that comes\u00a0in removing mutability. In some sense, I have to agree with this sentiment. Maybe we\u00a0really shouldn't be using mutable sequences anyway. If\u00a0you are using them, this had better be an exceptional case already. Return nothing on success; throw an exception on failure; don't forget to check the index beforehand.</p>"},{"location":"blog/sane-equality/","title":"Sane Equality","text":"<p>Equality: every programming language has it, the <code>==</code>\u00a0syntax is as universal as <code>1+1</code>, and it works almost the same in every language.\u00a0When the left and right operands are the same type, equality is easy. No one questions that <code>1==1</code> evaluates to <code>true</code> or that <code>\"a\"==\"b\"</code> evaluates to <code>false</code>. This post is about what to do when the operands are different types. What should <code>1==\"1\"</code> be? Or what should <code>Circle(1,2,2)==Point(1,2)</code> be? Or what should <code>ColoredPoint(1,2,red)==Point(1,2)</code> be?</p>"},{"location":"blog/sane-equality/#defining-equality","title":"Defining Equality","text":"<p>When defining how equality should work, it is helpful to think about what equality means. If two values are equal, that means that one value can be substituted for the other in some sense. Some programming languages flagrantly disobey this rule; they will say that two values are equal even if they cannot be used in the same way at all. The classic member of this club is Javascript, which returns equals for numerous unrelated values.</p> <p>It may be tempting to restrict equality to absolute indistinguishability\u2014two objects\u00a0are equal only if they can replace each other in every possible\u00a0case. And for immutable values without inheritance, this is\u00a0a clean choice with little drawback. It can be generated automatically for user-defined\u00a0types. But as I'll show later, mutable objects\u00a0and values satisfying multiple types may actually need several\u00a0parallel definitions of equality. It is often not sufficient to ask if two objects are equal. One must ask: equal in what way?</p>"},{"location":"blog/sane-equality/#design-options","title":"Design Options","text":"<p>When two different types are compared, there are a few options available to the language designer:</p> <ol> <li>Evaluate to <code>false</code></li> <li>Convert\u00a0them to a common type</li> <li>Compare the bits in memory</li> <li>Throw an error</li> <li>Return a sentinel value</li> </ol>"},{"location":"blog/sane-equality/#unsafe-equality","title":"Unsafe equality","text":"<p>Most of the professional\u00a0managed\u00a0programming languages like Python, Java, C#, Scala, and Ceylon take option #1\u00a0all say that <code>1!=\"1\"</code>.\u00a0In a naive sense, this is entirely correct because an integer and a string are not even comparable. But if they are incomparable, why allow this construction at all? This is not a novel observation.\u00a0This is a common source of hidden bugs, especially annoying in statically typed languages, which otherwise catch an incorrect mixing of types. Some IDEs will mark these with a warning, lessening the problem.</p> <p>Returning <code>false</code> on incomparable types implies that equality is defined for all types. In these languages, the <code>Anything</code> type has an <code>equals</code> method and usually defaults to reference equality for user-defined classes.</p> <p>Many scripting languages, like Perl, R, and Javascript take option #2 and say that\u00a0<code>1</code> cannot be compared to <code>\"1\"</code>, but they convert the number to a string and then compare the strings, giving <code>1==\"1\"</code> but <code>1!=\"2\"</code>. This is worse than always returning <code>false</code>. If equality is to mean anything, it has to mean that <code>a==b</code> implies that <code>a</code> and <code>b</code> can be used in place of each other in some sense. But a <code>\"1\"</code> cannot be used in the same way as a <code>1</code>. It is true that <code>1+1</code> is <code>2</code>, but <code>\"1\"+\"1\"</code> is not <code>2</code>.\u00a0At least, it better not be!</p> <p>In rare cases, programming languages like Matlab will allow two types to be compared as if they are the same type even if they are not. They see option #3 not as a pitfall, but as an hammock. In Matlab, <code>1!=\"1\"</code> but <code>49==\"1\"</code>, because <code>\"1\"</code> is the 49th ASCII character. It should not come as surprise to anyone following this blog, that I think such\u00a0a\u00a0design\u00a0is an abomination. It will get no further attention here.</p>"},{"location":"blog/sane-equality/#type-safe-equality","title":"Type-safe equality","text":"<p>Comparing things of different types requires that the types be available at runtime. Languages that are statically compiled without types, like C, C++, and Rust, must choose option #4 and fail to compile. This is called type-safe equality and it prevents comparing things that cannot be compared.</p> <p>Assuming that we want a trait to define the interface for equality, there are several ways to design it. One way is with an instance method that only accepts an argument of the same type as the implementing class:</p> <pre><code>trait Equatable[in Self &lt;: Equatable[Self]]:\n  # Assume that == and != somehow desugar to equals and not_equals\n  formal def equals(that: Self) -&gt; Boolean\n  def not_equals(that: Self) -&gt; Boolean:\n    return not equals(that)\n\nclass Point(x: Float, y: Float) extends Equatable[Point]:\n  actual def equals(that: Point):\n    return this.x == that.x and this.y == that.y\n\nclass Circle(x: Float, y: Float, r: Float) extends Equatable[Circle]:\n  actual def equals(that: Circle):\n    return this.x == that.x and this.y == that.y and this.r == that.r\n</code></pre> <p>Like all shape traits, it is up to the user to correctly pass the current type as the type parameter. Rust, which uses this design, also magically defaults the type parameter to the current class, which is a nice touch.</p> <p>This provides a totally type-safe equality:</p> <pre><code># All of these are true\nPoint(1,2) == Point(1,2)\nCircle(1,2,2) == Circle(1,2,2)\n\n# These all are compile errors\nPoint(1,2) == Circle(1,2)\nCircle(1,2) == Point(1,2)\nPoint(1,2):Anything == Circle(1,2):Anything\n</code></pre> <p>One cannot accidentally compare two objects that are incomparable.</p> <p>An alternative solution is accept option #5 and return a sentinel value, say <code>null</code>, when two objects are incomparable. This is also type-safe assuming that the <code>if</code> statement won't accept <code>null</code> (which it shouldn't!). This forces the programmer to recognize\u00a0the incomparable\u00a0case, while still leaving the possibility of converting the <code>null</code> into a <code>false</code> if he wants to. Some languages, like Ceylon, have a nice operator for converting <code>null</code>s called <code>else</code>. One could write <code>1==\"1\" else false</code> to recover the type-unsafe behavior, but in a type-safe way. The formal method <code>equals</code> above assumes that it is type-safe. If I provided a <code>null</code>-returning equality, I would bake it into the\u00a0trait so that a user extending <code>Equatable</code> would only have to write one method.</p> <pre><code>trait Equatable[Self &lt;: Equatable[Self]]:\n  def equals_or_null[That](that: That) -&gt; Boolean|Null:\n    if That &lt;: Self:\n      return this == that\n    else:\n      return null\n</code></pre> <p>Note that this method grabs the static type of <code>that</code> and uses it to see if normal equals would work. Not all languages allow the type parameter to be queried at runtime like this. Ceylon allows it, but Scala does not.\u00a0Some magical type computation could probably ensure that the return type was <code>Boolean</code> rather than <code>Boolean|Null</code> whenever the types were comparable, but that is beyond the scope of this post.</p>"},{"location":"blog/sane-equality/#equal-in-what-way","title":"Equal in what way?","text":"<p>One thing that is overlooked by every programming language I have used is how inheritance interacts with equality. Conceptually, when <code>B</code> inherits <code>A</code>, objects of type <code>B</code> now have two kinds of equality. They can be equal as <code>B</code>s or they can be equal as <code>A</code>s.</p> <p>Let's add another class, this time with inheritance involved:</p> <pre><code>class ColoredPoint(x: Float, y: Float, color: Color)\n    extends Point(x,y) and Equatable[ColoredPoint]:\n  new def equals(that: ColoredPoint):\n    return this.x == that.x and this.y == that.y and this.color == that.color\n</code></pre> <p>The <code>new</code> keyword works like it does in C#; it introduces a new method to hide an existing one. The hiding is partial; the new method is called only if the static type of the object and the argument are both <code>ColoredPoint</code>.</p> <p>Before we continue, let's remember what inheritance is for. Inheritance allows us to pass in instances of <code>ColoredPoint</code> to code that is expecting instances of\u00a0<code>Point</code> and that code needs to\u00a0work exactly like it had received a real instance of <code>Point</code>. In most languages, the equality operator is a virtual method, which means that this is true:</p> <pre><code># In most languages, but not the one espoused here\nColoredPoint(1,2,colors.red):Point != ColoredPoint(1,2,colors.blue):Point\n</code></pre> <p>Even when the static types of the objects are Points, they are not compared in their <code>Point</code>ness, but in their <code>ColoredPoint</code>ness. This matters because code that receives two <code>Point</code>s is expecting their equality to be determined entirely by the values of <code>x</code> and <code>y</code>, not some totally unrelated property that may not have even been defined when the code was first written.</p> <p>If it is a virtual method, it is also not symmetric:</p> <pre><code># In most languages, but not the one espoused here\nColoredPoint(1,2,colors.red) != Point(1,2)\nPoint(1,2) == ColoredPoint(1,2,colors.red)\n</code></pre> <p>This happens because each <code>equals</code>\u00a0accepts <code>Anything</code> and then tests is the type is compatible for testing. When the method is on a <code>Point</code>, it is compatible with accepting a <code>ColoredPoint</code>, but not vice versa.</p> <p>By using method hiding, my implementation has the expected behavior:</p> <pre><code># narrow\nColoredPoint(1,2,colors.red):Point == ColoredPoint(1,2,colors.blue):Point\n\n# symmetric\nPoint(1,2) == ColoredPoint(1,2,colors.blue)\nColoredPoint(1,2,colors.blue) == Point(1,2)\n</code></pre> <p>Doing the explicit casting in the narrow example above is rarely used. Where this really matters is functions that expect an instance of the base class\u00a0like:</p> <pre><code>def do_points_overlap(p1: Point, p2: Point) -&gt; Boolean:\n  return p1 == p2\n</code></pre> <p>The typical behavior does not do the right thing here if <code>p1</code> and <code>p2</code> are <code>ColoredPoint</code>s.</p>"},{"location":"blog/sane-equality/#mutable-equality","title":"Mutable equality","text":"<p>In an inheritance tree, which version of <code>equals</code> gets chosen is determined by the static type\u00a0we are currently working with. But mutable objects have two kinds of equality, reference equality and value equality. These two mutable arrays <code>a1 = Array(1,2,3)</code> and <code>a2 = Array(1,2,3)</code> have equal values, but are not indistinguishable. For all operations that read from the arrays, they are the same, but operations that write to the arrays reveal that <code>a1</code> and <code>a2</code> are not the same object. Many programming languages provide a separate operator, say <code>===</code>, to do reference equality. This operation can be, and probably should be, baked into the language. There is exactly one correct way to compare references (their memory pointers are equal) and it requires information often hidden from the user\u00a0(their\u00a0memory pointers).</p> <p>The language could provide a trait that implements reference equality:</p> <pre><code>trait Identifiable[Self &lt;: Identifiable[Self]] extends Equatable[Self]:\n  actual def equals(that: Self) -&gt; Boolean:\n    # magical implementation\n\nclass MovablePoint(var x: Float, var y: Float)\n    extends Identifiable[MovablePoint]:\n  pass\n</code></pre> <p>Here there is no special syntax. It is a simple extension of <code>Equatable</code>. If one's type does not implement its own <code>equals</code> method, it will inherit it from <code>Identifiable</code> so that <code>MovablePoint(1,2) != MovablePoint(1,2)</code>. If it implements it's own <code>equals</code> to do value equality, we can recover the reference equality by casting to the <code>Identifiable</code> trait like this: <code>MovablePoint(1,2):Identifiable[MovablePoint] != MovablePoint(1,2)</code>.</p> <p>Downcasting to <code>Identifiable</code> every time we want reference equality is very inconvenient. A better solution would be to move <code>Identifiable</code> out of the <code>Equatable</code> hierarchy entirely. Sure, reference equality can be used as the main equality, but that is not necessarily how the user wants it. Let the user specify how he wants equality to work on his object.</p> <pre><code>trait Identifiable[Self &lt;: Identifiable[Self]]:\n  # Assume that === desugars to identical\n  def identical(that: Self) -&gt; Boolean:\n    # magical implementation\n\nclass MovablePoint(var x: Float, var y: Float)\n    extends Identifiable[MovablePoint] &amp; Equatable[MoveablePoint]:\n  actual def equals(that: MoveablePoint):\n    return this.x == that.x and this.y == that.y\n</code></pre> <p>We could have chosen to\u00a0delegate to reference equality, but didn't. We now have two operators <code>==</code> and <code>===</code> that do different things for this type.\u00a0We have chosen to have <code>==</code> on immutable objects and mutable objects do value comparison. In one way, this is a good choice because the user likely expects the operator to do the same thing. In another way, this is a bad choice because now <code>==</code> is not a stable\u00a0property of mutable objects. It cannot be used in contexts where the equality must be stable, such as in sets, dictionary keys, or hash tables. Perhaps this is ok as long as <code>==</code> is used for transitory comparisons and the correct operation is aliased to a different operation, say <code>stable_equals</code>, which is used in collections. The language would be wise to provide metaclasses for immutable and mutable objects that implement this all correctly.</p>"},{"location":"blog/sane-equality/#equality-is-everywhere","title":"Equality is everywhere","text":"<p>And speaking of collections, equality is not just about the behavior of the nice syntax. If it was just about the behavior of <code>==</code>, then we could just provide all the possibilities: <code>=!=</code> for an equals that returned <code>false</code> and <code>=?=</code> for an equals that returned <code>null</code>. The issue is so much bigger because equality is baked into collections, like <code>Sequence</code> and <code>Set</code>.</p> <p>Consider <code>Set</code> first: all the elements are unique; none of the elements are equal to each other. But this begs the question, equal in what way? If I have a <code>Set[Point]</code>, I am probably expecting that <code>ColoredPoint(1,2,colors.red)</code> and <code>ColoredPoint(1,2,colors.blue)</code> are not both in there because those are equal as <code>Point</code>s. In Scala and languages like it, they both most certainly can be in there.</p> <p>Consider the <code>contains</code> method on <code>Sequence</code>. The signature in Scala\u00a0and friends is\u00a0<code>contains(element: Anything)</code>. Here is the type-unsafe equals rearing its ugly head again. You can have <code>List(1,2,3).contains(\"foo\")</code> quietly evaluate to <code>false</code>. (Famously in Scala, <code>List(1,2,3).toSet()</code> will also evaluate to <code>false</code> due to a confluence of design flaws, one of which is type-unsafe <code>equals</code>.) A better design of <code>contains</code> takes a type parameter that answers that question: equal in what way?</p> <pre><code>class Sequence[Element]:\n  def contains[That &lt;: Equatable[Element]](that: That):\n    for element in this:\n      if element == that:\n        return true\n    return false\n</code></pre> <p>Note that\u00a0<code>That</code> must be a supertype of <code>Element</code>, which is implied by it being a subtype of the contravariant <code>Equatable[Element]</code>. You can ask a sequence of colored points if it contains a particular point, but you can't ask a sequence of points if it contains a particular colored point, because its elements don't necessarily understand color.</p>"},{"location":"blog/sane-equality/#unions-require-special-care","title":"Unions require special care","text":"<p>One place where type-safe equals gets a little messy is when dealing with unions, like <code>Circle|Point</code>. Consider the following setup:</p> <pre><code>val circle:Circle|Point = Circle(1,2,2)\nval point:Circle|Point = Point(1,2)\n</code></pre> <p>We would expect that <code>circle</code> and <code>point</code> could be compared because they have the same static type. However, neither\u00a0<code>equals</code> method\u00a0will accept an argument of type <code>Circle|Point</code>, because neither\u00a0class\u00a0knows how to deal with the other.</p> <p>The solution is to remember that <code>Union</code> is itself a class. It is nice when\u00a0it is treated specially by the language, but as a container type, it can have its own methods. The <code>equals</code> method on <code>Union</code> needs to check if the dynamic types fall into the same branch. If they don't, return <code>false</code>; otherwise, dispatch to the branch that they satisfy.</p> <pre><code>class Union[Left, Right](left: Left, right: Right) extends Equatable[Left|Right]:\n  actual def equals(that: Left|Right):\n    switch [this, that]:\n      [Left&amp;Right, Left&amp;Right]:\n        return this as Left == that as Left and this as Right == that as Right\n      [Left, Left]:\n        return this == that # this and that now have static type Left\n      [Right, Right]:\n        return this == that # this and that now have static type Right\n      _:\n        return false\n</code></pre> <p>The first condition covers the possibility that the arguments satisfy both <code>Left</code> and <code>Right</code>.\u00a0Without this case, the union would be asymmetric. If <code>this</code> and <code>that</code> satisfied <code>A&amp;B</code>, then <code>this:A|B==that:A|B</code> would call <code>A.equals(A)</code> while <code>this:B|A==that:B|A</code> would call <code>B.equals(B)</code>. My version always calls them both, though in different orders, but that\u00a0is less problematic.</p>"},{"location":"blog/sane-equality/#conclusions","title":"Conclusions","text":"<p>The issue of syntax, such as whether <code>==</code> should ever mean value equality or whether <code>==</code> should always be a stable\u00a0equality, is secondary to the problem of building a hierarchy of sane equals. I think that there should be traits with names like <code>Equatable</code> for value equality, <code>Identifiable</code> for reference equality, and <code>Collectible</code> for stable\u00a0equality. Each of these should have members <code>value_equals</code>, <code>reference_equals</code>, and <code>stable_equals</code>. They are defined for whatever classes on which they make sense.</p> <p>Once that is established, then we decide what to do with the syntax. In all the code where it really\u00a0matters, the method name should be used directly. The <code>==</code> operator should point to whichever method is the most intuitive equality for that type. I think that <code>===</code> should point to <code>reference_equals</code>. I am on the fence about whether or not <code>value_equals</code> should get its own operator; it really depends on how many mutable classes I think should have <code>==</code> point to reference equality. I definitely don't think that <code>stable_equals</code> should gets its own operator because it should only be used with collections libraries, not user code.</p> <p>And as for what to do with incomparables, I would throw an error for the main operators, but\u00a0provide methods on the respective traits for\u00a0returning <code>false</code> and <code>null</code>. Maybe not even methods on the traits, but as obscure library functions.</p> <p>As difficult as this may be to believe, there is much I have left unsaid on this topic. Like all recursive types, <code>Equatable</code> and friends\u00a0have deep problems integrating into a static type system, particularly because it is easy to get infinite recursion when dealing with generic container types (something I ignored while talking about union types). Ross Tate has an interesting solution to this problem, but I am not qualified to judge it.</p>"},{"location":"blog/the-missing-11th-of-the-month/","title":"The Missing 11th of the Month","text":"Source xkcd. Image licensed under CC-BY-NC.  <p>On November 28th, 2012, Randall Munroe published an xkcd\u00a0comic\u00a0that was\u00a0a calendar in which\u00a0the size of each date was\u00a0proportional to\u00a0how often each date is referenced by its ordinal name\u00a0(e.g. \"October 14th\") in the Google Ngrams database since 2000. Most of the large\u00a0days are pretty much what you would expect:\u00a0July 4th, December 25th, the 1st of every month, the last day of most months, and of course a September 11th that shoves\u00a0its neighbors\u00a0into the margins. There are not many days that seem to be smaller than the typical size. February 29th is a tiny speck, for instance. But if\u00a0you stare at the comic\u00a0long enough, you may get the impression that the 11th of most\u00a0months is unusually small. The title\u00a0text of the comic concurs, reading \"In months other than September, the 11th is mentioned substantially less often than any other date. It's been that way since long before 9/11 and I have no idea why.\" After digging into the raw data, I believe I have figured out why.</p> <p>First I confirmed\u00a0that the <code>11th</code> is actually interesting. There are 31 days and one of them has to be smallest. Maybe the <code>11th</code> isn't an outlier; it's just on the smaller end and our eyes\u00a0are picking up on a pattern that doesn't exist. To confirm this is real,\u00a0I compared actual numbers, not text size. The Ngrams database returns the total number times a phrase is mentioned in a given year normalized by the total number of books published that year. The database only goes up to the year 2008, so it is presumably unchanged from when Randall queried it in 2012.</p> <p>I\u00a0retrieved the count for each day for the year (<code>January 1st</code>, <code>January 2nd</code> etc.) and took the median over the months for each day (median of <code>January 1st</code>, <code>February 1st</code>, etc.) for\u00a0each year. This summarizes\u00a0how often the <code>11th</code> and the other 30 days of the month appear in a given year. Using the median prevents outlier days like <code>July 4th</code> from dragging up the average for its\u00a0corresponding ordinal\u00a0(the <code>4th</code>). Only if a ordinal\u00a0is unusual for at least 6\u00a0of the 12 months will its median appear unusual.</p> <p>I took the\u00a0median for each ordinal over the years 2000-2008. The graph below is a histogram of the 31 medians. The <code>1st</code> of the month stands out far above them all\u00a0and the <code>15th</code> just barely distinguishes itself from the remainder. Being the first day and the middle day of the month, these two make sense.\u00a0However, the <code>11th</code> stands out as the lowest by a significant\u00a0margin (p-value &lt; 0.05), with no immediate explanation.</p> <p>This deficit\u00a0has been around for a long time. Below is all the ordinals for every year in the data set, 1800-2008. The data is smoothed over eleven\u00a0years to flatten out the noise. Even at the beginning, the <code>11th</code> is significantly lower than the main group. This mild deficit continues for a few decades and then something weird happens in 1860s; the\u00a011th suddenly diverges from its place just below the pack. The gap between the <code>11th</code> and the ordinary ordinals expands rapidly until the <code>11th</code> is about half of what one would expect it to be throughout the first half of the twentieth century. The gap shrinks in the second half of the twentieth century, but still\u00a0persists at a smaller level\u00a0until\u00a0the end.</p> <p>Astute graph readers will notice that something else weird is going on. There are four other lines that are much lower than they should be. From highest to lowest, they are the <code>2nd</code>, the <code>3rd</code>, the <code>22nd</code>, and the <code>23rd</code>.\u00a0They were\u00a0even lower than the <code>11th</code> from 1800 until\u00a0the 1890s. However, starting around 1900, their gaps started shrinking even as the <code>11th</code> diverged until the gap disappeared completely in the 1930s. There is an interesting story there, but because their effect doesn't persist to the present, I'll continue to focus on the <code>11th</code> and leave the others\u00a0for a future post.</p>"},{"location":"blog/the-missing-11th-of-the-month/#typographical-hijinks","title":"Typographical hijinks","text":"<p>When I began this study, I was hoping to find a hidden taboo of holding events on the 11th or typographical\u00a0bias\u00a0against the\u00a0shorthand ordinal. Alas, the reason is far is far more mundane: a numeral <code>1</code> looks a lot like a capital <code>I</code> or a lowercase <code>l</code> or a lowercase <code>i</code> in most of the fonts used for printing books. An <code>11</code> also looks like an <code>n</code>, apparently. Google's\u00a0algorithms made mistakes when reading the <code>11th</code> from a page, interpreting\u00a0the ordinal\u00a0as some other word.</p> <p>We can find some of these mistakes by directly searching for nonsense phrases like <code>March\u00a0llth</code> or <code>July IIth</code> or <code>May iith</code>. There are nine possible combinations\u00a0of <code>I</code>, <code>l</code>, and <code>i</code> that a <code>11</code> could be mistaken for. \u00a0Five of them can actually be found in the database for at least one month: <code>IIth</code>, <code>Ilth</code>, <code>iith</code>, <code>lith</code>, and <code>llth</code>. Also found was <code>1lth</code>, <code>1ith</code>, and <code>l1th</code>, in which only one letter was misread. I collectively refer to these errors as\u00a0<code>xxth</code>.\u00a0Google books\u00a0queries a newer database than the one on which Ngrams was built, but bona fide examples of the misreads can still be found. Here is something that Google books thinks says <code>January IIth</code>: . And here is one for <code>February llth</code>: . And finally one\u00a0for <code>March lith</code>: . There are hordes of these in the database. You can find other ordinals that were misread as well, but the <code>11th</code> with its slender and ambiguous <code>1</code>s was misread far more often than the others.</p> <p>I added back in every instance of <code>January IIth</code>, <code>January llth</code>, etc. to <code>January 11th</code> and did the same to the other months. The graph below shows that the <code>11th</code> gets a big\u00a0boost by adding back\u00a0the\u00a0nonsense phrases. Before the 1860s,\u00a0the difference between the <code>11th</code> and the main group\u00a0is erased. After the 1860s, about a quarter to a third of the difference is erased.</p>"},{"location":"blog/the-missing-11th-of-the-month/#to-the-nth-degree","title":"To the nth degree","text":"<p>So where did\u00a0the rest of the missing <code>11th</code> go? Well, starting in the 1860s, the Google algorithm starts to make a rather peculiar error\u2014it misreads <code>11th</code> as <code>nth</code>. Here is one\u00a0example from a page full of <code>January nth</code>s: . In some years, the number of incorrect reads actually exceeds the number of correct reads. I added <code>January nth</code> to <code>January 11th</code> and did the same for all the months. The graph below shows both the\u00a0<code>nth</code>\u00a0and its sum with the <code>11th</code>.\u00a0There was\u00a0little impact\u00a0before the 1860s, but then this error alone accounts for nearly all of the missing <code>11th</code>.</p>"},{"location":"blog/the-missing-11th-of-the-month/#combined-graph","title":"Combined\u00a0graph","text":"<p>When the <code>xxth</code> misreads and\u00a0<code>nth</code> misreads are both added back into\u00a0the <code>11th</code>, the gap disappears across the entire timeline and the <code>11th</code>\u00a0looks like an ordinary day of the year.\u00a0This suggests that the misreading of the <code>11th</code> as <code>nth</code>, <code>IIth</code>, <code>llth</code>, etc. is sufficient to explain the unusually low incidence of the <code>11th</code> as a day of the month.</p>"},{"location":"blog/the-missing-11th-of-the-month/#typographical-machines","title":"Typographical machines","text":"<p>While it makes sense that the <code>11th</code> was\u00a0misread more than others, why is the misread rate not uniform? What happened in the 1860s that caused the dramatic rise in the error rate? I suspect that it has something to do with\u00a0a special device invented in the 1860s_\u2014_the typewriter. The earliest typewriters did not have a separate key for the numeral <code>1</code>.\u00a0Typists were expected to use the lowercase <code>l</code> to represent a <code>1</code>. When the algorithm read <code>October llth</code>, it was far more\u00a0correct than we have been\u00a0giving it credit. There are not that many documents in Google books that are typewritten, but\u00a0this popular new contraption had a powerful effect on the evolution of fonts. The <code>1</code> and <code>l</code> were identical on the increasingly familiar typewriters, and\u00a0the fonts even of printed materials began to reflect this expectation. Compare the <code>l</code>s and <code>1</code>s in this font from\u00a01850: .\u00a0There is a clear difference between an <code>l</code> with no serifs on the top and the <code>1</code> with a pronounced serif. Compare that to a font from 1920: .\u00a0The characters are identical except for the kerning. Even to this day, most fonts represent both the <code>1</code> and the <code>l</code> as tall characters with two serifs on the bottom and one left-facing serif at the top. The only difference is that the serif on the <code>1</code> is slightly more angled than on the <code>l</code>. (In this post, I used a special monospace font to make it easier to tell the difference.)\u00a0The print quality of more recent books (post 1970s) has reduced the rate of\u00a0failure, but it still has not gone away entirely, so that the remaining failures were noticeable in the xkcd comic.</p> <p>The largest open question is why <code>nth</code> was chosen so often. It seems like such a strange error to make. The word <code>nth</code> is a legal word\u00a0in mathematical and scientific publications, so that should help its chances of getting picked. In most fonts the top of the <code>n</code> is really thin, and is likely invisible in many texts on which they trained the algorithm. But there is a big different in height between <code>1</code> and <code>n</code>, especially in the typewriter era, which is where the errors occur. And the phrase <code>January nth</code> is nonsense so that should have hurt its chances of being selected. Is it possible there\u00a0was an error in one of the modern training texts that had an <code>11th</code> labeled as <code>nth</code>, thereby confusing the algorithm? The only way to know for sure would be to crack open the source code of Google's text-reading algorithm. This is left as an exercise for the reader.</p>"},{"location":"blog/the-missing-11th-of-the-month/#the-code-used-for-the-analysis-in-this-post-is-available-on-github","title":"The code used for the analysis in this post is available on\u00a0Github.","text":""},{"location":"blog/the-missing-23rd-of-the-month/","title":"The Missing 23rd of the Month","text":"<p>Previously, I explained\u00a0why the 11th of most months is\u00a0mentioned far less than the other days in the Google Ngrams database of English literature from 1800-2008. This was to solve a\u00a0long-standing question posed in an\u00a0xkcd comic. While researching this, I encountered another mystery: the 2nd, 3rd, 22nd, and 23rd are unusually\u00a0low as well\u2014but only until the 1930s, at which point they become perfectly normal days. Last time, I set this question aside to focus on the 11th.\u00a0In this installment, I explain\u00a0the strange behavior of these\u00a0four\u00a0days.</p> <p>To remind everyone,\u00a0the graph\u00a0below is the mystery we are dealing with. The <code>2nd</code>, <code>3rd</code>, <code>22nd</code>, and <code>23rd</code> are practically unused in 1800, the earliest point in the database. Around 1810 is when the first\u00a0substantial uses appear; they grow at about the same rate as the other days, maintaining a substantial gap at about half of what one would expect until about the 1890s. Then suddenly, the gap shrinks and continues to do so until the 1930s when <code>2nd</code>, <code>3rd</code>, <code>22nd</code>, and <code>23rd</code> are absorbed into the main group.</p>"},{"location":"blog/the-missing-23rd-of-the-month/#ye-old-style","title":"Ye old style","text":"<p>So were <code>2</code> and <code>3</code> unlucky numbers in the 1800s? Did Google's algorithm have a hard time reading the <code>2</code>s and <code>3</code>s of old-timey fonts? Nope, it turns out that people used to write these ordinals\u00a0as <code>2d</code>, <code>3d</code>, <code>22d</code>, and <code>23d</code>. I took the median over\u00a0<code>January 2d</code>, <code>February 2d</code>, etc. for each year and did the same for the other old-style ordinals. The graph below shows the use of old-style ordinals, which start as normal days within the main group, but slowly diverge\u00a0until they drop off exponentially in the 1890s, reaching a tiny residue\u00a0by the 1930s.</p> <p>Sometimes you can encounter a modern use of the old-style abbreviations when the ordinal is\u00a0part of a name with a very long history, like the 3d Marine Division. This is not why the old-style has a small residual in the latter half of the twentieth\u00a0century.\u00a0If you search through Google Books for modern uses of <code>January 2d</code>, you will only find reprints of old\u00a0books and publications of old diaries.</p>"},{"location":"blog/the-missing-23rd-of-the-month/#combined-graph","title":"Combined graph","text":"<p>The old style falls away as the\u00a0new style emerges. When we\u00a0add the old-style and new-style ordinals together, we get\u00a0the graph\u00a0below, which shows that once the two styles are accounted for, these four days of the months are actually quite ordinary.</p> <p>I don't have a fully satisfying explanation\u00a0for why\u00a0the <code>2nd</code> and <code>3rd</code> now peek their heads above the main group from time to time. I guess if the <code>1st</code> on the month is hugely over-represented, it is reasonable to expect that the next smallest ordinals would be slightly over-represented. (\"Let's have our meeting on the first of the month.\" \"I have ten other meetings on the first!\" \"Ok then, the second.\") However, if I search Google Books for instances of <code>January 2d</code> or <code>January 2nd</code>, there are a sizable number of hits from lists like this:  Google Books apparently ignores commas. With the <code>1st</code>, <code>2nd</code>, <code>3rd</code>, and <code>4th</code> being the only\u00a0regular ordinals for weeks of the month, these might get a boost this way.</p>"},{"location":"blog/the-missing-23rd-of-the-month/#speculation","title":"Speculation","text":"<p>Why did writers\u00a0use these one-letter abbreviations? Probably to follow\u00a0Latin, where\u00a0this practice originated\u00a0and\u00a0the ordinal indicator is a single letter <code>o</code>. The Romance languages, like Spanish, Italian, and Portuguese, still use <code>o</code> and <code>a</code>. I expect that we would still be using <code>d</code>\u00a0if\u00a0it wasn't for <code>1st</code>, <code>4th</code>, etc. whose final consonant sound cannot be represented by a single letter. In the end, consistency within the English language by using two letters for all ordinals was more attractive than similarity to\u00a0Latin.</p>"},{"location":"blog/the-missing-23rd-of-the-month/#the-code-used-for-this-analysis-is-available-on-github","title":"The code used for this analysis is available on Github.","text":""},{"location":"blog/marginal-vs-conditional-subtyping/","title":"Marginal vs. Conditional Subtyping","text":"<p>In computer programming, a type is a just a set of objects\u2014not a physical set of objects in memory, but a conceptual set of objects. The type <code>Integer</code>\u00a0is the set <code>{0, 1, -1, ...}</code>. Types are used to reason about the correctness of programs.\u00a0A function that accepts an argument of type <code>Integer</code>, will work for any\u00a0value <code>0</code>, <code>1</code>, <code>-1</code>, etc.,\u00a0for some definition of \"works\". Subtyping is used to define relationships between types. Type <code>B</code>\u00a0is a subset of a type <code>A</code>\u00a0if the set of objects defined by <code>B</code>\u00a0is a subset of the objects defined by <code>A</code>. Every function that works on an instance of <code>A</code>\u00a0also works on an instance of <code>B</code>, for some definition of \"works\". If we were just doing math, subsets\u00a0would be the end of subtyping. But types as pure sets exist only conceptually. Actual\u00a0types must be concretely defined. It is not very efficient to define the <code>Integer</code>\u00a0class by writing all possible <code>Integer</code>s! In most programs, the possible values of a type are constrained\u00a0by their fields. Subtypes and supertypes typically differ by having more or fewer\u00a0fields than the other. Sometimes, the subtype has more fields, and sometimes, the supertype has more fields. Many\u00a0of you may be thinking, \"What? It's always one way, not the other!\" The funny thing is that some\u00a0of you think the subtype always has more fields and others think the supertype always has more fields. That's because there are two kinds of subtyping. The first is what I call \"marginal subtyping\", which is encountered in application programming and is well modeled by inheritance. The second is what I call \"conditional subtyping\", which is encountered\u00a0in mathematical programming and is well modeled by implicit conversions. Depending on the genre of programming you work in, the other kind of subtyping may be unknown and the language features needed to implement it\u00a0may be\u00a0maligned. But both needs\u00a0are real and both features are necessary.</p>"},{"location":"blog/marginal-vs-conditional-subtyping/#marginal-subtyping","title":"Marginal subtyping","text":"<p>Marginal subtyping occurs when the programmer has one class, say <code>Square</code>, and then needs a new class, say <code>ColoredSquare</code>, that is like the old class but it has some additional functionality that is independent of the\u00a0old functionality, usually in a totally separate domain. Marginal subtyping is well\u00a0implemented through traditional object oriented inheritance:</p> <pre><code>class Square:\n  x: Float\n  y: Float\n  width: Float\n  new(x, y, width):\n    self.x = x\n    self.y = y\n    self.width = width\n\nclass ColoredSquare extends Square:\n  color: Color\n  new(x, y, width, color):\n    super(x, y, width)\n    self.color = color\n</code></pre> <p>This is inheritance at its best. It has eliminated the need to copy the fields, methods, and constructors of Square\u00a0into <code>ColoredSquare</code>. Every function that requires a <code>Square</code>\u00a0can safely accept a <code>ColoredSquare</code>. The <code>Square</code>\u00a0part of the <code>ColoredSquare</code>\u00a0can even be mutated, the <code>x</code> and <code>y</code>\u00a0and <code>width</code> fields can be changed while the <code>color</code>\u00a0field is carried safely along.</p> <p>I call this marginal subtyping because relationship between the subtype\u00a0and the supertype is like that of a distribution and a marginal distribution. In marginal subtyping, the subtype has more dimensions, more degrees of freedom, more fields than the supertype.\u00a0In some sense, the Venn diagram of subtyping is little misleading.\u00a0There is not some universe\u00a0of possible squares\u00a0and then we carve out a subregion of squares\u00a0and call them colored squares. It is more that there is a universe of possible squares\u00a0and then there is an additional dimension, <code>color</code>, perpendicular to the diagram; the <code>ColoredSquare</code>s live in this space off the flat surface and each one can be projected down to its corresponding <code>Square</code>. It is a many-to-one mapping from subtype to supertype, with <code>ColoredSquare(0,1,2,Color.black)</code>\u00a0and <code>ColoredSquare(0,1,2,Color.cornflower_blue)</code>\u00a0both\u00a0being interpreted as <code>Square(0,1,2)</code>\u00a0whenever a <code>Square</code>\u00a0is required.</p>"},{"location":"blog/marginal-vs-conditional-subtyping/#conditional-subtyping","title":"Conditional subtyping","text":"<p>Conditional subtyping occurs when one class is a specific kind of another class or, identically, one class is a generalization of the other. Conditional subtyping is well\u00a0implemented through implicit conversions:</p> <pre><code>class Rectangle:\n  x: Float\n  y: Float\n  width: Float\n  height: Float\n  new(x, y, width, height):\n    self.x= x\n    self.y = y\n    self.width = width\n    self.height = height\n\nclass Square:\n  x: Float\n  y: Float\n  width: Float\n  new(x, y, width):\n    self.x= x\n    self.y = y\n    self.width = width\n  implicit static def to_rectangle(r: Square) -&gt; Rectangle:\n    return Rectangle(r.x, r.y, r.width, r.width)\n</code></pre> <p>I am following Scala's implicit conversion design, where the implicit conversion can be defined in either the class being converted from or the\u00a0class being converted to. By defining the implicit conversions as part of the class, it ensures that the user of the class does not have to import the conversion functions separately. Now anytime a function requires a <code>Rectangle</code>, like <code>intersect(rectangle1, rectangle2)</code>, one can actually pass in a <code>Square</code>, like <code>intersect(square, rectangle)</code>, and the implicit function will automatically be applied, like <code>intersect(Square.to_rectangle(square), rectangle)</code>.</p> <p>I call the kind of subtyping required here conditional subtyping because the relationship between the supertype and the subtype is like the relationship between a distribution and a conditional distribution. In conditional subtyping, the subtype has fewer degrees of freedom and, in an ideal world, fewer fields than the supertype. Here, the Venn diagram of subtyping is quite accurate, <code>Square</code>s really are nothing more than\u00a0a special region within <code>Rectangle</code>s. <code>Square</code>s are <code>Rectangle</code>s under the condition that <code>width</code> and <code>height</code> are equal, a condition guaranteed by the <code>Square</code> constructor, assuming there is no mutability to spoil it later.</p>"},{"location":"blog/marginal-vs-conditional-subtyping/#alternatives","title":"Alternatives","text":"<p>Both inheritance and implicit conversions are often\u00a0reviled as useless and complicated. This is not that surprising on its own. Both features are very powerful, often overused, and sadly, designed poorly in some languages. An additional difficulty\u00a0is that marginal subtyping is often the only relationship model needed for application programming and conditional subtyping is often the only relationship needed by mathematical programmers.\u00a0The unstoppable juggernaut of\u00a0object oriented programming particularly encourages the use of inheritance everywhere, even on relationships that requires conditional subtyping. It may seem advisable to get rid of one or the other, but this leaves bad solutions for many problems, as shown below.</p>"},{"location":"blog/marginal-vs-conditional-subtyping/#replace-inheritance-with-membership","title":"Replace inheritance with membership","text":"<p>If one wanted to avoid the use of inheritance, one could make the <code>ColoredSquare</code>\u00a0class like this:</p> <pre><code>class ColoredSquare:\n  circle: Square\n  color: Color\n  new(x, y, width, color):\n    self.circle = Square(x, y, width)\n    self.color = color\n</code></pre> <p>But this would require one to access the circle member every time one wanted to use a <code>ColoredSquare</code>\u00a0as a <code>Square</code>, writing <code>my_square.square.area()</code> instead of <code>my_square.area()</code> or <code>intersection(one_square.square, another_square.square)</code> instead of <code>intersection(one_square, another_square)</code>. Over an entire program, eliminating these \"hasa\"\u00a0casts can save a lot of boilerplate and noise.</p>"},{"location":"blog/marginal-vs-conditional-subtyping/#replace-inheritance-with-implicits","title":"Replace inheritance with implicits","text":"<p>One can try to replace inheritance with implicit conversions like this:</p> <pre><code>class Square:\n  x: Float\n  y: Float\n  width: Float\n  new(x, y, width):\n    self.x = x\n    self.y = y\n    self.width = width\n\nclass ColoredSquare:\n  x: Float\n  y: Float\n  color: Color\n  new(x, y, width, color):\n    self.x = x\n    self.y = y\n    self.color = color\n  implicit static def to_square(s: ColoredSquare) -&gt; Square:\n    return Square(s.x, s.y, s.width)\n</code></pre> <p>The first problem with this is a large amount of repetition compared to the original above. There is no <code>super</code> constructor to which to\u00a0delegate, so all fields must be set in each class. This is not\u00a0that bad, because the user is also freed from having to call the <code>super</code>\u00a0constructor, a responsibility that can cause problems when one wants finer control over the order in which fields are initialized. There is more flexibility in this design and, while I have manually copied each constructor argument to its corresponding field in all my examples here, most modern languages have syntax that makes fields for\u00a0some or all parameters of the constructor.</p> <p>The second problem is fatal however. If these are mutable classes, then the object returned by the implicit conversion has no relationship to the original object. A change made to a field on the down-cast object is not reflected in the corresponding field on the original object, and vice versa:</p> <pre><code>val colored_square = ColoredSquare(1.0, 2.0, 3.0)\nval square: Square = colored_square  # ColoredSquare.to_square(colored_square)\n\ncolored_square.x = 5.0\nsquare.x  # Still 1.0\n\nsquare.y = 4.0\ncolored_square.y  # Still 2.0\n</code></pre>"},{"location":"blog/marginal-vs-conditional-subtyping/#replace-implicits-with-inheritance","title":"Replace implicits with inheritance","text":"<p>Because of the popularity of object oriented programming languages, using inheritance where implicit conversions would be more appropriate is very common. It is also very easy to screw up. Consider this attempt to fit <code>Square</code>\u00a0and <code>Rectangle</code>\u00a0into an inheritance hierarchy:</p> <pre><code>class Square:\n  x: Float\n  y: Float\n  width: Float\n  new(x, y, width):\n    self.x = x\n    self.y = y\n    self.width = width\n\nclass Rectangle extends Square:\n  height: Float\n  new(x, y, width, height):\n    super(x, y, width)\n    self.height = height\n</code></pre> <p>Like <code>ColoredSquare</code>, <code>Rectangle</code> has one additional field compared to a <code>Square</code>, so it becomes the child class. Programmatically, this is completely legal. Conceptually, this is wrong, wrong, wrong. Remember the diagram of a subtype from above? Subtyping must satisfy an \"is a\" relationship\u2014<code>Rectangle</code>\u00a0is only a subtype of <code>Square</code>\u00a0if every <code>Rectangle</code>\u00a0\"is a\" <code>Square</code>. This is mathematically wrong\u2014the worst kind of wrong.</p> <p>If I remember my middle school math correctly, a square is a rectangle with two equal sides. To do this properly with inheritance we need to flip the relationship:</p> <pre><code>class Rectangle:\n  x: Float\n  y: Float\n  width: Float\n  height: Float\n  new(x, y, width, height):\n    self.x= x\n    self.y = y\n    self.width = width\n    self.height = height\n\nclass Square extends Rectangle:\n  new(x, y, width):\n    super(x, y, width, width)\n</code></pre> <p>It may seem\u00a0a little strange for the subtype to add no fields, and we'll come back to that in a bit. But programmatically, this is completely legal. Conceptually, this also\u00a0correct, but only\u00a0if an important property holds\u2014instances of <code>Square</code> and <code>Rectangle</code> must be immutable. That is, there cannot be a <code>stretch_horizontally</code>\u00a0method that mutates an <code>Rectangle</code>, because <code>Square</code>\u00a0would have to inherit it and calling\u00a0such a method on\u00a0a <code>Square</code> makes it no longer a <code>Square</code>. Variables typed as <code>Square</code> may not point to things that a reasonable person would consider a square:</p> <pre><code>val square: Square = Square(0.0, 1.0, 2.5)\nsquare.stretch_horizontally(2.0)\n# square is still type Square, but width does not equal height\n</code></pre> <p>This problem is unresolvable as long as the objects are mutable. When a rectangle is stretched in math, it is a new rectangle, but in programming, mutable objects can\u00a0be changed and still remain the same object, just with a different values. The reality is that the mutable square\u00a0type is\u00a0disjoint with the mutable rectangle\u00a0type; there is no object that is both a mutable square\u00a0and a mutable rectangle. That is, there is no object that can change into any rectangle\u00a0while also always remaining a square. If a mutable rectangle\u00a0is the best model for your problem, by all means use it, but understand that there is no type relationship between it and the square version; they are totally unrelated classes.</p> <p>Ok, so let's limit ourselves\u00a0to immutable objects. Technically, the\u00a0classes model the concepts they are supposed to. Something is still a little bit weird about modeling these concepts\u00a0with inheritance. The problems are clearer when we go to add some more classes in this hierarchy. The rectangle\u00a0can be generalized further\u00a0into a parallelogram. Let's put that into the type hierarchy:</p> <pre><code>class Parallelogram:\n  x: Float\n  y: Float\n  width: Float\n  height: Float\n  angle: Float\n  new(x, y, width, height, angle):\n    self.x= x\n    self.y = y\n    self.width = width\n    self.height = height\n    self.angle = angle\n\nclass Rectangle extends Parallelogram:\n  new(x, y, width, height):\n    super(x, y, width, height, tau/4)\n\nclass Square extends Rectangle:\n  new(x, y, width, height):\n    super(x, y, width, width)\n</code></pre> <p>The first problem with this is that adding a new generalization requires adding a supertype to an existing class. It would not be possible to add the <code>Parallelogram</code> class in this way if <code>Rectangle</code> and <code>Square</code> were in a different library. With PEP 3141, Python\u00a0created the class hierarchy <code>Number &gt;: Complex &gt;:\u00a0Real &gt;: Rational &gt;: Integral</code>. This is all correct, but if you ever wanted to make\u00a0a <code>Quaternion</code> class, you'd be out of luck, because you can't \"supertype\" an existing class. If you wanted to add <code>Rhombus</code>, <code>Trapezoid</code>, <code>Quadrilateral</code>, or <code>Polygon</code>\u00a0to our shape classes, you'd also be out of luck unless you owned the library that defined <code>Square</code> and <code>Rectangle</code>.</p> <p>The second problem with this is that the simpler classes get progressively more complicated by the addition of the\u00a0general base classes. When it was just <code>Rectangle</code> and <code>Square</code>, <code>Square</code> had both <code>width</code> and <code>height</code> even though it only needed one number to describe both.\u00a0When <code>Parallelogram</code>\u00a0was added, each class got\u00a0<code>angle</code>. If we wanted to add <code>Polygon</code>, each class would need a variable length list, even though the simpler classes could be fixed-size. This wastes a ton of memory, which is why languages that do this usually turn\u00a0the classes into interfaces and have only one concrete class for each interface. Python may have the <code>Complex</code> interface, but <code>float</code> doesn't have to actually have a field for the imaginary part.\u00a0And\u00a0there is a separate <code>complex</code>\u00a0class that provides the actual implementation of <code>Complex</code>.</p> <p>So modeling mathematical relationships\u00a0with object-oriented\u00a0inheritance means you need two classes for every concept or tons of extra state on simple classes, and you can't generalize existing classes no matter what. It is no wonder that object oriented programming is often denigrated for its excessive complexity and awkward boilerplate. The purists aren't wrong, traditional OOP really is bad for this kind of subtyping.</p> <p>Implicit conversions make it easy to add additional generalizations. One can define <code>Parallelogram</code> separately from <code>Rectangle</code> and <code>Square</code> like this:</p> <pre><code>class Parallelogram:\n  x: Float\n  y: Float\n  width: Float\n  height: Float\n  angle: Float\n  new(x, y, width, height, angle):\n    self.x= x\n    self.y = y\n    self.width = width\n    self.height = height\n    self.angle = angle\n  implicit static def from_rectangle(r: Rectangle) -&gt; Parallelogram:\n    return Parallelogram(r.x, r.y, r.width, r.height, tau/4)\n  implicit static def from_square(s: Square) -&gt; Parallelogram:\n    return Parallelogram(s.x, s.y, s.width, s.width, tau/4)\n</code></pre>"},{"location":"blog/marginal-vs-conditional-subtyping/#living-in-harmony","title":"Living in harmony","text":"<p>Both inheritance and implicit conversion are necessary in a programming language that intends to be\u00a0universally useful because marginal subtyping and conditional subtyping both exist.\u00a0In my projects, one kind or the other typically dominates. When I am writing graph\u00a0algorithms, I have a lot of implicit conversions. When I am writing GUIs, I have a lot of inheritance. One thing I liked about Scala was that I could\u00a0fit\u00a0the algorithm and the user interface comfortably in the same program. This is probably not too surprising as the marriage of object oriented programming and functional programming was the principle design goal of Scala. If I were designing a language, I would start with Scala's design of inheritance and implicit conversion. They play together pretty well in practice, but they were not universally acclaimed. The developers recently put user-defined implicit conversions behind a feature gate. This may be a good idea, not because implicit conversions are bad, but because most new Scala programmers come from an object-oriented background\u00a0and most programs are applications rather than math libraries. I think the problems that came from Scala's implicit conversions could have been avoided if they (1) had made them harder for new programmers to find\u00a0and (2) didn't overuse implicit conversions in the standard library, of which the worst offender is the infamous and deprecated <code>any2StringAdd</code>.</p>"},{"location":"blog/the-sanctification-button/","title":"The Sanctification Button","text":"<p>I have a browser extension installed on my work laptop that blocks my access to Reddit, Facebook, and other news and social media sites. My employer didn't install it; I did. I have the same extension installed at home, albeit blocking a more limited set of time-wasting sites. On its face, setting up a system that does nothing but restrict my future options seems like a waste of time. Wouldn't it be easier just to choose to not visit those sites at imprudent times? In theory, sure. But I don't trust my future self, and restraint is taxing. I'd have trouble explaining why. Admittedly it's weird, but I think I can assume that you, dear human reader, at least understand\u00a0where I am coming from, regardless of how much you rely on such things yourself. In behavioral economics, we call these kinds of mechanisms \"precommitments\". Odysseus bound himself to the mast before sailing past\u00a0the Sirens. Gamblers leave behind their checkbooks and credit cards before a casino vacation. I am one of many who find it prudent to\u00a0occasionally bind my future actions, restrict my future options, or simply nudge my older self in a certain\u00a0direction. There remain plenty of mistakes that I would like to prevent, but for which there is no mechanism to preempt. Inevitably, technology will improve; new products will become available. Some of these, like the browser blocker, will be increasingly capable precommittment tools. How far should you\u00a0go with this? Artificial intelligence combined with cybernetics could make any undesirable behavior potentially preemptable. Leaving aside the technical difficulties, if you could not only end your ability to lie, cheat, and steal, but also gossip and insult, should you?\u00a0Taken to its extreme, if there were a button that removed your ability to sin, would you push it?</p> <p>The meaning of this question depends on your definition of sin.\u00a0I'll approach this mostly from a Christian paradigm, because that's the sin I know, but the button question is applicable regardless of your worldview. We all believe in right and wrong, and there is pretty good overlap in our definitions. (If you wish to insist that there is no right and wrong, then I guess any old button will do.) Whatever your definition of sin, if you could eliminate your ability to commit wrong actions, only committing right actions forever, would you choose to do so?</p> <p>The question is meant to be considered abstractly, but as mentioned in the opening paragraph, I think it is closer to reality than most people realize. Consider the following examples:</p> <ul> <li>Developers today could develop an app that forced you to live within your budget. If such a system were made easy, would you direct deposit your paycheck into such a system?</li> <li>Your phone can easily monitor your entire life, what you hear, what you see, what you say, what you do etc. It is not inconceivable to build an artificial intelligence that would publicly correct you every time you told a lie, whether spoken or typed. Would you install such an app?</li> <li>The same phone AI could report you to the police if you ever stole something. Even if you hid the phone during the heist, it would know from your bank account records\u00a0everything you owned and could tell if you possessed something that didn't belong to you.\u00a0If stealing meant forever giving up your phone, I think\u00a0you'd consider yourself forced into making an honest living.</li> <li>Last I checked, adultery is the last consensual sexual sin that is still considered, well, a sin\u00a0by almost everybody in America. Would you like to never want to be tempted to adultery?\u00a0It would not take the most sophisticated cybernetics imaginable to disable your relatively primitive arousal function for anyone but your spouse.</li> </ul> <p>Obviously, the fully functional button as described is out of reach for the foreseeable future. But\u00a0thinking about the ultimate solution helps guide our understanding of the specific solutions that will arise over the next few decades. It allows us to discuss them abstractly without getting distracted by the details: how effective is it, how much effort is it, how much does it cost, what are the side effects, how will this be used by authorities. For the sake of argument, assume that it is free, it is voluntary, and it is flawless. Do you still push the button?</p> <p>Does it matter if the button removes sinful actions, but doesn't touch thoughts? As a Christian, I care more about my thoughts than I do about my actions, or at least I should. My sinful actions are the expression of my sinful thoughts. The tangible negative consequences of sinful actions (physical pain, punishments by authorities, broken relationships, etc.) are actually good things; they bring me to confess and correct the attitudes behind the actions. If the button merely prevented sinful actions but did nothing within, then there is a argument that such a button would actually be deeply harmful to my soul. Would I bother to cultivate an attitude of appreciation and love toward my wife if I knew I would never snap at her anyway? Would I smile and make small talk and be helpful with the chores, while in my head is the cursing at how unfair or inconvenient everything is? The Bible describes heaven and hell as separated by a chasm, but if you wanted to imagine a collocated heaven and hell, you could consider a place where everyone's actions flow out of pure love and joy, but while some individuals cultivate an attitude of charity making all actions\u00a0their own, others scream curses in their head through all eternity while looking out the eyes of a steadfastly polite and faithful, but ultimately zombie-like, friend.</p> <p>How about if the button rejuvenated your attitudes also, so that evil thoughts were just as impossible as evil deeds? In one sense, this version doesn't seem as practical to me. Adjusting someone's thoughts to any practical precision will be out of the reach of cybernetics for quite awhile. But in another sense, this version of the button already exists. In fact, I've already chosen to push it by believing\u00a0in Jesus Christ. The Bible\u00a0offers an eternity\u00a0without\u00a0sin\u00a0for those who choose to accept it. Even if the Bible turned out to be false, I've still chosen to push the button, albeit one that did not function as advertised. From a strictly Christian perspective it could be argued that one should not circumvent the saving work of Christ by pushing a button, but I think that is beside the point. The important aspect of the question is not the physical button, but whether or not one should choose to accept a mechanism that will purge sin from one's thoughts to a totality that could not be achieved through human effort alone. It is a mechanism not only permitted, not only encouraged, but mandated by scripture.</p> <p>Fixing your actions without fixing your thoughts risks a prison, but does fixing your thoughts risk suicide? Defining what makes you you touches on a corner of philosophy that I'd rather not go into here. Interestingly, Christianity\u00a0has an opinion on this question anyway. The Bible\u00a0repeatedly says, yeah, kill the flesh, die to your sinful nature\u2014that's the whole point\u2014that's the only way the real you gets to be born. The idea is meant to be taken metaphorically rather than philosophically. Jesus makes other hyperbolic statements about the lengths we should go to remove sin from our lives, such as \"If your right eye causes you to sin, gouge it out.\" Compared to that, the free and flawless button I posited seems like the way to go.\u00a0This doesn't mean that I endorse creating or pushing a sanctification button in the practical sense. Such a device or its cybernetic equivalent would necessarily be made by fallible humans and should be judged in that light. But the hypothetically, or perhaps theologically, flawless button\u2014yes, I think I would.</p>"},{"location":"blog/ready-for-a-national-id/","title":"Ready for a National ID","text":"<p>When Yahoo was hacked, we threw away our passwords and got new ones. When Target was hacked, we threw away our credit cards and got new ones. Now that Equifax has been hacked, we'll have to throw out our social security cards and get new ones. Alas, such a thing is not currently possible, and that's a big problem. It's not that we shouldn't have a national ID number. A robust credit system requires (1) a standardized system to identify who owes what so the government knows whose stuff to take if a debt is not paid and (2) a standardized system for recording past and current credit so that borrowers can support their creditworthiness. It was point (2) that got hacked, but it was the design of point (1) that makes the hack such a big problem. The social security number (SSN) is poorly suited for its role. As long as the SSN is both the account number and the unchangeable password for all our financial instruments, we will endure costly and rampant fraud. Just as the size of the Target hack forced the US to finally rethink credit card security, the size of the Equifax hack should force us to rethink our national ID security.</p> <p>The basic silliness of our current system has been well covered by others, but in short, the social security number is merely our de facto national ID in the US, a task for which it was not originally intended and for which it has never been suited. The main problem is that there is no authentication associated with this number\u2014simply knowing the number is considered sufficient proof that you are who you say you are. The number we give to our dental hygienist to organize our records is the same number we recite to receive a loan from the bank. And if you can't trust a hygienist to not take out a loan in your name, who can you trust? Identity theft was a problem before, but now with one of the credit reporting bureaus leaking essentially everyone's information, the existing system will be unworkable. Undoubtedly, Congress will make patches in the near future, but this post is for rethinking the national ID system as a whole.</p>"},{"location":"blog/ready-for-a-national-id/#add-authentication","title":"Add authentication","text":"<p>There are many ways to try to add security to the national ID system. I expect that Congress will add a PIN in the short term. That would be an easy-to-implement stopgap measure because at least it could be changed when stolen. A far better solution would be to associate a password-protected online account. Once there is an internet-facing account, huge swaths of financial transactions can be trivially made secure via a variety of forms of authentication that can be attached to it.\u00a0When signing up for a credit card or a mortgage, the credit card company could send the contract to the account, where it can be digitally signed and returned with the click of a button. Maybe even have all contracts signed this way. A pen signature is a comically bad way to prove that someone if who he says he is\u2014it is easy for a faker to make a signature that looks real, it is easy for a real person to make a signature that looks fake, and no way to check anything at the time of signing. When a contract comes back saying, \"David Hagen, ID#\u00a06487-ed51-10b4-611a, agrees to pay back the loan of $21987 for a car SN# 2b7e-1516-28ae-d2a6,\" the bank can rest assured that the real David Hagen is driving off with the car, and at other times, the real David Hagen can rest assured that no one is driving off with a car under his name.</p> <p>Creating these accounts in the first place would necessarily be a bit onerous\u2014on the order of getting a passport. It would be important to carefully verify that the account being created is for a real person and that the person applying for it is who he says he is. Proving this under the current system is kind of hard (which is why we need a national ID service in the first place), but it can be reasonably done by checking a birth certificate, a social security card, and then taking a photo and fingerprints. Fingerprints in particular would be necessary to prevent duplicates.</p>"},{"location":"blog/ready-for-a-national-id/#use-a-big-number","title":"Use a big number","text":"<p>It would be tempting to simply add authentication to a system that used the social security number as the ID. This would be better than nothing, but the SSN is actually not a good number for ID for several reasons. The first reason is that it is too short. A social security number has 9 decimal digits, which enough to identify 1 billion people. With 450 million numbers already used, 4 million birth a year, and 1.4 million immigrants a year,\u00a0we have about 100 years left before exhaustion. It is tempting to round up to a 32-bit number (about 4 billion numbers), which would last a few hundred years assuming nothing changes drastically with birth rates or immigration. But ultimately, I think we should not make the mistake allocating just enough to meet current needs, kicking the can to a future that may come sooner than expected. Let's allocate enough numbers so that we will never run out. I propose using a 64-bit number, which is sixteen hexadecimal digits like\u00a06487-ed51-10b4-611a, which is still quite memorizable, especially as this could replace every other ID I have ever received. At current birth rates this would last trillions of years. Not only is this enough for the USA forever; at a worldwide\u00a0birth rate of 130 million, we could number all of humanity for hundreds of billions of years. The US could draw from a single batch of numbers to classify visitors and visa holders, those who would not normally receive social security numbers. Ultimately, I would welcome other nations to join a common numbering system so that people could use the same ID no matter where they went.</p>"},{"location":"blog/ready-for-a-national-id/#make-the-number-completely-random","title":"Make the number completely random","text":"<p>Another problem with the SSN is that there is so much structure to it. Until 2011, numbers were allocated sequentially based on location. So for someone born before 2011, his SSN reveals approximately where and when he was born. Conversely, knowing when and where someone was born makes it possible to make a good guess at his SSN. As a society, we may or may not want the circumstances of all citizens' birth to be public knowledge, but the structure of the ID number should not make it impossible for anyone to hide this or any other information. The ID number should do one thing and do it well: act as a unique ID number. Therefore, each person's number should be generated completely randomly.</p>"},{"location":"blog/ready-for-a-national-id/#add-a-checksum","title":"Add a checksum","text":"<p>Another problem with the SSN is that making a error when writing the number down is very likely to result is a valid SSN, just somebody else's. It is standard practice to add a couple of redundant digits to any important ID number. Adding n redundant digits typically allows for detecting an error if fewer than n digits are wrong. Two redundant digits is usually good because the two most common errors, writing one digit wrong or reversing two digits, will always be detected. If two of the 16 digits are redundant, the pool would still be big enough to last for billions of years.</p>"},{"location":"blog/ready-for-a-national-id/#create-a-national-address-book","title":"Create a national address book","text":"<p>When I moved recently, I had to type or write my new address dozens of times to change it on various accounts. I wished that there was one authoritative place where I could put my address and all these businesses who already knew who I was could just look it up. The national ID system will already be a big database of people; why not add an option for people to store and update their residential address and mailing address? Such a system could also be used for all kinds of things, such as to prevent voting in multiple states. Unlike impersonation which gets all the media attention, this is a form of voting fraud that actually happens.</p> <p>It's not just addresses that should be consolidated. There is lots of information that I have to tell the government over and over (phone number, email address, children, spouse, etc.) because it doesn't store it all in one spot. I am sure there will be privacy objections about creating such a database, but I posit that this will increase privacy rather than decrease it. This is information that the government already stores; it just stores it in many places with varying or perhaps undefined security. Is the WIC database as secure as the IRS database? With a central database, the attack surface is reduced and all data can be stored at maximum security at minimum cost. If you are trying to keep your love child a secret from your coworkers, the IRS can probably be trusted to keep your child support payments secret, but what about the elementary school's emergency contacts list? A central database also forces decisions to be actually made about what is public and what is private and even allows a citizen to make that decision on an individual basis. Most people don't have a desire to keep their address a secret. People tend to forget that not that long ago everyone's address and phone number was published yearly in a giant book and thrown on the doorstep of every house. But some, such as those being pursued by ex-lovers or criminal enterprises, want to keep their address a secret. But there is currently no good way to keep an address secret without severely curtailing one's interactions. Businesses buy, sell, and trade personal information with minimal regulations and enforcement. Want to keep our address a secret? Better not have any magazine subscriptions, ordering anything online, or vote. But if there was a national address book and especially if everyone was public by default, there would simply be no value in buying or selling something that was available for free. Those who opted out for whatever reason would end up with higher privacy than they currently have.</p>"},{"location":"blog/ready-for-a-national-id/#value-added","title":"Value added","text":"<p>The existence of the ZIP Code has been estimated to be worth about $10 billion per year. This system condenses the ambiguous and constantly changing set of addresses into a fixed and finite set of approximate locations. Want to calculate your shipping costs? ZIP Code. Want to add a password to credit cards from out of town? ZIP Code. Want to get a car insurance estimate? ZIP Code. These things could be done without the ZIP Code, but its existence makes them cheaper and easier for both businesses and consumers. Public standards reduce cost and complexity and right now a national ID is the lowest hanging fruit on public standards tree. There is an easy opportunity here for reducing fraud and increasing convenience.</p>"},{"location":"blog/superior-pairing-function/","title":"Superior Pairing Function","text":"<p>Given two sequences of objects, it is often desirable to generate a sequence which is all possible pairwise combinations of those sequences\u2014the Cartesian product. If the sequences are finite in length, then it is a trivial function to write in any programming language. The function even exists in many standard libraries and packages, such as <code>itertools.product</code>\u00a0in Python. But if the sequences are infinite in length (that is, they are streams rather than arrays, depending on your terminology), the typical approach fails. Finding a way to iterate over all pairs of two infinitely long sequences is called a \"pairing function\" in mathematics and has practical uses. There are some existing pairing functions, but many have limitations. I describe the properties of a superior pairing function and a couple of methods that satisfy them.</p>"},{"location":"blog/superior-pairing-function/#finite-cartesian-product","title":"Finite Cartesian product","text":"<p>Before considering pairing functions for infinite streams, let's consider them for finite streams first. Technically, it is not a pairing function if it does not work on infinite streams, but for illustrative purposes, I'll use the same terminology to describe any function that takes two indexes as its arguments and returns the index assigned to that pair. This is a fairly simple task for finite streams, and it illustrates the difficulties with handling infinite streams.</p> <p>Whether dealing with the finite case or the infinite case, there exist different methods for indexing each pair. For example, with a 3 by 2 matrix, column-major ordering will say the entry in the second row and second column is the 5th element, while row-major ordering will say it is the 4th element.\u00a0As long as each method is a one-to-one mapping of <code>(x, y)</code>\u00a0to <code>i</code>, then it is a valid pairing function.\u00a0In fact, there is a symmetry between the <code>x</code> and <code>y</code>\u00a0values. One could swap the <code>x</code>\u00a0and <code>y</code>\u00a0values in any method and still have a valid pairing function (column-major and row-major ordering are mirrored in this way). (In the literature, a method may be described as one version or the other. In this post, I will always choose the version that starts out <code>[[0,0], [1,0], ...]</code> rather than the one that starts <code>[[0,0], [0,1], ...]</code>\u00a0because that is the one Wikipedia picked.)</p> <p>Each method is a single one-to-one mapping, but there are three programmatic functions that we might need depending on how we want to get values out of it:</p> <ul> <li>Pairing function <code>(Index, Index) -&gt; Index</code> which takes two indexes into streams and returns the index into the Cartesian product stream</li> <li>Unpairing function <code>(Index) -&gt; [Index, Index]</code> which takes an index into the Cartesian product stream and produces two indexes into the individual streams</li> <li>Cartesian product function <code>(Iterable[A], Iterable[B]) -&gt; Iterable[[A, B]]</code> which takes two streams and produces a stream of all possible combinations</li> </ul> <p>For simplicity of math, I will be using 0-indexing for all of the pairing and unpairing functions even though 1-indexing may be more natural. In languages with 1-indexing, simply add\u00a0<code>+1</code>\u00a0and <code>-1</code> where needed. For simplicity of implementation, I'll use Python generator syntax for creating the Cartesian products even though other Iterable designs may be better for general programming.</p> <p>I will be making use of 2D plots to illustrate the pairing functions. The index into one stream will be <code>x</code> and the index into the other stream will be <code>y</code>. The possible pairs of indexes will be laid out as points on the grid. A pairing function can be thought of as stepping from one point to the next. By requirement of being a one-to-one mapping, it must land on each point exactly once\u2014no more, no less. The order that each method steps through the points is what makes the method special, so they will be labeled with both numbers and arrows.</p>"},{"location":"blog/superior-pairing-function/#box-pairing-function","title":"Box pairing function","text":"<p>If I have two finite lists of objects, the naive solution to the Cartesian product is to simply create a nested loop, pairing each element of the first list with the first element of the second list, then pairing each element of the first list with the second element of the second list, and so on. Intuitively, this walks down one complete row or column of a matrix (depending on your orientation) and then walks down the next row or column. This finite Cartesian product is illustrated in the figure below, where each of the possible pairs from a list of length 3 and a list of length 2 are labeled 0 to 5.</p> <pre><code>def box_product(stream1: Iterable, stream2: Iterable):\n  for element2 in stream2:\n    for element1 in stream1:\n      yield [element1, element2]\n\ndef box_pairing(length1: Integer, length2: Integer,\n    index1: Index0, index2: Index0):\n  return length2 * index1 + index2\n\ndef box_unpairing(length1: Integer, length2: Integer, index: Index0):\n  index1 = floor(index / length2)  # integer division\n  index2 = mod(index, length2)  # modulo operation\n  return [index1, index2]\n</code></pre> <p>Calling <code>box_product([1,2,3], [4,5])</code>\u00a0produces the stream <code>[[1,4], [2,4], [3,4], [1,5], [2,5], [3,5]]</code>. Each possible pair of elements between <code>[1,2,3]</code>\u00a0and <code>[4,5]</code>\u00a0are produced.</p>"},{"location":"blog/superior-pairing-function/#infinite-cartesian-product","title":"Infinite Cartesian product","text":"<p>The problem with the box method is that it does not work if the first stream is infinite. The first step, pairing each element of the first list with the first element of the second list, is infinitely long, so the second element of the second list is never reached.</p>"},{"location":"blog/superior-pairing-function/#cantor-pairing-function","title":"Cantor pairing function","text":"<p>If we have two infinite sequences, then all possible pairs form an infinite two-dimensional grid of points, which we are trying to unravel into one dimension. Going row-by-row or column-by-column will never work because there is no end to any row or column. Intuitively, we need some systematic method of starting in the corner and working our way out. George Cantor, the 1800s mathematician, proved this is possible with his famous pairing function.</p> <p>Cantor's solution to the problem was fairly straightforward; don't walk row by row or column by column, but start in the corner and walk along each diagonal.</p> <pre><code>def cantor_product(stream1: Iterable, stream2: Iterable):\n  start_index = 0\n  sequence1 = stream1.lazy_sequence()  # Turns a stream into indexable\n  loop:\n    index1 = start_index\n    index2 = 0  # Not used, but included for reference\n    for element2 in stream2:\n      element1 = sequence1(index1)  # Gets the ith element of the sequence\n      yield [element1, element2]\n      if index1 == 0:\n        break\n      else:\n        index1 = index1 - 1\n        index2 = index2 + 1\n    start_index = start_index + 1\n\ndef cantor_pairing(index1: Index0, index2: Index0):\n  return (index1 + index2) * (index1 + index2 + 1) / 2 + index2\n\ndef cantor_unpairing(index: Index0):\n  w = floor((sqrt(8 * index + 1) - 1) / 2)\n  t = (w ^ 2 + w) / 2\n  index2 = index - t\n  index1 = w - index2\n  return [index1, index2]\n</code></pre>"},{"location":"blog/superior-pairing-function/#szudzik-pairing-function","title":"Szudzik pairing function","text":"<p>One disadvantage of walking along the diagonal is that extreme values of the streams are used before some less extreme values. For example, if we ask for the first four elements of the Cartesian product between <code>[0,1,2,...]</code> and <code>[0,1,2,...]</code>, then we might expect that we would get the elements <code>[[0,0], [1,0], [0,1], [1,1]]</code>, not necessarily in that order. But with <code>cantor_product</code>, we get <code>[[0,0], [1,0], [0,1], [0,2]]</code>. The <code>[0,2]</code> element appears before the <code>[1,1]</code> element. It is undesirable in some situations to see the element <code>x</code>\u00a0of a stream before all combinations that include element <code>x-1</code> have been exhausted.</p> <p>Requirement 1: When element <code>[x,y]</code>\u00a0is produced by the Cartesian product stream, all elements <code>[a,b]</code>\u00a0where <code>max(a,b) &lt; max(x,y)</code>\u00a0have already been produced.</p> <p>Another disadvantage is that the generator cannot run efficiently because it must walk the first iterable backwards. I use a <code>lazy_sequence</code>\u00a0method to cheat on this; it is meant to represent a sequence that iterates over a stream once and caches all the values it generates so that future index lookups are fast. Unless the iterables are such that they can be walked backwards and forwards, the generator must either store <code>O(sqrt(index))</code> elements\u00a0or use <code>O(sqrt(index))</code>\u00a0time to fast forward to the <code>index1</code>\u00a0element each time.</p> <p>Matthew\u00a0Szudzik recently invented a new pairing function\u00a0that intentionally avoids the first disadvantage of Cantor's pairing function and (I think) unintentionally avoids the second as well. It was constructed to ensure that for each stream, no index\u00a0<code>x</code>\u00a0appears before all possible combinations of <code>x-1</code>\u00a0have been exhausted. It does this by walking down the edges of successively larger squares as illustrated below. I call the current square the \"shell\" and its two edges the \"legs\".</p> <pre><code>def szudzik_product(stream1: Iterable, stream2: Iterable):\n  shell = 0\n\n  loop:\n    index2 = 0\n    for element2 in stream2:\n      if index2 == shell:\n        max_element2 = element2\n        break\n      yield [max_element1, element2]\n      index2 = index2 + 1\n\n    index1 = 0\n    for element1 in stream1:\n      if index1 &gt; shell:\n        max_element1 = element1\n        break\n      yield [element1, max_element2]\n      index1 = index1 + 1\n\n    shell = shell + 1\n\ndef szudzik_pairing(index1: Index0, index2: Index0):\n  if index1 &gt; index2:\n    return index1 ^ 2 + index2\n  else:\n    return index2 ^ 2 + index2 + index1\n\ndef szudzik_unpairing(index: Index0):\n  shell = floor(sqrt(index))\n  if index - shell ^ 2 &lt; shell:\n    return [shell, index - shell ^ 2]\n  else:\n    return [index - shell ^ 2 - shell, shell]\n</code></pre>"},{"location":"blog/superior-pairing-function/#peter-pairing-function","title":"Peter pairing function","text":"<p>Edit: I had derived this myself, but after this post was first published, it was brought to my attention that\u00a0Rozsa Peter had derived a pairing function with the same sequence using a pair of recursive functions in Recursive Functions (1951), page 44. I have yet to find an online reference to this pairing function.</p> <p>While the Szudzik pairing function is useful in that it completes shell <code>s</code>\u00a0before continuing to the next shell, it completes each leg one at a time. This means that Cartesian product stream is biased toward containing more of the larger indexes of the first stream than the second stream until a shell completes. For example, if I ask for the first six elements of the Cartesian product of <code>[0,1,2,...]</code>\u00a0and <code>[0,1,2,...]</code>\u00a0then I might expect to get <code>[[0,0], [1,0], [0,1], [1,1], [2,0], [0,2]]</code>. But with <code>szudzik_product</code>\u00a0I get\u00a0<code>[[0,0], [1,0], [0,1], [1,1], [2,0], [2,1]]</code>, because in shell <code>s</code>\u00a0I get all the <code>[s,y]</code>\u00a0elements before any of the <code>[x,s]</code>\u00a0elements. If I always take <code>s^2</code>\u00a0elements, then it does not matter, but if I often take a slice of unrelated length, then it would be nice if the sequence was not biased toward one of the legs.</p> <p>Requirement 2: When element <code>[x,y]</code>\u00a0is produced by the Cartesian product stream, the difference between the number of times <code>x</code>\u00a0or <code>y</code> has appeared in the first position versus the second position is at most 1.</p> <p>How would this be solved? Instead walking down one entire leg and then the other, I alternate between legs with each step until the shell is complete as illustrated below.</p> <pre><code>def peter_product(stream1: Iterable, stream2: Iterable):\n  shell = 0\n  max_element1 = stream1.iterator().next()\n  max_element2 = stream2.iterator().next()\n\n  loop:\n    index1 = 0 # Not used, just for reference\n    index2 = 0\n    leg1 = stream1.iterator()\n    leg2 = stream2.iterator()\n\n    loop:\n      yield [max_element1, leg2.next()]\n      index2 = index2 + 1\n\n      if index2 &gt; shell:\n        leg1.next()\n        max_element1 = leg1.next()\n        max_element2 = leg1.next()\n        break\n\n      yield [leg1.next(), max_element2]\n      index1 = index1 + 1\n\n    shell = shell + 1\n\ndef peter_pairing(index1: Index0, index2: Index0):\n  shell = max(index1, index2)\n  step = min(index1, index2)\n  if step == index2:\n    flag = 0\n  else:\n    flag = 1\n  return shell ^ 2 + step * 2 + flag\n\ndef peter_unpairing(index: Index0):\n  shell = floor(sqrt(index))\n  remainder = index - shell ^ 2\n  step = floor(remainder / 2)\n  if mod(remainder, 2) == 0:  # remainder is even\n    return [shell, step]\n  else:\n    return [step, shell]\n</code></pre>"},{"location":"blog/superior-pairing-function/#alternative-pairing-function","title":"Alternative pairing function","text":"<p>While the previous method alternates between legs as it walks down them, it always starts on the same leg, the leg with element <code>[s,0]</code>. An alternative method would also alternate between starting with\u00a0<code>[0,s]</code>\u00a0and <code>[s,0]</code>. This won't change the maximum imbalance, which is still one more element in one leg than another, it just changes it so that the <code>[0,s]</code>\u00a0leg sometimes has one more element than the other. Whether this is a better method is probably a matter of taste.</p> <pre><code>def alternative_product(stream1: Iterable, stream2: Iterable):\n  shell = 0\n  flag = true\n  max_element1 = stream1.iterator().next()\n  max_element2 = stream2.iterator().next()\n\n  loop:\n    index1 = 0\n    index2 = 0\n    leg1 = stream1.iterator()\n    leg2 = stream2.iterator()\n\n    loop:\n      flag = not flag\n\n      if flag:\n          yield [max_element1, leg2.next()]\n          index2 = index2 + 1\n          if index2 &gt; shell:\n            leg1.next()\n            max_element1 = leg1.next()\n            max_element2 = leg2.next()\n      else:\n          yield [leg1.next(), max_element2]\n          index1 = index1 + 1\n          if index2 &gt; shell:\n            leg2.next()\n            max_element2 = leg2.next()\n            max_element1 = leg1.next()\n\ndef alternative_pairing(index1: Index0, index2: Index0):\n  shell = max(index1, index2)\n  step = min(index1, index2)\n  if mod(shell, 2) == 0 and step == index1\n      or mod(shell, 2) == 1 and step == index2:\n    flag = 0\n  else:\n    flag = 1\n  return shell ^ 2 + step * 2 + flag\n\ndef alternative_unpairing(index: Index0):\n  shell = floor(sqrt(index))\n  step = floor((index - shell ^ 2) / 2)\n  if mod(index, 2) == 0:  # index is even\n    return [step, shell]\n  else:\n    return [shell, step]\n</code></pre>"},{"location":"blog/superior-pairing-function/#conclusion","title":"Conclusion","text":"<p>I asserted that this was a practical exercise, and you may be wondering what practical purpose making such fine distinctions between methods could serve.</p> <p>The first requirement was the 4th element from either component stream could not appear in the Cartesian product stream until all pairs involving the 1st, 2nd, and 3rd elements from both had been exhausted. The Szudzik method had already incorporated this requirement, because it was a more practical generator of SK-combinator expressions.\u00a0For an example closer to my biological realm, consider that I have two base classes of drugs and an unbounded number of undeveloped modifications schemes. Once I develop a modification scheme, I can apply it to both classes of drugs to make a variant of each, but each development is costly. I wish to find a combination therapy that is effective against some disease by successively trying pairs of variants in some standard experiment. Because a modification scheme is costly to develop, then I will want to try all combinations involving the variants based on modifications I already have before developing a new scheme. Now in the real world, I suppose I could just find all combinations of my existing schemes by hand, but if I am simulating this whole process, then I need a function that correctly and reproducibly generates pairs for the experiment.</p> <p>What about the requirement that the 4th element from one stream cannot appear 3 times before the 4th element from the other stream appears at least 2 times? Say that the failures in the pairwise experiments are correlated; that is, if the 4th variant of a drug fails in an experiment, then any experiments containing the 4th variant are also more likely to fail. That means I should first try combinations that involve variants that have been tried the least, for instance, the 4th variant of the other drug. Combined with the previous rule, this means I should alternate which drug class I take the new variant from and which drug class I take an older variant from to be its pair.</p>"},{"location":"blog/superior-pairing-function/#the-code-used-to-generate-the-figures-in-this-post-is-available-on-github-included-is-python-versions-of-all-pairing-unpairing-and-cartesian-product-functions-described","title":"The code used to generate the figures in this post is available on Github. Included is Python versions of all pairing, unpairing, and Cartesian product functions described.","text":""},{"location":"blog/multidimensional-pairing-functions/","title":"Multidimensional Pairing Functions","text":"<p>In the previous post, I compared ways to take two infinite streams and generate a new stream that is all possible combinations of the elements in those streams. This post takes it up another level and generalizes this procedure to an arbitrarily long list of infinite streams. This is a trickier task than the 2-dimensional case, utilizing recursion into each dimension to cleanly generate all combinations.</p> <p>Throughout this post, the caret <code>^</code>\u00a0will indicate exponentiation and parentheses <code>list(index)</code>\u00a0will be used to indicate indexing a list. As before, 0-indexing will be used because it makes the math simpler.\u00a0</p>"},{"location":"blog/multidimensional-pairing-functions/#multidimensional-box-pairing-function","title":"Multidimensional box pairing function","text":"<p>Many programming languages have a function that will generate the Cartesian product of <code>n</code>\u00a0lists of objects. In Python, it is called\u00a0<code>itertools.product</code>.\u00a0The standard implementation starts with the first element from each list. Then it takes the second element from the last list and the first element from the remaining lists. It continues walking through the last list until it is exhausted. Then it increments the second to last list by one element and walks through the last list again.\u00a0 This is repeated until the first list (which is walked slowest) is finally walked through once; at that point, all possible combinations have been generated. An algorithm to do this is straightforward to implement. It can with be done imperatively by updating a mutable list of indexes (like the Python version) or it can be done functionally using a recursive function (like the version below).</p> <pre><code>def multidimensional_box_product(*streams: List[Iterable]) -&gt; Iterable[List]:\n  def recursive_product(i_stream) -&gt; Iterable[List]:\n    for element in streams(i_stream):\n      if i_stream == streams.indexes.last:\n        # On final dimension, yield each element\n        yield [element]\n      else:\n        # On other dimensions, yield each element followed by each\n        # possible combination of elements in the remaining dimensions\n        for remaining_elements in recursive_product(i_stream + 1):\n          yield [element] ++ remaining_elements\n\n  for elements in recursive_product(0):\n    yield elements\n\n\ndef multidimensional_box_pairing(lengths: List[Integer],\n                                 indexes: List[Integer]) -&gt; Integer:\n  # Should assert that all indexes are less than corresponding lengths\n  index = 0\n  dim_product = 1\n  # Compute indexes from last to first because that is the order the\n  # product is grown\n  for dim in lengths.indexes.reverse:\n    index += indexes(dimension) * dim_product\n    dim_product *= lengths(dimension)\n\n  return index\n\n\ndef multidimensional_box_unpairing(lengths: List[Integer],\n                                   index: Integer) -&gt; List[Integer]:\n  # Should assert that index is less than the product of lengths\n  indexes = List.fill(length=lengths.length, element=0)  # Preallocate list\n  dim_product = 1\n  # Compute indexes from last to first because that is the order the\n  # product is grown\n  for dim in lengths.indexes.reverse:\n    indexes(dim) = mod(floor(index / dim_product), lengths(dim))\n    dim_product *= lengths(dim)\n\n  return indexes\n</code></pre>"},{"location":"blog/multidimensional-pairing-functions/#multidimensional-szudzik-pairing-function","title":"Multidimensional Szudzik pairing function","text":"<p>The two-dimensional Szudzik pairing function first divides the x-y plane into shells, each shell defined by <code>max(x,y)</code>. This is a straightforward property to generalize. The n-dimensional space is divided into n-dimensional shells defined by <code>max(x1, x2, ..., xn)</code>. Next, the two-dimensional Szudzik pairing function walks down each leg of the shell until the <code>x==y</code>\u00a0element is reached. There are several ways to generalize this property.</p> <p>A three-dimensional shell will have three faces. One sensible way to fill the shell would be to fill each face one at a time, treating each face as a plane to be filled with the two-dimensional Szudzik pairing function. Such a method could be recursively scaled up to arbitrary dimensions.</p> <p>However, the original presentation briefly suggests in the penultimate slide a different generalization: pure lexicographical ordering within a shell. No implementation of the pairing function, unpairing function, or Cartesian product function is shown. But because this generalization is perfectly reasonable and the implementations straightforward to construct, it is the one I'll show here.</p> <p>The top level of the implementation is an infinite loop iterating over the shells, starting from the innermost shell. In each shell, all possible combinations of the streams are generated by taking each element in the first stream (until the shell is reached) and appending all possible combinations of the remaining streams, which is a recursive process. By itself, this recursive process would generate the Cartesian product of all elements within the cube bounded by the shell, like a multidimensional finite Cartesian product function. To restrict this process to just the elements on the shell, the process keeps track of whether or not an element on the shell has been emitted as it recurses. If it has not been emitted yet when the recursion reaches the final stream, only the final (shell) element of that stream is emitted, not the entire stream up to the shell, so that it guarantees that only points with at least one index on the shell are emitted within that shell's iteration.\u00a0</p> <pre><code>def multidimensional_szudzik_product(*streams: List[Iterable])-&gt;Iterable[List]:\n  n = streams.length\n\n  # In the recursive function, it must possible to get the shell\n  # element of the final stream. This iterator is stepped through at\n  # each shell to produce final_shell_element.\n  final_shell_iterator = streams.last.iterator()\n\n  def recursive_product(i_stream: Integer,\n                        shell_used: Boolean) -&gt; Iterable[List]:\n    if i_stream == n - 1:\n      # At the final dimension\n      if shell_used:\n        # A shell element was emitted earlier in the item so emit all elements\n        for [i_element, element] in enumerate(streams(i_stream).take(shell+1)):\n          yield [element]\n      else:\n        # No shell element has been emitted for this item yet and the\n        # recursive function is at the last stream, so a shell element\n        # only must be emitted or else this item will not be on the shell.\n        yield [final_shell_element]\n    else:\n      for [i_element, element] in enumerate(streams(i_stream).take(shell+1)):\n        next_dim = i_stream + 1\n        next_shell_used = shell_used or i_element == shell\n\n        for remaining_elements in recursive_product(next_dim, next_shell_used):\n          yield [element] ++ remaining_elements\n\n  shell = 0\n  loop:\n    final_shell_element = final_shell_iterator.next()\n    for elements in recursive_product(0, False):\n      yield elements\n    shell = shell + 1\n\n\ndef multidimensional_szudzik_pairing(*indexes: List[Integer]) -&gt; Integer:\n  n = indexes.length\n  shell = max(indexes)\n\n  def recursive_index(dim: Integer) -&gt; Integer:\n    # Number of dimensions of a slice perpendicular to the current axis\n    slice_dims = n - dim - 1\n\n    # Number of elements in a slice (only those on the current shell)\n    subshell_count = (shell + 1) ^ slice_dims - shell ^ slice_dims\n\n    index_i = indexes(dim)\n    if index_i == shell:\n      # Once an index on the shell is encountered, the remaining\n      # indexes follow multidimensional box pairing\n      lengths = List.fill(element=shell + 1, length=slice_dims)\n      return subshell_count * shell + multidimensional_box_pairing(\n          lengths, indexes.slice(from=dim+1))\n    else:\n      # Compute the contribution from the next index the same way\n      # by recursing to the next dimension\n      return subshell_count * index_i + recursive_index(dim + 1)\n\n  # Start with the number of elements from before this shell and recursively\n  # find the contribution from each index to the linear index\n  return shell ^ n + recursive_index(0)\n\n\ndef multidimensional_szudzik_unpairing(n: Integer,\n                                       index: Integer) -&gt; List[Integer]:\n  shell = floor(index ^ (1 / n))\n\n  def recursive_indexes(dim: Integer, remaining: Integer) -&gt; List[Integer]:\n    if dim == n - 1:\n      # If this is reached, that means that no index so far has been on\n      # the shell. By construction, this index must be on the shell or\n      # else the point itself will not be on the shell.\n      return [shell]\n    else:\n      # Number of dimensions of a slice perpendicular to the current axis\n      slice_dims = n - dim - 1\n\n      # Number of elements in a slice (only those on the current shell)\n      subshell_count = (shell + 1) ^ slice_dims - shell ^ slice_dims\n\n      index_i = min(floor(remaining / subshell_count), shell)\n      if index_i == shell:\n        # Once an index on the shell is encountered, the remaining\n        # indexes follow multidimensional box unpairing\n        lengths = List.fill(element=shell + 1, length=slice_dims)\n        still_remaining = remaining - subshell_count * shell\n        return [shell] + multidimensional_box_unpairing(lengths,\n                                                        still_remaining)\n      else:\n        # Compute the next index the same way by recursing to the next\n        # dimension\n        still_remaining = remaining - subshell_count * index_i\n        return [index_i] + recursive_indexes(dim + 1, still_remaining)\n\n  # Subtract out the elements from before this shell and recursively\n  # find the index at each dimension from what remains\n  return recursive_indexes(0, index - shell ^ n)\n</code></pre>"},{"location":"blog/multidimensional-pairing-functions/#conclusion","title":"Conclusion","text":"<p>I had spent considerable time making a multidimensional version of the Peter pairing function. I had succeeded in writing the product function, got most of the way done with the unpairing function, and was started on the pairing function. However, before I finished, I became convinced that the Szudzik pairing function was better suited for my real-life problem. A multidimensional pairing function that meets only requirement 1 from the previous post (i.e. do it by shells) is much easier to implement than one that also meets requirement 2 (i.e. alternate between legs).</p>"},{"location":"blog/multidimensional-pairing-functions/#the-code-used-to-generate-the-figures-in-this-post-is-available-on-github-in-the-same-repository-as-the-previous-post-python-versions-of-the-pairing-unpairing-and-cartesian-product-functions-are-included","title":"The code used to generate the figures in this post is available on Github in the same repository as the previous post. Python versions of the pairing, unpairing, and Cartesian product functions are included.","text":""},{"location":"blog/finally-a-synthetic-organism-that-worries-me/","title":"Finally, a synthetic organism that worries me","text":"<p>It was recently in the news that researchers have genetically engineered tobacco with 40% more efficient photosynthesis. I had first seen this kind of research at a seminar during my graduate years at MIT. The presenter noted that the genetically engineered plants grew faster and showed memorable side-by-side pictures of scrawny-looking normal plants next to their larger and lusher engineered brothers. I tried to find out more after the fact, but had forgotten the name of the presenter and lab. When I searched online for the research, I found lots of people proposing doing this kind of thing, but not the lush success story I had just seen. I wanted to learn more not because it was super cool and hugely important (which it was), but because it was the first (and to date only) example of what I thought was a dangerous genetically modified organism.</p>"},{"location":"blog/finally-a-synthetic-organism-that-worries-me/#unambiguously-safe-engineering","title":"Unambiguously safe engineering","text":"<p>Back when I was working in the world of genetically engineered organisms, I would often talk to non-scientists about actual and potential uses of synthetic biology. Invariably, one question always came out in those conversations: But what if one accidentally escaped, wouldn't that be a disaster? Anyone who works in genetic engineering and synthetic biology has heard this question many times. It prompts a sigh, then a firm no, and if we are feeling patient, an explanation that that is not how ecology works.</p> <p>The PEPCK supermouse we made in Richard Hanson's lab ran faster, ran longer, lived longer, and was fertile for longer than a normal mouse. When people learned about this mouse, they would sometime worry what would happen if such a mouse escaped. Would the city be overrun by fast, immortal, virile rodents? No, that is simply impossible. These mice were created by over-expressing a single protein in their muscles. If over-expression of this protein would create super fit mice (evolutionarily not athletically), then it would have already happened by random chance. Our data actually revealed the fatal flaw: the mice required a lot more food. An escaped supermouse would have quickly starved to death.</p> <p>Fundamentally, if an organism escapes in the environment, it must compete with similar organisms in that environment in order to survive. Any simple change we make to an organism is just not going to improve the fitness of that organism relative to the wild type. Any increase or decrease of existing genes cannot make the organism more fit or evolution would have already done it. This includes gene knockouts, the most popular type of genetic engineering. If removing one or more genes made the organism more competitive, it would have already happened.</p> <p>The issue is the same for changes that introduce new genes, like adding vitamin A to rice, programming bacteria to make gasoline, or making fish glow-in-the-dark. There is no reason to think that these changes would do anything but lower the fitness of the organisms in the wild. In a survival-of-the-fittest world, survive they would not.</p>"},{"location":"blog/finally-a-synthetic-organism-that-worries-me/#improving-photosynthesis","title":"Improving photosynthesis","text":"<p>The core enzyme of photosynthesis is rubisco. This is the enzyme that takes carbon dioxide out of the air and incorporates it into sugar. As anyone familiar with chemistry knows, this is a ridiculously difficult reaction to preform and it is a miracle that life does it at all. It is made even more difficult because oxygen likes to undergo the same reaction. Oxygen is smaller in size (two atoms versus carbon dioxide's three) and is in higher concentration (21% versus carbon dioxide's 0.04%) so it gets into the active site of the enzyme and attaches to the sugar instead. This poisons the sugar and the plant is forced to spend a lot of energy removing the oxygen through a process known as photorespiration. For years, protein engineers tried to improve photosynthesis by tweaking rubisco to either attach carbon dioxide better and oxygen worse in order to improve photosynthesis in desirable plants. This failed despite tremendous effort, and in retrospect, it is obvious that it should have. Any possible improvement to this molecule would have already been made in its multibillion-year history.</p> <p>So if evolution is so efficient, how did synthetic biologists manage to improve photosynthesis in the case that made the news? Well, I used the terms \"simple\" and \"tweak\" and \"existing\" to describe the changes that evolution can make. If an improvement can only be obtained by supplying several new genes all at once, evolution is ineffective because the odds are too low that all of the required genes would arrive at once. The recent tobacco paper made several improvements to photorespiration, including one that was made to arabidopsis previously. This improvement took the glycolate oxidation pathway out of E. coli and put it into the chloroplast of the plant. This alone boosted growth by 10%. Plants have been around billions of years, yet not once did a plant evolve this game-changing attribute. This is because it took transferring five genes at once, a feat that is simply beyond the capabilities of natural evolution.</p>"},{"location":"blog/finally-a-synthetic-organism-that-worries-me/#my-concern","title":"My concern","text":"<p>So why am I worried about this plant? Because the safety of synthetic biology has long rested on the assumption that the ecology is at a stable equilibrium, the native organisms are already optimized for their environment. This work to improve photorespiration appears to violate that assumption. When we make mice that can run faster than all the other mice out there or bacteria that can make oil better than all the other bacteria out there, there is no risk to the biosphere. But when we make a plant that is better at growing and utilizing energy and carbon dioxide than all the other plants out there, I am unable to convince myself that this is still safe. If one of these plants escapes, why would it not slowly spread as it outcompeted all the plants of similar size? Each species is specially adapted for its niche, which makes it hard for other species to enter, but 40% better energy utilization is an unrivaled superpower.</p> <p>The research so far been on the arabidopsis and tobacco. Neither of these plants would I like to see everywhere. If the researchers accomplish their next goal of engineering food crops, maybe it would not be that bad of an outcome to accidentally blanket the planet with food. But I don't think that is even close the worst-case scenario. Genes can hop between plants through a process called horizontal gene transfer. We may make a prolific wheat plant with a genetic construct, but it is doubtful that wheat is the most prolific plant that could possibly hold that construct. Those genes will eventually be stolen by other photosynthetic lifeforms. When the five genes were spread around the E coli genome, the odds were vanishingly small that they would all transfer at once to a welcoming plant host. But to build the construct the researchers connected all the genes together in a single short segment of DNA and modified all the proteins so that they would end up in the chloroplast. I don't know the overall rate of horizontal gene transfer from plants, but a single segment of DNA is certainly more likely to jump species. If we start growing crops with this construct, eventually pieces of those plants will end up in the ocean, and eventually some algae will absorb the DNA and receive an competitive advantage the world has not seen in eons. This is the outcome that worries me the most.</p>"},{"location":"blog/finally-a-synthetic-organism-that-worries-me/#conclusion","title":"Conclusion","text":"<p>It may sound like I think this research should be stopped. But that is not at all what I am trying to get at. I am simply not knowledgeable enough about the details of this engineering or ecology to make recommendations. I want to raise this issue because I don't see anyone else raising it. The best outcome of this post is that someone points out why I am wrong about the dangers of improving photosynthesis. Maybe photorespiration is important for the health of offspring. Maybe photorespiration is important for robustness to changes in the environment. Things like these may doom a genetically engineered plant that escaped but would still leave the plant useful to us. There is a huge upside to this research if that 40% increase in biomass can be translated to food mass in crops. People who are starving will be fed. People everywhere will see the cost of their food go down.</p> <p>I think that the chance of a worst-case scenario (a single organism overrunning a huge chunk of the biosphere) is low but not zero. One mitigation would be to split the genes into different DNA segments before putting them into plants will be grown out in the field. Splitting the genes up will eliminate the risk of horizontal gene transfer. A plant already bred for food is far less likely to spread uncontrollably than an algae that picked up the construct.</p> <p>I want to see this research continue. The potential benefit to humanity is too great to simply stop. But I would love to see experts address or mitigate the dangers before unleashing this on the world's fields.</p>"},{"location":"blog/leap-years-something-the-gregorian-calendar-gets-right/","title":"Leap Years: Something the Gregorian Calendar Gets Right","text":"<p>Calendars coordinate people with people. It is better to be on vacation the same week your family is also. It is better for kids to be in school the same days as their teachers. It is better to be at work when everyone else is. (Though it is worse to be driving to said work when everyone else is.) In the modern world, it can be easy to think that coordinating people with people is all calendars do. If that were all, we could certainly do with a much simpler calendar\u20144 weeks to a month, 12 months to a year, no leap days, no irregularities forever. I won't argue that it couldn't be simpler, but I will argue that it cannot be perfectly simple. Because calendars also coordinate people with nature.</p> <p>Seconds, minutes, and hours divide time into somewhat arbitrary periods; that is, there is no physical process with which they are meant to be synchronized. However, days and years in the Gregorian calendar and months in lunar calendars are intended to be synchronized with tangible astronomical processes. It is nice if the sun reaches its zenith at roughly the same time each day. It is nice if the sun reaches its equinox on roughly the same day each year. It is nice if the moon is full on roughly the same day each month.</p> <p>Synchronizing with astronomical events is an indispensible when those events have a large impact on human behavior. The day is by far the most important. If you tell me you would like to have a meeting at 8:00 AM, I know that that is in the morning, early, but not too early. The circadian rhythm is so critical to human functioning that it is difficult to even imagine a calendar not synchronized with terrestrial days.</p> <p>The year is the second most important astronomical cycle. The life of a pre-industrial person revolved around the seasons. A calendar synchronized with the tropical year would tell him when to plant and harvest crops, when to hunt and fish, when his domesticated animals were available for breeding, etc. The modern world is not ruled so tightly by the seasons (thanks to global trade we can buy any produce any day of the year), but it is still very strongly affected by it. It will likely be fun to have an barbecue in August (New England latitude). It will likely be dangerous to drive in January. I will need to start mowing the grass again in April.</p> <p>The month is the third most important astronomical cycle. Before the invention streetlighting, the moon had a meaningful effect on people's lives. When there is a full moon, people can still walk around at night without artificial light, meaning that people can travel, socialize, or whatever. If there is no moon, it is too dark to do anything. Many cultures have holidays close to full moons because of this. The Gregorian calendar is not synchronized with the moon, but one holiday, Easter, moves around because its definition includes being the next Sunday after a full moon. However, widespread artificial lighting has eliminated any need to consider the phase of the moon before heading out. In fact, I would be surprised if 10% of the people reading this could even say what phase the moon was in right now. The absence of moon synchronization is one thing the Gregorian calendar gets right.</p>"},{"location":"blog/leap-years-something-the-gregorian-calendar-gets-right/#leap-xs","title":"Leap Xs","text":"<p>Synchronizing a calendar with more than one astronomical cycle poses an important difficulty to making a calendar: no astronomical cycle divides cleanly into another astronomical cycle.</p> <ul> <li>Mean solar days per tropical year: 365.24217</li> <li>Mean solar days per synodic month: 29.530587981</li> <li>Synodic months per tropical year: 12.368266</li> </ul> <p>It is not possible to count out any integer number of days or integer number of months to start a new year without ending up desynchronized with the seasons. The desire to have perfectly regular periods is fundamentally incompatible with the desire to have perfectly synchronized periods. There must be a different number of days per calendar year if that calendar is to remain synchronized with the seasons. The only question is which years get an extra day/days.</p>"},{"location":"blog/leap-years-something-the-gregorian-calendar-gets-right/#observation","title":"Observation","text":"<p>One solution is to synchronize by observing the actual astronomical phenomenon of interest. Perhaps day one of the month is when the new moon is observed. Perhaps day one of the year is the first new moon after the winter solstice is observed. This ensures that the length of the longer unit differs by at most one smaller unit.</p> <p>One problem with this is that sometimes it can be uncertain if the desired astronomical event was observed. When looking for a new moon, it may be cloudy, or just a really close call. When a close call happens, people in two towns may think the new year/month started on two different days, ruining the main purpose of a calendar, which is to coordinate separated people. Assigning this job to a central authority (like a priest in the capital) can help, but then requires distant towns to wait on messengers to deliver the verdict.</p> <p>A more serious problem with the observation approach is that the number of days in a month or months in a year is unpredictable. It is possible to predict in advance some months and some years, but some will be too close to call until the judgement is rendered.</p>"},{"location":"blog/leap-years-something-the-gregorian-calendar-gets-right/#math","title":"Math","text":"<p>An alternative solution is to rely on a mathematical formula for determining leap days and leap months. Ideally, the mathematical formula would be simple, but the ratios above suggest that no simple formula exists. There is a fundamental trade-off between the complexity of the formula and how closely it tracks the true astronomical cycle.</p> <p>For example, the Julian calendar adds one leap day every four years. This is a very simple formula, but it drifts relative to the tropical year by adding about 3 too many leap days every 4 centuries. After about a millennium, the seasons come about a week too early. (To appreciate the importance of simple formulas, read about how hilariously long it took the Romans to correctly implement even this formula after deciding to use it.)</p> <p>The Gregorian calendar is a slight modification of the Julian calendar. The leap day is skipped if the year is divisible by 100, unless the year is also divisible by 400. By dropping three leap days every 400 years, the calendar has essentially no long term drift, with maybe 1 too many leap days every few thousand years.</p> <p>Despite the absence of long-term drift, the Gregorian calendar still drifts more than necessary in the short term. Any calendar with leap days must drift about one day, but the drift in the Gregorian calendar is more than two days. The winter solstice is listed as December 21, but it actually happens sometime between the morning of December 20 and the afternoon of December 22. When a leap day is skipped every 100 years, it means that 8 years go by all at once with no leap day. The seasons would drift less if, instead of letting 8 years pass every 100 years or so, it let 5 years pass every 33 or 34 years. This would reduce the amount of drift that was allowed to accumulate, but make it much harder to compute if a given year (say, 2020) was a leap year or not. The fact that the Gregorian calendar has an extra day of drift within the 400 year cycle does not appear to be something that anyone cares about.</p>"},{"location":"blog/leap-years-something-the-gregorian-calendar-gets-right/#conclusions","title":"Conclusions","text":"<p>When designing an optimal calendar, the first choice to be made is which astronomical cycles to synchronized to. The more cycles that are captured, the more useful information about the environment can be extracted from a given date. I think the solar calendar, including the Gregorian calendar, gets this right. The time of day and the season of the year still matter a lot even in the modern world full of artificial light, air conditioning, and desk jobs. The phase of the moon, however, no longer matters at all. Each cycle that much be synchronized adds considerable complexity and trade-offs. Lunisolar calendars differ in length by a month rather than differing in length by a year. If I were designing an optimal calendar, I would drop the moon and keep the sun and earth.</p> <p>Once deciding to use a solar calendar, one needs to decide the formula for the leap years. Here is probably the most remarkable thing about the Gregorian calendar. Some renaissance dudes managed to get everyone to switch to a new calendar whose only purpose was to fix a one-day per century drift in the Julian calendar. Furthermore, even with modern astronomy, we probably can't come up with a better formula. If I were designing an optimal calendar, I would keep the every-4-years-except-every-100-except-every-400 formula.</p> <p>If you are looking for something to remark upon this Leap Day, remember that the weather is just as bad (New England latitude) as it has been every Leap Day since a pope with an abacus ensured that Leap Days would stay in the dead of winter forever.</p>"},{"location":"blog/the-sort-within-groups-problem/","title":"The sort-within-groups problem","text":"<p>There is an interesting edge case in data grammars when a grouped data table is sorted by non-group columns. For example, what should the following dplyr code produce?</p> <pre><code>library(dplyr)\n\ndf &lt;- data.frame(\n  group = c(2, 2, 1, 1, 2, 2),\n  value = c(3, 4, 3, 1, 1, 3)\n)\n\ndf %&gt;% group_by(group) %&gt;% arrange(value)\n</code></pre> <p>In this intro example, there is one group column and one other column by which the table is being explicitly sorted. There are two groups, <code>group=1</code> and <code>group=2</code>, with <code>group=2</code> being non-contiguous\u2014meaning the rows of that group are spread throughout the table rather than being all next to each other. There are actually four defensible answers to how the rows should be sorted in the face of grouping:</p> <ol> <li>Ignore the grouping</li> <li>Sort by group columns first</li> <li>Cluster by group columns first</li> <li>Sort within the rows owned by each group</li> </ol>"},{"location":"blog/the-sort-within-groups-problem/#ignore-groups","title":"Ignore groups","text":"<p>In the beginning, there was only dplyr and, through version 0.3, dplyr simply ignored the grouping when sorting. At that time, the example problem from the intro would have produced:</p> <pre><code> group value\n     1     1\n     2     1  # Note that value is\n     2     3  # completely sorted\n     1     3\n     2     3\n     2     4\n</code></pre> <p>Now, this is admittedly a little unexpected. No other verb in the data grammar completely ignored groups (except <code>group_by</code> itself, I suppose).</p>"},{"location":"blog/the-sort-within-groups-problem/#sort-by-groups-first","title":"Sort by groups first","text":"<p>dplyr was already quite popular when 0.3 came out\u2014popular enough that there were Stack Overflow questions (1, 2, 3) and GitHub issues (1, 2) from users confused that <code>arrange</code> was ignoring grouping. It was in that last GitHub issue that ignoring groups was considered a bug by the lead maintainer Hadley Wickham. Starting in dplyr 0.4, the <code>arrange</code> verb was explicitly defined as sorting by group columns first and then the supplied columns. This sorting by group columns first ensured that all rows belonging to a group were contiguous. The rows would be sorted by the supplied columns, as expected, within the block, but not globally across the whole table. The example problem from the intro would result in:</p> <pre><code> group value\n     1     1  # Note how all the group=1 row\n     1     2  # have been moved to the top\n     2     1\n     2     3\n     2     3\n     2     4\n</code></pre> <p>This change caused a handful of Stack Overflow questions (1, 2) and a multitude of GitHub issues (1, 2, 3, 4) to be be raised by users confused about the change. These complaints must have made an impression on Hadley because he decided to revert it back to the ignore-groups behavior and, in version 0.5, that happened. This triggered a new avalanche of Stack Overflow questions (1, 2, 3, 4, 5) and GitHub issues (1, 2) from the users who were still confused that <code>arrange</code> ignored groups. Hadley said that reverting it yet again was out of the question. Eventually, a <code>.by_group=TRUE</code> option was added in 0.7 to cause <code>arrange</code> to also sort by group columns like in 0.4.</p> <p>I would argue that those users didn't actually want <code>arrange</code> to ignore groups; they didn't know they had groups when they used <code>arrange</code>. If you look at the six complaints about the sort-by-groups-first behavior, you notice that that are all doing the exact same sequence of operations: group by two columns, summarize a third column, and finally sort by that third column. They seemed to not realize that the table was still grouped after the <code>summarize</code>. You see, <code>summarize</code> in dplyr drops the rightmost group column, leaving the table still grouped by the remaining columns. If you group by one column, summarizing returns an ungrouped table. But if you group by two columns, summarizing returns a table that is still grouped by the first column. This is the confusing behavior of dplyr, not the sorting by groups first.</p> <p>Partial ungrouping after <code>summarize</code> probably warrants its own blog post. I understand why dplyr does it this way, but it can be surprising even to an experienced user. I think the reason this comes up with <code>arrange</code> instead of the other verbs is that, if you are expecting an ungrouped table, most of verbs usually still do what you want. If you do <code>filter,</code> <code>mutate</code>, <code>transmute</code>, even <code>group_by</code> then <code>summarize</code>, you usually get pretty much what you expect even if there are unexpected groups left over. But <code>arrange</code> is not like them. If you <code>arrange</code> a table with unexpected groups, you get really weird garbage out if the sorting prioritizes the group columns above the columns you requested.</p>"},{"location":"blog/the-sort-within-groups-problem/#cluster-by-groups","title":"Cluster by groups","text":"<p>Back when I used dplyr 0.4 (my first exposure to dplyr), I was little bit annoyed that it sorted groups because my groups were usually already contiguous and in the order that I wanted them. I thought that better behavior would be to \"cluster\" by group columns first and then sort within groups. Under clustering-before-sorting, the example problem from the intro would look like:</p> <pre><code> group value\n     2     1  # Note how group=2\n     2     3  # is now contiguous\n     2     3  # but still first\n     2     4\n     1     1\n     1     3\n</code></pre>"},{"location":"blog/the-sort-within-groups-problem/#cluster","title":"Cluster","text":"<p>Cluster is a verb in its own right; it takes a list of columns and and behaves a lot like sort in that the rows of all groups are contiguous at the end. However, the groups retain the same order relative to each other. Running <code>cluster</code> on a table who groups are already contiguous is a no-op. For tables who groups are not contiguous, the groups are ordered according to the order that the first row from each group appears.</p> <p>The cluster operation can be defined (rather inefficiently) as:</p> <ol> <li>Add a temporary column saving the current row indexes</li> <li>Group by the cluster columns</li> <li>Set the temporary equal to the minimum value in each group</li> <li>Ungroup</li> <li>Sort by the temporary column</li> <li>Delete the temporary column</li> </ol> <p>In dplyr, clustering alone can be done like this:</p> <pre><code>df %&gt;%\n  mutate(temp = row_number()) %&gt;%\n  group_by(group) %&gt;%\n  mutate(temp = min(temp)) %&gt;%\n  ungroup() %&gt;%\n  arrange(temp) %&gt;%\n  select(-temp)\n</code></pre> <pre><code> group value\n     2     3  # Groups have been\n     2     4  # clustered, but not\n     2     1  # sorted\n     2     3\n     1     3\n     1     1\n</code></pre>"},{"location":"blog/the-sort-within-groups-problem/#rows-owned-by-groups","title":"Rows owned by groups","text":"<p>When grouping, each row belongs to a particular group. For example, rows 1, 2, 5, and 6 belong to group <code>group=2</code> in our example table. We could define sorting on a grouped table as sorting each group in isolation, but the same rows belong to that group at the end. For example, using the problem from the intro, we pull out rows 1, 2, 5, and 6, sort them, and then put those four rows back into rows 1, 2, 5, and 6 following their new order:</p> <pre><code> group value\n     2     1\n     2     3\n     1     1  # Relative order of\n     1     3  # rows is unchanged\n     2     3\n     2     4\n</code></pre> <p>Arguably, this is the most technically correct solution. It most strongly reinforces the idea that grouping creates subtables isolated from each other and applies subsequent verbs to each subtable individually. This is basically how <code>mutate</code> works conceptually; it pulls each group out, mutates the given columns, and then puts the rows back where they came from.</p>"},{"location":"blog/the-sort-within-groups-problem/#conclusion","title":"Conclusion","text":"<p>The seeming correctness of the owned-rows solution is tempting. This is identical to the cluster-first solution if the table is already clustered by groups, which is probably the most typical situation when users care about the row order of a grouped table.</p> <p>The Polars library actually does it this way. If you use the equivalent (albeit, not as readable) syntax, you get the same result as in the \"Rows owned by groups\" section:</p> <pre><code>import polars as pl\n\ndf = pl.DataFrame({'group': [2,2,1,1,2,2], 'value': [3,4,3,1,1,3]})\n\ndf.with_column(pl.col('value').sort().over('group'))\n\n# shape: (6, 2)\n# \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n# \u2502 group \u2506 value \u2502\n# \u2502 ----- \u2506 ----- \u2502\n# \u2502 i64   \u2506 i64   \u2502\n# \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n# \u2502 2     \u2506 1     \u2502\n# \u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n# \u2502 2     \u2506 3     \u2502\n# \u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n# \u2502 1     \u2506 1     \u2502\n# \u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n# \u2502 1     \u2506 3     \u2502\n# \u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n# \u2502 2     \u2506 3     \u2502\n# \u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n# \u2502 2     \u2506 4     \u2502\n# \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The Pandas <code>groupby</code> operation itself sorts by default and clusters if sorting is turned off, rendering moot the question of how sort should be behave on grouped tables. Here is the operation using Pandas's characteristic syntax:</p> <pre><code>import pandas as pd\n\ndf = pd.DataFrame({'group':[2,2,1,1,2,2], 'value':[3,4,3,1,1,3]})\n\ndf.groupby('group', sort=False).apply(lambda x: x.sort_values('value'))\n\n#          group  value\n# group\n# 2     4      2      1\n#       0      2      3\n#       5      2      3\n#       1      2      4\n# 1     3      1      1\n#       2      1      3\n</code></pre> <p>Always clustering groups is interesting in its own right. Clustering allows the group columns to be indexed efficiently and guarantees that a number of edge cases that come from non-contiguous groups simply can never happen. But ultimately, the dplyr behavior is more versatile even if it leaves a lot of weird edge cases. As a user, you often want to preserve row order while still using various verbs, including grouping.</p> <p>Nevertheless, I think that Pandas is actually on to something here. Once the group verb has executed, it is reasonable to expect that all subsequent verbs will operate within each group. However, it is perfectly reasonable for the group verb itself to have three modes of operation:</p> <ol> <li>Sort rows by groups</li> <li>Cluster rows by groups</li> <li>Preserve row order</li> </ol> <p>\"Groups are completely isolated from each other\" is nice in principle, but being pedantic about this can lead to some weird places. If groups are completely isolated from each other then <code>df %&gt;% group_by(group) %&gt;% arrange(group)</code> is a no-op because the sorting is done within each group (where the group column is always constant). Even <code>cluster</code>, if implemented as its own verb, could not cluster along group columns because the relationship between groups was already frozen. This is definitely unexpected, and simply doing nothing, is putting principle over practicality. That does not mean library writers should sacrifice principles. Principles keep away edge cases. I am currently inclined toward having the three modes of operation and raising a well-crafted error when a group column is sorted that could guide users to sort before grouping, sort after ungrouping, or sort during grouping.</p>"},{"location":"blog/excellent-error-messages/","title":"Excellent error messages","text":"<p>Error messages are pervasive throughout programming, yet little has been written on the design of error messages for languages, libraries, and APIs. Much good advice can be found via simple web search on good error messages as shown to end users in GUIs, but standards for error messages intended for an audience of programmers is hard to find. This is not due to a lack of attention to error messages. There are certainly places where error messages are neglected, but neglect is far from universal. In fact, some of the best discussions on good error messages come from specific efforts by big projects to improve their error messages.</p> <p>That error messages are meant to be read by humans instead of computers is probably the main explanation for the absence of standardization. Inconsistent error messages even within a project does not create extra work in the way that an inconsistent API design does. An error message works the same whether it says \"Division by zero\" or \"Divide by zero\" in a way that \"object.to_string\" does not work the same as \"object.string\". But this functional insensitivity can be an advantage: error messages can be updated without breaking backwards compatibility.</p> <p>There are 6 main features of an excellent error message:</p> <ul> <li> <p>ID</p> </li> <li> <p>Location</p> </li> <li> <p>Context</p> </li> <li> <p>Expected</p> </li> <li> <p>Actual</p> </li> <li> <p>Suggestions (optional)</p> </li> </ul>"},{"location":"blog/excellent-error-messages/#id","title":"ID","text":"<p>This is a stable and unique ID for errors of this type. Typical IDs would be <code>IndexOutOfBounds</code> or <code>NotFound</code>. I would recommend that these are automatically generated from the exception class in any language whenever that is appropriate. In some sense, this is the least important part of the error message. It should not provide any information to the user that is not spelled out more clearly later in the message. However, it is relatively low-cost to add and has some useful properties.</p> <p>The ID functions as a title for the error message. A programmer experienced with the language, library, or API could recognize the error at the first word and not even need to read the rest of the message.</p> <p>The ID also functions as something that is searchable on the web or on documentation. The human-readable text may not be amenable to search or the human-readable text may be improved in subsequent versions, which would otherwise make it hard to find still-valid documentation written against previous versions.</p> <p>Finally, the ID is a stable, machine-readable part of the error. Sometimes, it is appropriate for machines to read errors, particularly when it is possible for them to automatically recover from them. This is prevalent in languages that use exceptions for flow control (a bad idea that deserves its own blog post), but this is also necessary to use various APIs. For example, an endpoint receiving an OAuth2 access token may return the following error message:</p> <pre><code>{\n  \"error\": \"invalid_token\",\n  \"error_description\": \"The access token expired\"\n}\n</code></pre> <p>The <code>invalid_token</code> is an ID tells OAuth2 clients that they should try to refresh the access token. Ideally, the application refreshes the token and retries the request without the user being involved or even informed.</p>"},{"location":"blog/excellent-error-messages/#location","title":"Location","text":"<p>The location of the error is relevant when the error is being generated from inside a complex system. The stack trace is the most famous incarnation of this, but the line number of a compiler error and the location in a JSON data structure serve this function also. This is probably the most important item on the list. I would rather get a \"Something went wrong on line 54 of file X\" than a \"Segmentation Fault\" with no location information. Now, an error that simply says \"something went wrong\" is particularly egregious, but it is better to get that and the location rather than the specific error and no location.</p>"},{"location":"blog/excellent-error-messages/#context","title":"Context","text":"<p>The context of an error is anything \"nearby\" the error that is not directly related to the error. Compilers typically do a good job of this by showing the text where the error occurred. Parsita, for example, produces errors like this:</p> <pre><code># parsita.state.ParseError: Expected positive integer but found '0'\n# Line 10, character 17\n#\n# 'n_replicates': 0,\n#                 ^\n</code></pre> <p>Printing the entire line is context. The caret pointer does not actually contain any additional information; it is just \"Line 10, character 17\" in graphical form, but marrying the location to the context can be convenient to the user.</p> <p>In an API that takes a large object as input, context is the particular piece that failed validation. For example, an API that took a list of model names to simulate would be painful to debug if the error message was just \"Model not found\" rather than this:</p> <pre><code>{\n    \"location\": [\"models\", 5],\n    \"type\": \"model_not_found\",\n    \"message\": \"Model 'mouse_pk' not found\"\n}\n</code></pre>"},{"location":"blog/excellent-error-messages/#expected-and-actual","title":"Expected and actual","text":"<p>If most projects were to list out the important attributes of an error message, I would suspect that they would not mention \"expected\" and \"actual\" and, instead, say something about \"message\" or \"description\". In fact, this is the main motivation of this post. What was expected and what was actually received is important information that is often missing from error messages today. Take the <code>IndexError</code> from Python 3.10:</p> <pre><code># test.py\ndef main():\n  a = [1,2,3]\n  i = 3\n  a[i]\n\nmain()\n\n# python test.py\n# Traceback (most recent call last):\n#   File \"/home/david/test.py\", line 6, in &lt;module&gt;\n#     main()\n#   File \"/home/david/test.py\", line 4, in main\n#     a[i]\n# IndexError: list index out of range\n</code></pre> <p>This error has a great ID, passable location information, and decent context. What is wrong with it is the message \"list index out of range\". This is a very common message format\u2014fact about what went wrong\u2014and I hate it. What is the legal range? What was the illegal value? No idea; power up your debugger.</p> <p>A better error message would be:</p> <pre><code># IndexError\n# Expected: Integer between -3 and 2\n# Actual: 3\n</code></pre> <p>I am unsure if this multiline style should be the standard. It is definitely different from what people are used to. A more standard message that conveys the same information would be \"Expected an integer between -3 and 2, but received 3\". A standard style would make it easier to read error messages in the same way that having an error ID makes them easier to read. Interestingly, providing the wrong type to an indexing operation in Python does follow the expected/actual pattern.</p> <pre><code>[1,2,3]['a']\n# SyntaxWarning: list indices must be integers or slices, not str; perhaps you missed a comma?\n# Traceback (most recent call last):\n#   File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n# TypeError: list indices must be integers or slices, not str\n</code></pre> <p>So this is something that Python does sometimes, but not always. I suspect that these message strings are assembled eagerly and Python wants to return a static string in the <code>IndexError</code> case because formatting a string would slow down the code when using <code>IndexError</code> is caught for control flow, whereas catching a <code>TypeError</code> is rarely on the critical path. Python would probably do better to modify the <code>IndexError</code> class to have a two attributes <code>length</code> and <code>index</code> and then have <code>__str__</code> generate a good error message lazily. (I leave as an exercise to the reader where to put the comma to make the above code a legal Python statement.)</p> <p>Rust, which does not use index out of bounds for flow control, does include this information in its error message, albeit not in a standardized format:</p> <pre><code>fn main() {\n    vec![1, 2, 3][3];\n}\n// thread 'main' panicked at 'index out of bounds: the len is 3 but the index is 3', src/main.rs:2:5\n</code></pre>"},{"location":"blog/excellent-error-messages/#suggestions","title":"Suggestions","text":"<p>Suggestions in error messages are the cherry on top of an already good error message. They are genuinely optional because a good enough language with good enough error messages about what went wrong should not need suggestions. The user should be able to figure out what to do without them. And suggestions are not without risk. They can actively make the error messages worse. For example, the Python index error above suggesting that a comma was missing is simply ridiculous.</p> <p>If the suggestion was always correct, then the language is actually redundant. It could simply apply that suggestion for the user. But in most cases, there is simply nothing to suggest. There is a set of expected values and what was provided as not one of them.</p> <p>There are two situations where suggestions shine. The first is when there is ambiguity in what the user intended and some action is required by the user to resolve that ambiguity. For example, when interpreting a boolean NumPy array as a Python boolean, the operation is ambiguous when there is more than one element. NumPy raises an error with a helpful suggestion</p> <pre><code>if np.array([1,2,3]) == 1:\n  pass\n# ValueError: The truth value of an array with more than one\n# element is ambiguous. Use a.any() or a.all()\n</code></pre> <p>Now, using <code>any</code> or <code>all</code> may still not be what was intended. In my experience, it always indicates an error elsewhere, like a failure to vectorize this section of the code. Even good suggestions run the risk of being counterproductive.</p> <p>The second situation where suggestions shine is where there is only one interpretation of the user's request, but it is not safe, so the language or API requires an extra hurdle to make extra sure that that is what the user intended.</p>"},{"location":"blog/excellent-error-messages/#standardization","title":"Standardization","text":"<p>This post is mainly a plea for more information to be attached to the exception, mainly in terms of what was expected, what was received, and if part of a larger input, the immediate context of the error. I am unsure about how much there is to gain from standardization here. A standardized message of the form \"Expected: blah \\n Actual: blah\" makes it quick to read, but not all errors fit so neatly into this form. \"Expected: object to be in database; Actual: object was not in database; ID: foo\" contains no more information than \"Object foo not found\".</p> <p>In a language like Python, it is likely to be cleanest to make a custom exception class for each exception type, use attributes to store the relevant context, and implement the <code>__str__</code> method to actually compose the message. Python already does well with the ID, location, and context. Including the expected and actual input is in the hands of the programmer.</p>"},{"location":"blog/the-phase-of-the-day-something-else-the-gregorian-calendar-gets-right/","title":"The Phase of the Day: Something else the Gregorian Calendar Gets Right","text":"<p>In the last calendars post, we concluded that days and years were the proper periods for a calendar. But their underlying natural phenomena are cycles without beginning and without end. To be useful in a calendar, we need to be able to put particular events on particular days and years. We need to be able to count and label days and years. To do that, we need to pick a dividing line between one day and the next, between one year and the next. When does one day start and one day end? When does one year roll over to the next? We'll call these properties the phases of our days and years. For the phase of the day, we'll find that the Gregorian calendar once again gets something spot on.</p> <p>The spinning of the Earth causes each day to cycle through two phases, one confusingly called \"day\" and the other \"night\". Our species' natural sleep cycle is aligned with this day-night cycle and provides a natural starting point for the definition of a day. With the exception of those constrained by occupation to a night shift, we go unconscious for eight hours mostly within the night portion of the daily cycle wherever we live. For each of us personally, our \"day\" begins when we wake up and ends when we go to sleep. This being largely coordinated with those near us provides a constraint on where we could reasonably put the divider between one day and the next. A universal divider between the days should occur while the bulk of us are asleep, which naturally occurs sometime at night.</p> <p>As far as I could find, no culture has ever put the dividing line between days anytime between sunrise and sunset. One possible deviation is historical astronomers, who switched days at solar zenith (noon). As astronomy was a historically nighttime activity, this seems more like an exception that proves the rule and suggests how a nocturnal species might choose to delineate its days (nights?).</p> <p>As a diurnal species, it seems we have settled on three possibilities: the day starts at sunset, the day starts at sunrise, or the day starts at solar nadir (midnight). The Jewish and Islamic days go from sunset to sunset. The Hindu days run from sunrise to sunrise. The European and Chinese days change over at solar nadir (midnight).</p> <p>In my research, I found a lot of fuzziness and uncertainty around the sunrise and sunset definitions. It seems that it may be most natural to count the start of the day at sunrise, the end of the day at sunset, and the night being not part of any particular day. I'm guessing that in a world before electrical lighting, little happened and even less was scheduled during the night, so it didn't matter much. The need to consistently assign every particular minute of the night to one particular day or the other is a post-industrial habit that our ancestors might find a tad obsessive.</p>"},{"location":"blog/the-phase-of-the-day-something-else-the-gregorian-calendar-gets-right/#sunrise","title":"Sunrise","text":"<p>Everywhere on Earth gets 12 hours of daylight, on average. Everywhere on Earth, the sun rises at 6 AM, on average, ignoring timezones. That would seem to be a useful universal, except that \"average\" is doing a lot of work here.</p> <p>Because of the tilt of the earth, the time from sunrise to sunrise is shorter in the winter and spring as the days get longer, and the time between sunrises is longer in summer and fall as the days get shorter. Depending on your latitude, this can accumulate to a very large change. At my latitude, the sun rises about an hour and a half before the average on the summer solstice and rises about an hour and a half later than the average on the winter solstice. Higher latitudes experience more extreme shifts. While the average sunrise may be 6 AM everywhere, the actual sunrise moves ahead and behind with the seasons everywhere except the equator.</p> <p>Assuming that we don't want the length of the day to change every day, we have little choice but to declare the mean sunrise as the \"civil sunrise\" and the start of the day. But then for half the year, the day is very much flipping over during daylight, which is something we decided earlier was the only truly unnatural time to do it.</p> <p>Perhaps in a post-electrical world it is not that big of a deal. We are not as driven by the sun as our ancestors. The sun may rise at 5 AM, but I'm still sleeping in. (This is not strictly true. Daylight saving time moves sunrise in this case back to 6 AM on the clock, which is a topic for another time.)</p>"},{"location":"blog/the-phase-of-the-day-something-else-the-gregorian-calendar-gets-right/#sunset","title":"Sunset","text":"<p>Sunset makes even less sense to me. I am actually amazed that several cultures ended up with this convention. It drifts throughout the solar day just like sunrise, but has one additional drawback. There are about 12 hours of daylight, but humans sleep only about 8 hours. We put as many of those sleeping hours during the night, but that still leaves 4 night hours that we are typically awake for. With these extra hours, we tend to stay up late rather than get up early. This effect is so strong, it must be for biological reasons. As much as we may talk about night owls vs morning larks, going to sleep at 10 PM and waking up at 6 AM is wildly more common than going to sleep at 6 PM and waking up 2 AM, even though these are symmetric sleep schedules about the nadir. From a purely objective standpoint, humans are night owls.</p> <p>Because of this bias, switching over the day at sunset invariably switches the day while almost everyone is awake and in the middle of doing things year round. Perhaps this makes sense as a religious calendar. You can have fasting days that don't actually require you to go a day without food. But as a civil calendar, this is confusing.</p> <p>If we all lived on the equator or if the Earth had no tilt, it could make sense to have sunrise or sunset be the dividing line between two days. In either of those cases, the sun always rises and sets at the same time. But with the current planet and population distribution upon it, this cannot be made to work.</p>"},{"location":"blog/the-phase-of-the-day-something-else-the-gregorian-calendar-gets-right/#midnight","title":"Midnight","text":"<p>Our eight-hour sleeping habits do not all perfectly overlap. And as mentioned above, there is a bias toward sleeping later rather than earlier in the night. Nevertheless, the window from midnight to 4 AM is a pretty clean deadzone. From a pure perspective of inactivity, 2 AM or 3 AM may make more sense. We switch over daylight saving time at 2 AM for this reason. But there is no astronomical meaning to 2 AM or 3 AM; it's just arbitrary.</p> <p>Midnight is unique in that it is the sun's nadir, the time when the sun is at its lowest point in the sky. And fortunately for the purpose of making a regular calendar, there is the same amount of time between every nadir.</p> <p>Actually, that is not strictly true. A perfectly constant period between nadirs rests on three assumptions: (1) that the period of the Earth rotation is constant, (2) that the revolution of the Earth around the sun is constant, and (3) that no other process is driving the sun higher or lower in the sky. Inaccuracies in the first assumption are on the order of milliseconds and can only be detected with atomic clocks. Inaccuracies in the second assumption contribute to variations on the order of minutes. The Earth orbit is slightly elliptical, which causes it to travel faster at some times than at others. Inaccuracies in the third assumption also contribute on the order of minutes. Because the path of the sun itself gets higher and lower with the seasons, it takes different amounts of time to reach its zenith and its nadir. Overall, the last two effects combine and the length of the solar day varies up to 15 minutes above and below the mean solar day.</p>"},{"location":"blog/the-phase-of-the-day-something-else-the-gregorian-calendar-gets-right/#conclusion","title":"Conclusion","text":"<p>It seems that the Gregorian calendar's division of the day, at midnight, is about as good as we can make it. This puts the division in a natural spot in our biological cycle while also aligning the day with an appropriate astronomical phenomenon.</p> <p>There is one rather large wrinkle in this design though. It isn't midnight at the same time everywhere on the planet, but time zones are a topic for another time.</p>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/archive/2022/","title":"2022","text":""},{"location":"blog/archive/2020/","title":"2020","text":""},{"location":"blog/archive/2019/","title":"2019","text":""},{"location":"blog/archive/2018/","title":"2018","text":""},{"location":"blog/archive/2017/","title":"2017","text":""},{"location":"blog/archive/2016/","title":"2016","text":""},{"location":"blog/archive/2015/","title":"2015","text":""},{"location":"blog/archive/2014/","title":"2014","text":""},{"location":"blog/archive/2013/","title":"2013","text":""},{"location":"blog/category/calendars/","title":"calendars","text":""},{"location":"blog/category/programming/","title":"programming","text":""},{"location":"blog/category/biology/","title":"biology","text":""},{"location":"blog/category/policy/","title":"policy","text":""},{"location":"blog/category/philosophy/","title":"philosophy","text":""},{"location":"blog/category/culture/","title":"culture","text":""},{"location":"blog/page/2/","title":"Blog","text":""},{"location":"blog/page/3/","title":"Blog","text":""}]}